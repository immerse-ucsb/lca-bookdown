[{"path":"index.html","id":"mixture-modeling-with-mplusautomation","chapter":"Mixture Modeling with MplusAutomation","heading":"Mixture Modeling with MplusAutomation","text":" Welcome! collection resources teach apply mixture modeling using Mplus1 MplusAutomation2! resources serve comprehensive guide understanding applying LCA using Mplus automation capabilities MplusAutomation. , learn start finish apply mixture modeling using Mplus MplusAutomation package.Note: book continuous work progress. code presented may updated /expanded research progresses. Please treat material living document rather final product.","code":""},{"path":"index.html","id":"stay-in-touch","chapter":"Mixture Modeling with MplusAutomation","heading":"Stay in touch!","text":"Please visit website learn IMMERSE fellowship.\ncode materials found Bookdown, see .\nPlease visit website learn IMMERSE fellowship.code materials found Bookdown, see .Visit GitHub account access IMMERSE training materials.Visit GitHub account access IMMERSE training materials.Follow us BlueSky X stay--date fellowship!Follow us BlueSky X stay--date fellowship!","code":""},{"path":"index.html","id":"acknowledgements","chapter":"Mixture Modeling with MplusAutomation","heading":"Acknowledgements","text":"reference workshop: Institute Mixture Modeling Equity-Oriented Researchers, Scholars, Educators (2025). IMMERSE Online Resources (IES . 305B220021). Institute Education Sciences. https://mixture-modeling.netlify.app/","code":""},{"path":"index.html","id":"authors-contributors","chapter":"Mixture Modeling with MplusAutomation","heading":"Authors & Contributors","text":"resource developed IMMERSE team:Dina Arch, PhD, Postdoctoral ScholarDina Arch, PhD, Postdoctoral ScholarKaren Nylund-Gibson, PhD, Principal InvestigatorKaren Nylund-Gibson, PhD, Principal InvestigatorMarsha Ing, PhD, Co-Principal InvestigatorMarsha Ing, PhD, Co-Principal InvestigatorAdditional code contributors:Adam Garber, PhDAdam Garber, PhDDelwin Carter, PhDDelwin Carter, PhDYidi Zhang, MAYidi Zhang, MAWe also thank IMMERSE fellows provided feedback development materials.","code":""},{"path":"r-and-rstudio.html","id":"r-and-rstudio","chapter":"1 R and RStudio","heading":"1 R and RStudio","text":"walkthrough presented IMMERSE team go common tasks carried R.\nmany free resources available get started R RStudio.\nOne favorites R Data Science.R3 free, open-source programming language environment widely used statistical computing, data analysis, data visualization.R3 free, open-source programming language environment widely used statistical computing, data analysis, data visualization.RStudio4 integrated development environment (IDE) R, providing intuitive interface makes coding, visualization, project management accessible.RStudio4 integrated development environment (IDE) R, providing intuitive interface makes coding, visualization, project management accessible.Mplus5 statistical modeling program used analyzing complex data, latent variable models, structural equation modeling, growth modeling.\nbook uses R package called MplusAutomation automate process running models, extracting results, generating data visualizations.Mplus5 statistical modeling program used analyzing complex data, latent variable models, structural equation modeling, growth modeling.\nbook uses R package called MplusAutomation automate process running models, extracting results, generating data visualizations.","code":""},{"path":"r-and-rstudio.html","id":"installation","chapter":"1 R and RStudio","heading":"1.1 Installation","text":"","code":""},{"path":"r-and-rstudio.html","id":"step-0-install-r-rstudio-and-mplus","chapter":"1 R and RStudio","heading":"1.1.1 Step 0: Install R, RStudio, and Mplus","text":"find guide installing R R Studio.\ncan also install Mplus .Note: installation Mplus requires paid license mixture add-.\nIMMERSE fellows given copy Mplus use one year training.","code":""},{"path":"r-and-rstudio.html","id":"set-up","chapter":"1 R and RStudio","heading":"1.2 Set-up","text":"","code":""},{"path":"r-and-rstudio.html","id":"step-1-create-a-new-r-project-in-rstudio","chapter":"1 R and RStudio","heading":"1.2.1 Step 1: Create a new R-project in RStudio","text":"R-projects help us organize folders , filepaths, scripts.\ncreate new R project:File –> New Project…Click “New Directory” –> New Project –> Name project","code":""},{"path":"r-and-rstudio.html","id":"step-2-create-an-r-markdown-document","chapter":"1 R and RStudio","heading":"1.2.2 Step 2: Create an R-markdown document","text":"R-markdown file provides authoring framework data science allows us organize reports using texts code chunks.\ndocument reading made using R-markdown!create R-markdown:File –> New File –> R Markdown…window pops , give R-markdown title “Introduction R RStudio” Click “OK.” see new markdown example text code chunks.\nwant clean document start delete everything line 10 .\nGo ahead save document R Project folder.","code":""},{"path":"r-and-rstudio.html","id":"step-3-load-packages","chapter":"1 R and RStudio","heading":"1.2.3 Step 3: Load packages","text":"first code chunk given markdown packages using.\ninsert code chunk, etiher use keyboard shortcut ctrl + alt + Code –> Insert Chunk click green box letter C .\npackages want markdown read :reminder, function work receive error like : find function \"random_function\"; try load package receive error like : package called `random_package` , need install package using install.packages(\"random_package\") console (bottom-left window R studio).\ninstalled package never need install , however must always load packages beginning R markdown using library(random_package), shown document.style code package using called tidyverse6 .\nfunctions within tidyverse package , ’ve indicated packages used code chunk .","code":"\nlibrary(psych) # describe()\nlibrary(here) #helps with filepaths\nlibrary(gt) # create tables\nlibrary(tidyverse) #collection of R packages designed for data science"},{"path":"r-and-rstudio.html","id":"explore-the-data","chapter":"1 R and RStudio","heading":"1.3 Explore the data","text":"","code":""},{"path":"r-and-rstudio.html","id":"step-4-read-in-data","chapter":"1 R and RStudio","heading":"1.3.1 Step 4: Read in data","text":"demonstrate mixture modeling training program online resource components IES grant utilize Civil Rights Data Collection (CRDC) (CRDC) data repository.\nCRDC federally mandated school-level data collection effort occurs every year.\npublic data currently available selected latent class indicators across 4 years (2011, 2013, 2015, 2017) US states.\nexample, use Arizona state sample.\nutilize six focal indicators constitute latent class model example; three variables report harassment/bullying schools based disability, race, sex, three variables full-time equivalent school staff hires (counselor, psychologist, law enforcement).\ndata source also includes covariates variety subjects distal outcomes reported 2018 math/reading assessments graduation rates.read data R:Ways view data R:click data Global Environment (upper right pane) use…summary() gives basic summary statistics & shows number NA values (great checking data read correctly)names() provides list column names. useful don’t memorized!head() prints top 6 rows dataframe","code":"\ndata <- read_csv(here(\"data\", \"crdc_lca_data.csv\")) \nView(data)\nsummary(data)\n#>     leaid             ncessch            report_dis    \n#>  Length:2027        Length:2027        Min.   :0.0000  \n#>  Class :character   Class :character   1st Qu.:0.0000  \n#>  Mode  :character   Mode  :character   Median :0.0000  \n#>                                        Mean   :0.0425  \n#>                                        3rd Qu.:0.0000  \n#>                                        Max.   :1.0000  \n#>                                        NA's   :27      \n#>   report_race      report_sex   counselors_fte  \n#>  Min.   :0.000   Min.   :0.00   Min.   :0.0000  \n#>  1st Qu.:0.000   1st Qu.:0.00   1st Qu.:0.0000  \n#>  Median :0.000   Median :0.00   Median :0.0000  \n#>  Mean   :0.103   Mean   :0.17   Mean   :0.4595  \n#>  3rd Qu.:0.000   3rd Qu.:0.00   3rd Qu.:1.0000  \n#>  Max.   :1.000   Max.   :1.00   Max.   :1.0000  \n#>  NA's   :27      NA's   :27     NA's   :27      \n#>    psych_fte         law_fte      \n#>  Min.   :0.0000   Min.   :0.0000  \n#>  1st Qu.:0.0000   1st Qu.:0.0000  \n#>  Median :0.0000   Median :0.0000  \n#>  Mean   :0.4742   Mean   :0.1255  \n#>  3rd Qu.:1.0000   3rd Qu.:0.0000  \n#>  Max.   :1.0000   Max.   :1.0000  \n#>  NA's   :30       NA's   :27\nnames(data)\n#> [1] \"leaid\"          \"ncessch\"        \"report_dis\"    \n#> [4] \"report_race\"    \"report_sex\"     \"counselors_fte\"\n#> [7] \"psych_fte\"      \"law_fte\"\nhead(data)\n#> # A tibble: 6 × 8\n#>   leaid   ncessch      report_dis report_race report_sex\n#>   <chr>   <chr>             <dbl>       <dbl>      <dbl>\n#> 1 0400001 040000100120          0           0          0\n#> 2 0400001 040000100616          0           0          1\n#> 3 0400001 040000101204          0           0          1\n#> 4 0400001 040000101871          0           1          1\n#> 5 0400001 040000101872          0           0          0\n#> 6 0400001 040000102344          0           0          0\n#> # ℹ 3 more variables: counselors_fte <dbl>,\n#> #   psych_fte <dbl>, law_fte <dbl>"},{"path":"r-and-rstudio.html","id":"step-5-descriptive-statistics","chapter":"1 R and RStudio","heading":"1.3.2 Step 5: Descriptive Statistics","text":"Let’s look descriptive statistics variable.\nlooking ID variables’ (leaid) (necessch) descriptives unnecessary, use select() remove variable using minus (-) sign:Alternatively, can use psych::describe() function give information:want look subset data?\nexample, want subset data observe specific school district?\n(leaid) can use tidyverse::filter() subset data using certain criteria.Since binary data (0,1), helpful look proportions:","code":"\ndata %>% \n  select(-leaid, -ncessch) %>% \n  summary()\n#>    report_dis      report_race      report_sex  \n#>  Min.   :0.0000   Min.   :0.000   Min.   :0.00  \n#>  1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.00  \n#>  Median :0.0000   Median :0.000   Median :0.00  \n#>  Mean   :0.0425   Mean   :0.103   Mean   :0.17  \n#>  3rd Qu.:0.0000   3rd Qu.:0.000   3rd Qu.:0.00  \n#>  Max.   :1.0000   Max.   :1.000   Max.   :1.00  \n#>  NA's   :27       NA's   :27      NA's   :27    \n#>  counselors_fte     psych_fte         law_fte      \n#>  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n#>  1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n#>  Median :0.0000   Median :0.0000   Median :0.0000  \n#>  Mean   :0.4595   Mean   :0.4742   Mean   :0.1255  \n#>  3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n#>  Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n#>  NA's   :27       NA's   :30       NA's   :27\ndata %>% \n  select(-leaid, -ncessch) %>% \n  describe()\n#>                vars    n mean   sd median trimmed mad min\n#> report_dis        1 2000 0.04 0.20      0    0.00   0   0\n#> report_race       2 2000 0.10 0.30      0    0.00   0   0\n#> report_sex        3 2000 0.17 0.38      0    0.09   0   0\n#> counselors_fte    4 2000 0.46 0.50      0    0.45   0   0\n#> psych_fte         5 1997 0.47 0.50      0    0.47   0   0\n#> law_fte           6 2000 0.13 0.33      0    0.03   0   0\n#>                max range skew kurtosis   se\n#> report_dis       1     1 4.53    18.55 0.00\n#> report_race      1     1 2.61     4.82 0.01\n#> report_sex       1     1 1.76     1.08 0.01\n#> counselors_fte   1     1 0.16    -1.97 0.01\n#> psych_fte        1     1 0.10    -1.99 0.01\n#> law_fte          1     1 2.26     3.11 0.01\ndata %>% \n  filter(leaid == \"0408800\") %>% \n  describe() \n#>                vars  n  mean    sd median trimmed   mad min\n#> leaid*            1 86  1.00  0.00    1.0    1.00  0.00   1\n#> ncessch*          2 86 43.50 24.97   43.5   43.50 31.88   1\n#> report_dis        3 86  0.05  0.21    0.0    0.00  0.00   0\n#> report_race       4 86  0.15  0.36    0.0    0.07  0.00   0\n#> report_sex        5 86  0.19  0.39    0.0    0.11  0.00   0\n#> counselors_fte    6 86  0.95  0.21    1.0    1.00  0.00   0\n#> psych_fte         7 86  0.19  0.39    0.0    0.11  0.00   0\n#> law_fte           8 86  0.14  0.35    0.0    0.06  0.00   0\n#>                max range  skew kurtosis   se\n#> leaid*           1     0   NaN      NaN 0.00\n#> ncessch*        86    85  0.00    -1.24 2.69\n#> report_dis       1     1  4.23    16.10 0.02\n#> report_race      1     1  1.91     1.68 0.04\n#> report_sex       1     1  1.59     0.52 0.04\n#> counselors_fte   1     1 -4.23    16.10 0.02\n#> psych_fte        1     1  1.59     0.52 0.04\n#> law_fte          1     1  2.04     2.21 0.04\n\n\n#You can use any operator to filter: >, <, ==, >=, etc.\ndata %>% \n  drop_na() %>% \n  pivot_longer(report_dis:law_fte, names_to = \"variable\") %>% \n  group_by(variable) %>% \n  summarise(prop = sum(value)/n(),\n            n = n()) %>%\n  arrange(desc(prop))\n#> # A tibble: 6 × 3\n#>   variable         prop     n\n#>   <chr>           <dbl> <int>\n#> 1 psych_fte      0.481   1970\n#> 2 counselors_fte 0.459   1970\n#> 3 report_sex     0.173   1970\n#> 4 law_fte        0.127   1970\n#> 5 report_race    0.105   1970\n#> 6 report_dis     0.0431  1970"},{"path":"mplusautomation.html","id":"mplusautomation","chapter":"2 MplusAutomation","heading":"2 MplusAutomation","text":"MplusAutomation7 designed streamline use Mplus, powerful statistical software modeling complex data developed Muthen Muten (https://www.statmodel.com). MplusAutomation, researchers can automate process estimating latent variable models, running batches models, extracting results, generating data visualizations - within R environment.?MplusAutomation R packageIt “wraps around” Mplus programRequires R & Mplus softwareRequires learning basics 2 programming languagesCar metaphor: R/Rstudio steering wheel dashboard & Mplus engineWHY?MplusAutomation can provide clearly organized work procedures every research decision can documented single placeIncrease reproducibility, organization, efficiency, transparencyHOW?interface MplusAutomation entirely within R-Studio. need open MplusThe code presented repetitive designBelow template mplusObject() & mplusModeler() functions. Use template run statistical models Mplus.","code":"\nm_template  <- mplusObject(\n  \n  TITLE = \n    \"\", \n  \n  VARIABLE = \n    \"\",\n  \n  ANALYSIS = \n    \"\",\n  \n  PLOT = \n    \"\",\n  \n  OUTPUT = \n    \"\",\n \n  usevariables = colnames(), \n  rdata =  )\n\nm_template_fit <- mplusModeler(m_template, \n                  dataout=here(\"\", \".dat\"),\n                  modelout=here(\"\", \".inp\"),\n                  check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"enumeration.html","id":"enumeration","chapter":"3 Enumeration","heading":"3 Enumeration","text":"Example: Bullying SchoolsTo demonstrate mixture modeling training program online resource components IES grant utilize Civil Rights Data Collection (CRDC)8 data repository.\nCRDC federally mandated school-level data collection effort occurs every year.\npublic data currently available selected latent class indicators across 4 years (2011, 2013, 2015, 2017) US states.\nexample, use Arizona state sample.\nutilize six focal indicators constitute latent class model example; three variables report harassment/bullying schools based disability, race, sex, three variables full-time equivalent school staff hires (counselor, psychologist, law enforcement).\ndata source also includes covariates variety subjects distal outcomes reported 2018 math/reading assessments graduation rates.Load packages","code":"\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(glue)\nlibrary(MplusAutomation)\nlibrary(here)\nlibrary(janitor)\nlibrary(gt)\nlibrary(cowplot)\nlibrary(DiagrammeR) "},{"path":"enumeration.html","id":"variable-description","chapter":"3 Enumeration","heading":"3.1 Variable Description","text":"Variables transformed dichotomous indicators using following coding strategyHarassment bullying count variables recoded 1 school reported least one incident harassment (0 indicates reported incidents).\noriginal scale reported CDRC staff variables full time equivalent employees (FTE) represented 1 part time employees represented values 1 0.\nSchools greater one staff designated type represented values greater 1.\nvalues greater zero recorded 1s (e.g., .5, 1,3) indicating school staff present campus least part time.\nSchools staff designated type indicated 0 dichotomous variable.","code":""},{"path":"enumeration.html","id":"prepare-data","chapter":"3 Enumeration","heading":"3.2 Prepare Data","text":"","code":"\ndf_bully <- read_csv(here(\"data\", \"crdc_lca_data.csv\")) %>% \n  clean_names() %>% \n  dplyr::select(report_dis, report_race, report_sex, counselors_fte, psych_fte, law_fte) "},{"path":"enumeration.html","id":"descriptive-statistics","chapter":"3 Enumeration","heading":"3.3 Descriptive Statistics","text":"Save imageFrequency Plot","code":"\ndframe <- df_bully %>%\n  pivot_longer(\n    c(report_dis, report_race, report_sex, counselors_fte, psych_fte, law_fte),\n    names_to = \"Variable\"\n  ) %>%\n  group_by(Variable) %>%\n  summarise(\n    Count = sum(value == 1, na.rm = TRUE),\n    Total = n(),\n    .groups = \"drop\"\n  ) %>%\n  mutate(`Proportion Endorsed` = round(Count / Total, 3)) %>%\n  select(Variable, `Proportion Endorsed`, Count)\n\ngt(dframe) %>%\n  tab_header(\n    title = md(\"**LCA Indicator Endorsement**\"),\n    subtitle = md(\"&nbsp;\")\n  ) %>%\n  tab_options(\n    column_labels.font.weight = \"bold\",\n    row_group.font.weight = \"bold\"\n  )\ngtsave(prop_table, here(\"figures\", \"prop_table.png\"))\ndata_long <- df_bully %>%\n  pivot_longer(c(report_dis, report_race, report_sex, counselors_fte, psych_fte, law_fte), names_to = \"variable\")\n\n# Bar plot for 0/1 indicators\nggplot(data_long, aes(x = factor(value))) +\n  geom_bar(fill = \"#69b3a2\", color = \"black\") +\n  geom_text(stat = \"count\", aes(label = after_stat(count)), \n            vjust = -0.5, size = 3.5) +\n  facet_wrap(~ variable) +\n  labs(\n    title = \"Binary Indicator Distributions\",\n    x = \"Value (0 = No, 1 = Yes)\",\n    y = \"Count\"\n  ) +\n  theme_cowplot()"},{"path":"enumeration.html","id":"enumeration-1","chapter":"3 Enumeration","heading":"3.4 Enumeration","text":"code uses mplusObject function MplusAutomation package saves model runs enum folder.IMPORTANT: moving forward, make sure open output document ensure models estimated normally.","code":"\n\nlca_6  <- lapply(1:6, function(k) {\n  lca_enum  <- mplusObject(\n      \n    TITLE = glue(\"{k}-Class\"), \n  \n    VARIABLE = glue(\n    \"categorical = report_dis-law_fte; \n     usevar = report_dis-law_fte;\n     classes = c({k}); \"),\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 200 100; \n    processors = 10;\",\n  \n  OUTPUT = \"sampstat residual tech11 tech14;\",\n  \n  PLOT = \n    \"type = plot3; \n    series = report_dis-law_fte(*);\",\n  \n  usevariables = colnames(df_bully),\n  rdata = df_bully)\n\nlca_enum_fit <- mplusModeler(lca_enum, \n                            dataout=glue(here(\"enum\", \"bully.dat\")),\n                            modelout=glue(here(\"enum\", \"c{k}_bully.inp\")) ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"enumeration.html","id":"examine-and-extract-mplus-files","chapter":"3 Enumeration","heading":"3.5 Examine and extract Mplus files","text":"Code Delwin Carter (2025)Check Models :WarningsErrorsConvergence Loglikelihood Replication Information","code":"\nsource(here(\"functions\", \"extract_mplus_info.R\"))\n\n# Define the directory where all of the .out files are located.\noutput_dir <- here(\"enum\")\n\n# Get all .out files\noutput_files <- list.files(output_dir, pattern = \"\\\\.out$\", full.names = TRUE)\n\n# Process all .out files into one dataframe\nfinal_data <- map_dfr(output_files, extract_mplus_info_extended)\n\n# Extract Sample_Size from final_data\nsample_size <- unique(final_data$Sample_Size)"},{"path":"enumeration.html","id":"examine-mplus-warnings","chapter":"3 Enumeration","heading":"3.5.1 Examine Mplus Warnings","text":"","code":"\nsource(here(\"functions\", \"extract_warnings.R\"))\n\nwarnings_table <- extract_warnings(final_data)\nwarnings_table\n\n# Save the warnings table\n#gtsave(warnings_table, here(\"figures\", \"warnings_table.png\"))"},{"path":"enumeration.html","id":"examine-mplus-errors","chapter":"3 Enumeration","heading":"3.5.2 Examine Mplus Errors","text":"","code":"\nsource(here(\"functions\", \"error_visualization.R\"))\n\n# Process errors\nerror_table_data <- process_error_data(final_data)\nerror_table_data\n\n# Save the errors table\n#gtsave(error_table, here(\"figures\", \"error_table.png\"))"},{"path":"enumeration.html","id":"examine-convergence-and-loglikelihood-replications","chapter":"3 Enumeration","heading":"3.5.3 Examine Convergence and Loglikelihood Replications","text":"N = 2027Random StartsFinal starting value sets convergingLL ReplicationSmallest ClassModelBest LLnparInitialFinalf%f%f%1-Class-5,443.4096200100100100%100100.0%2,027100.0%2-Class-5,194.136132001005757%57100.0%44421.9%3-Class-5,122.478202001009393%8086.0%21610.6%4-Class-5,111.757272001004747%2042.6%21210.5%5-Class-5,105.589342001003737%718.9%432.1%6-Class-5,099.881412001003232%412.5%361.8%","code":"\nsource(here(\"functions\", \"summary_table.R\"))\n\n# Print Table with Superheader & Heatmap\nsummary_table <- create_flextable(final_data, sample_size)\nsummary_table\n\n# Save the flextable as a PNG image\n#invisible(save_as_image(summary_table, path = here(\"figures\", \"housekeeping.png\")))"},{"path":"enumeration.html","id":"check-for-loglikelihood-replication","chapter":"3 Enumeration","heading":"3.5.4 Check for Loglikelihood Replication","text":"Visualize examine loglikelihood replication values ouput file individuallyVisualize examine loglikelihood replication output file together1-Class2-Class3-Class4-Class5-Class6-ClassLLN%LLN%LLN%LLN%LLN%LLN%-5443.409100100-5194.13657100-5122.4788086-5111.7572042.6-5105.589718.9-5,099.881412.5——————-5123.9451010.8-5111.75936.4-5105.66112.7-5,100.27213.1——————-5123.97933.2-5112.25348.5-5105.79138.1-5,100.84226.2—————————-5112.95512.1-5105.799410.8-5,100.87413.1—————————-5115.5321123.4-5106.74825.4-5,100.92826.2—————————-5115.53812.1-5106.98312.7-5,101.01713.1—————————-5115.88412.1-5107.16925.4-5,101.07126.2—————————-5116.98136.4-5107.17238.1-5,101.08913.1—————————-5117.82936.4-5107.44912.7-5,101.11713.1————————————-5107.4512.7-5,101.31613.1————————————-5107.45812.7-5,101.33213.1————————————-5107.72812.7-5,101.45213.1————————————-5107.95812.7-5,101.49413.1————————————-5108.05812.7-5,101.51213.1————————————-5108.08412.7-5,101.59213.1————————————-5108.09612.7-5,101.59313.1————————————-5108.86410.8-5,101.91313.1————————————-5109.00212.7-5,102.07513.1————————————-5110.47412.7-5,102.61313.1———————————————-5,102.61613.1———————————————-5,102.66213.1———————————————-5,104.16713.1———————————————-5,104.46213.1———————————————-5,105.30913.1———————————————-5,107.30213.1———————————————-5,107.62413.1","code":"\n# Load the function for separate plots\nsource(here(\"functions\", \"ll_replication_plots.R\"))\n\n# Generate individual log-likelihood replication tables\nll_replication_tables <- generate_ll_replication_plots(final_data)\nll_replication_tables\nll_replication_table_all <- source(here(\"functions\", \"ll_replication_processing.R\"), local = TRUE)$value\nll_replication_table_all"},{"path":"enumeration.html","id":"table-of-fit","chapter":"3 Enumeration","heading":"3.6 Table of Fit","text":"First, extract data:, create table:Save table","code":"\noutput_enum <- readModels(here(\"enum\"), filefilter = \"bully\", quiet = TRUE)\n\n# Extract fit indices\nenum_extract <- LatexSummaryTable(\n  output_enum,\n  keepCols = c(\n    \"Title\",\n    \"Parameters\",\n    \"LL\",\n    \"BIC\",\n    \"aBIC\",\n    \"BLRT_PValue\",\n    \"T11_VLMR_PValue\",\n    \"Observations\"\n  ),\n  sortBy = \"Title\"\n)\n\n# Calculate additional fit indices\nallFit <- enum_extract %>%\n  mutate(CAIC = -2 * LL + Parameters * (log(Observations) + 1)) %>%\n  mutate(AWE = -2 * LL + 2 * Parameters * (log(Observations) + 1.5)) %>%\n  mutate(SIC = -.5 * BIC) %>%\n  mutate(expSIC = exp(SIC - max(SIC))) %>%\n  mutate(BF = exp(SIC - lead(SIC))) %>%\n  mutate(cmPk = expSIC / sum(expSIC)) %>%\n  dplyr::select(Title, Parameters, LL, BIC, aBIC, CAIC, AWE, BLRT_PValue, T11_VLMR_PValue, BF, cmPk) %>%\n  arrange(Parameters)\n\n# Merge columns with LL replications and class size from `final_data`\nmerged_table <- allFit %>%\n  mutate(Title = str_trim(Title)) %>%\n  left_join(\n    final_data %>%\n      select(\n        Class_Model,\n        Perc_Convergence,\n        Replicated_LL_Perc,\n        Smallest_Class,\n        Smallest_Class_Perc\n      ),\n    by = c(\"Title\" = \"Class_Model\")\n  ) %>%\n  mutate(Smallest_Class = coalesce(Smallest_Class, final_data$Smallest_Class[match(Title, final_data$Class_Model)])) %>%\n  relocate(Perc_Convergence, Replicated_LL_Perc, .after = LL) %>%\n  mutate(Smallest_Class_Combined = paste0(Smallest_Class, \"\\u00A0(\", Smallest_Class_Perc, \"%)\")) %>%\n  select(\n    Title,\n    Parameters,\n    LL,\n    Perc_Convergence,\n    Replicated_LL_Perc,\n    BIC,\n    aBIC,\n    CAIC,\n    AWE,\n    T11_VLMR_PValue,\n    BLRT_PValue,\n    Smallest_Class_Combined,\n    BF,\n    cmPk\n  )\nfit_table1 <- merged_table %>%\n  select(Title, Parameters, LL, Perc_Convergence, Replicated_LL_Perc, \n         BIC, aBIC, CAIC, AWE, \n         T11_VLMR_PValue, BLRT_PValue, \n         Smallest_Class_Combined) %>% \n  gt() %>%\n  tab_header(title = md(\"**Model Fit Summary Table**\")) %>%\n  tab_spanner(label = \"Model Fit Indices\", columns = c(BIC, aBIC, CAIC, AWE)) %>%\n  tab_spanner(label = \"LRTs\", columns = c(T11_VLMR_PValue, BLRT_PValue)) %>%\n  tab_spanner(label = md(\"Smallest\\u00A0Class\"), columns = c(Smallest_Class_Combined)) %>%\n  cols_label(\n    Title = \"Classes\",\n    Parameters = md(\"Par\"),\n    LL = md(\"*LL*\"),\n    Perc_Convergence = \"% Converged\",\n    Replicated_LL_Perc = \"% Replicated\",\n    BIC = \"BIC\",\n    aBIC = \"aBIC\",\n    CAIC = \"CAIC\",\n    AWE = \"AWE\",\n    T11_VLMR_PValue = \"VLMR\",\n    BLRT_PValue = \"BLRT\",\n    Smallest_Class_Combined = \"n (%)\"\n  ) %>%\n  tab_footnote(\n    footnote = md(\n      \"*Note.* Par = Parameters; *LL* = model log likelihood;\nBIC = Bayesian information criterion;\naBIC = sample size adjusted BIC; CAIC = consistent Akaike information criterion;\nAWE = approximate weight of evidence criterion;\nBLRT = bootstrapped likelihood ratio test p-value;\nVLMR = Vuong-Lo-Mendell-Rubin adjusted likelihood ratio test p-value;\n*cmPk* = approximate correct model probability.\"\n    ),\nlocations = cells_title()\n  ) %>%\n  tab_options(column_labels.font.weight = \"bold\") %>%\n  fmt_number(\n    columns = c(3, 6:9), \n    decimals = 2\n  ) %>%\n  sub_missing(1:11,\n              missing_text = \"--\") %>%\n  fmt(\n    c(T11_VLMR_PValue, BLRT_PValue),\n    fns = function(x)\n      ifelse(x < 0.001, \"<.001\",\n             scales::number(x, accuracy = .01))\n  ) %>%\n  fmt_percent(\n    columns = c(Perc_Convergence, Replicated_LL_Perc),\n    decimals = 0,\n    scale_values = FALSE\n  ) %>%\n  \n  cols_align(align = \"center\", columns = everything()) %>%  \n  tab_style(\n    style = list(cell_text(weight = \"bold\")),\n    locations = list(\n      cells_body(columns = BIC, row = BIC == min(BIC)),\n      cells_body(columns = aBIC, row = aBIC == min(aBIC)),\n      cells_body(columns = CAIC, row = CAIC == min(CAIC)),\n      cells_body(columns = AWE, row = AWE == min(AWE)),\n      cells_body(columns = T11_VLMR_PValue, \n                 row = ifelse(T11_VLMR_PValue < .05 & lead(T11_VLMR_PValue) > .05, T11_VLMR_PValue < .05, NA)),\n      cells_body(columns = BLRT_PValue, \n                 row = ifelse(BLRT_PValue < .05 & lead(BLRT_PValue) > .05, BLRT_PValue < .05, NA))\n    )\n  )\n\nfit_table1\ngtsave(fit_table1, here(\"figures\", \"fit_table.png\"))"},{"path":"enumeration.html","id":"information-criteria-plot","chapter":"3 Enumeration","heading":"3.7 Information Criteria Plot","text":"Save figure","code":"\nallFit %>%\n  dplyr::select(2:7) %>%\n  rowid_to_column() %>%\n  pivot_longer(`BIC`:`AWE`,\n               names_to = \"Index\",\n               values_to = \"ic_value\") %>%\n  mutate(Index = factor(Index,\n                        levels = c (\"AWE\", \"CAIC\", \"BIC\", \"aBIC\"))) %>%\n  ggplot(aes(\n    x = rowid,\n    y = ic_value,\n    color = Index,\n    shape = Index,\n    group = Index,\n    lty = Index\n  )) +\n  geom_point(size = 2.0) + geom_line(linewidth = .8) +\n  scale_x_continuous(breaks = 1:nrow(allFit)) +\n  scale_colour_grey(end = .5) +\n  theme_cowplot() +\n  labs(x = \"Number of Classes\", y = \"Information Criteria Value\", title = \"Information Criteria\") +\n  theme(\n    text = element_text(family = \"serif\", size = 12),\n    legend.text = element_text(family=\"serif\", size=12),\n    legend.key.width = unit(3, \"line\"),\n    legend.title = element_blank(),\n    legend.position = \"top\"  \n  )\nggsave(here(\"figures\", \"info_criteria.png\"), dpi=300, height=5, width=7, units=\"in\")"},{"path":"enumeration.html","id":"compare-class-solutions","chapter":"3 Enumeration","heading":"3.8 Compare Class Solutions","text":"Compare probability plots \\(K = 1:6\\) class solutionsSave figure:","code":"\nmodel_results <- data.frame()\n\nfor (i in 1:length(output_enum)) {\n  \n  temp <- output_enum[[i]]$parameters$probability.scale %>%                                       \n    mutate(model = paste(i,\"-Class Model\"))                                                  \n  \n  model_results <- rbind(model_results, temp)\n}\n\nrm(temp)\n\ncompare_plot <-\n  model_results %>%\n  filter(category == 2) %>%\n  dplyr::select(est, model, LatentClass, param) %>%\n  mutate(param = as.factor(str_to_lower(param))) \n\ncompare_plot$param <- fct_inorder(compare_plot$param)\n\nggplot(\n  compare_plot,\n  aes(\n    x = param,\n    y = est,\n    color = LatentClass,\n    shape = LatentClass,\n    group = LatentClass,\n    lty = LatentClass\n  )\n) +\n  geom_point() + \n  geom_line() +\n  scale_colour_viridis_d() +\n  facet_wrap( ~ model, ncol = 2) +\n  labs(title = \"Bullying Items\",\n       x = \" \", y = \"Probability\") +\n  theme_minimal() +\n  theme(panel.grid.major.y = element_blank(),\n                          axis.text.x = element_text(angle = -45, hjust = -.1))                            \nggsave(here(\"figures\", \"compare_kclass_plot.png\"), dpi=300, height=5, width=7, units=\"in\")"},{"path":"item-probability-plots.html","id":"item-probability-plots","chapter":"4 Item Probability Plots","heading":"4 Item Probability Plots","text":"Example: Bullying SchoolsTo demonstrate mixture modeling training program online resource components IES grant utilize Civil Rights Data Collection (CRDC)9 data repository.\nCRDC federally mandated school-level data collection effort occurs every year.\npublic data currently available selected latent class indicators across 4 years (2011, 2013, 2015, 2017) US states.\nexample, use Arizona state sample.\nutilize six focal indicators constitute latent class model example; three variables report harassment/bullying schools based disability, race, sex, three variables full-time equivalent school staff hires (counselor, psychologist, law enforcement).\ndata source also includes covariates variety subjects distal outcomes reported 2018 math/reading assessments graduation rates.Load packages","code":"\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(glue)\nlibrary(MplusAutomation)\nlibrary(here)\nlibrary(janitor)\nlibrary(gt)\nlibrary(cowplot)\nlibrary(DiagrammeR) "},{"path":"item-probability-plots.html","id":"variable-description-1","chapter":"4 Item Probability Plots","heading":"4.1 Variable Description","text":"Variables transformed dichotomous indicators using following coding strategyHarassment bullying count variables recoded 1 school reported least one incident harassment (0 indicates reported incidents).\noriginal scale reported CDRC staff variables full time equivalent employees (FTE) represented 1 part time employees represented values 1 0.\nSchools greater one staff designated type represented values greater 1.\nvalues greater zero recorded 1s (e.g., .5, 1,3) indicating school staff present campus least part time.\nSchools staff designated type indicated 0 dichotomous variable.","code":""},{"path":"item-probability-plots.html","id":"prepare-data-1","chapter":"4 Item Probability Plots","heading":"4.2 Prepare Data","text":"","code":"\ndf_bully <- read_csv(here(\"data\", \"crdc_lca_data.csv\")) %>% \n  clean_names() %>% \n  dplyr::select(report_dis, report_race, report_sex, counselors_fte, psych_fte, law_fte) "},{"path":"item-probability-plots.html","id":"descriptive-statistics-1","chapter":"4 Item Probability Plots","heading":"4.3 Descriptive Statistics","text":"Save image","code":"\ndframe <- df_bully %>%\n  pivot_longer(\n    c(report_dis, report_race, report_sex, counselors_fte, psych_fte, law_fte),\n    names_to = \"Variable\"\n  ) %>%\n  group_by(Variable) %>%\n  summarise(\n    Count = sum(value == 1, na.rm = TRUE),\n    Total = n(),\n    .groups = \"drop\"\n  ) %>%\n  mutate(`Proportion Endorsed` = round(Count / Total, 3)) %>%\n  select(Variable, `Proportion Endorsed`, Count)\n\ngt(dframe) %>%\n  tab_header(\n    title = md(\"**LCA Indicator Endorsement**\"),\n    subtitle = md(\"&nbsp;\")\n  ) %>%\n  tab_options(\n    column_labels.font.weight = \"bold\",\n    row_group.font.weight = \"bold\"\n  )\ngtsave(prop_table, here(\"figures\", \"prop_table.png\"))"},{"path":"item-probability-plots.html","id":"class-probability-plot","chapter":"4 Item Probability Plots","heading":"4.4 3-Class Probability Plot","text":"Continuing example bookdown, use plot_lca function provided folder plot item probability plot. function requires one argument:\n- model_name: name Mplus readModels object (e.g., output_enum$c3_bully.)Save figure:","code":"\nsource(here(\"functions\", \"plot_lca.R\"))\n\n# Extract outputs from the enumeration folder\noutput_enum <- readModels(here(\"enum\"), filefilter = \"bully\", quiet = TRUE)\n\nplot_lca(model_name = output_enum$c3_bully.out)\nggsave(here(\"figures\", \"C3_bully_LCA_Plot.png\"), dpi=\"retina\", height=5, width=7, units=\"in\")"},{"path":"enumeration-2.html","id":"enumeration-2","chapter":"5 Enumeration","heading":"5 Enumeration","text":"Polytomous LCA deals variables two categories, survey questions responses like never, sometimes, always. workflow polytomous LCA model similar LCA model binary indicators. However, polytomous LCA captures complex response patterns, can make interpretation complex. following code demonstrates example, along visualization model.","code":""},{"path":"enumeration-2.html","id":"example-elections","chapter":"5 Enumeration","heading":"5.1 Example: Elections","text":"“Two sets six questions four responses , asking respondents’ opinions well various traits describe presidential candidates Al Gore George W. Bush. Also potential covariates vote choice, age, education, gender, party ID. Source: National Election Studies (2000).” (poLCA, 2016) See documentation hereTwo sets six questions four responses , asking respondents’ opinions well various traits describe presidential candidates Al Gore George W. Bush. election data set, respondents 2000 American National Election Study public opinion poll asked evaluate well series traits—moral, caring, knowledgeable, good leader, dishonest, intelligent—described presidential candidates Al Gore George W. Bush. question four possible choices: (1) extremely well; (2) quite well; (3) well; (4) well .Load packages","code":"\n\nlibrary(poLCA)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(gt)\nlibrary(MplusAutomation)\nlibrary(here)\nlibrary(glue)"},{"path":"enumeration-2.html","id":"prepare-data-2","chapter":"5 Enumeration","heading":"5.2 Prepare Data","text":"","code":"\ndata(election)\n\n# Detaching packages that mask the dpylr functions \ndetach(package:poLCA, unload = TRUE)\ndetach(package:MASS, unload = TRUE)\n\ndf_election <-  election %>% \n  clean_names() %>% \n  select(moralb:intelb) %>% \n  mutate(across(everything(), \n                ~ as.factor(as.numeric(gsub(\"\\\\D\", \"\", .))), \n                .names = \"{.col}1\")) \n\n# Quick summary\nsummary(df_election)\n#>                moralb                  caresb   \n#>  1 Extremely well :340   1 Extremely well :155  \n#>  2 Quite well     :841   2 Quite well     :625  \n#>  3 Not too well   :330   3 Not too well   :562  \n#>  4 Not well at all: 98   4 Not well at all:342  \n#>  NA's             :176   NA's             :101  \n#>                knowb                   leadb    \n#>  1 Extremely well :274   1 Extremely well :266  \n#>  2 Quite well     :933   2 Quite well     :842  \n#>  3 Not too well   :379   3 Not too well   :407  \n#>  4 Not well at all:133   4 Not well at all:166  \n#>  NA's             : 66   NA's             :104  \n#>               dishonb                  intelb    moralb1   \n#>  1 Extremely well : 70   1 Extremely well :329   1   :340  \n#>  2 Quite well     :288   2 Quite well     :967   2   :841  \n#>  3 Not too well   :653   3 Not too well   :306   3   :330  \n#>  4 Not well at all:574   4 Not well at all:110   4   : 98  \n#>  NA's             :200   NA's             : 73   NA's:176  \n#>  caresb1     knowb1     leadb1    dishonb1   intelb1   \n#>  1   :155   1   :274   1   :266   1   : 70   1   :329  \n#>  2   :625   2   :933   2   :842   2   :288   2   :967  \n#>  3   :562   3   :379   3   :407   3   :653   3   :306  \n#>  4   :342   4   :133   4   :166   4   :574   4   :110  \n#>  NA's:101   NA's: 66   NA's:104   NA's:200   NA's: 73"},{"path":"enumeration-2.html","id":"descriptive-statistics-2","chapter":"5 Enumeration","heading":"5.3 Descriptive Statistics","text":"","code":"\nds <- df_election %>% \n  pivot_longer(moralb1:intelb1, names_to = \"variable\") %>% \n  count(variable, value) %>%  # Count occurrences of each value for each variable\n  group_by(variable) %>%\n  mutate(prop = n / sum(n)) %>% \n  arrange(desc(variable))\n\n# Create the table\nprop_table <- ds %>% \n  gt() %>% \n  tab_header(title = md(\"**Descriptive Summary**\")) %>%\n  cols_label(\n    variable = \"Variable\",\n    n = md(\"*N*\"),\n    prop = md(\"Proportion\")\n  ) %>%\n  fmt_number(c(\"n\", \"prop\"), decimals = 2) %>%  # Format both n and prop columns\n  cols_align(\n    align = \"center\",\n    columns = c(prop, n)\n  ) \n\n# View the table\nprop_table\n\n# Save as a Word doc\n#gtsave(prop_table, here(\"figures\", \"prop_table.docx\"))"},{"path":"enumeration-2.html","id":"enumeration-3","chapter":"5 Enumeration","heading":"5.4 Enumeration","text":"code uses mplusObject function MplusAutomation package.","code":"\n\nlca_enumeration  <- lapply(1:6, function(k) {\n  lca_enum  <- mplusObject(\n      \n    TITLE = glue(\"{k}-Class\"), \n  \n    VARIABLE = glue(\n    \"categorical = moralb1-intelb1; \n     usevar = moralb1-intelb1;\n     classes = c({k}); \"),\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 500 100; \n    processors = 10;\",\n  \n  OUTPUT = \"sampstat residual tech11 tech14 svalues;\",\n\n  \n  usevariables = colnames(df_election),\n  rdata = df_election)\n\nlca_enum_fit <- mplusModeler(lca_enum, \n                            dataout=glue(here(\"poLCA\", \"election.dat\")),\n                            modelout=glue(here(\"poLCA\", \"c{k}_election.inp\")) ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"enumeration-2.html","id":"table-of-fit-1","chapter":"5 Enumeration","heading":"5.4.1 Table of Fit","text":"Save table:","code":"\nsource(here(\"functions\",\"enum_table.R\"))\n\noutput_election <- readModels(here(\"poLCA\"), filefilter = \"election\", quiet = TRUE)\n\n# To see rows:\n#seeRows(output_election)\n\n# Arguments for `enum_table`\n# 1. readModels objects\n# 2-5. Rows of successfully estimated models \nfit_table <- enum_table(output_election, 1:6)\nfit_table\ngtsave(fit_table, here(\"figures\", \"fit_table.png\"))"},{"path":"enumeration-2.html","id":"information-criteria-plot-1","chapter":"5 Enumeration","heading":"5.4.2 Information Criteria Plot","text":"Save figure:","code":"\nic_plot(output_election)\nggsave(here(\"figures\", \"info_criteria.png\"), dpi=\"retina\", bg = \"white\", height=5, width=7, units=\"in\")"},{"path":"enumeration-2.html","id":"class-probability-plot-1","chapter":"5 Enumeration","heading":"5.4.3 4-Class Probability Plot","text":"functions poLCA_stacked poLCA_grouped create visualizations class probabilities LCA polytomous indicators. function takes following arguments:model_name: LCA model read R using readModels function MplusAutomation package.category_labels: character vector category labels response options (e.g., survey answers).Note: Double check labels correct order!Alternative plot","code":"\nsource(here(\"functions\",\"poLCA_plot.R\"))\n\npoLCA_stacked(output_election$c5_election.out, category_labels = c(\"1\" = \"1: Extremely well\", \n                                                                   \"2\" = \"2: Quite Well\", \n                                                                   \"3\" = \"3: Not Too Well\", \n                                                                   \"4\" = \"4: Not Well at All\"))\npoLCA_grouped(output_election$c5_election.out, category_labels = c(\"1\" = \"1: Extremely well\", \n                                                                   \"2\" = \"2: Quite Well\", \n                                                                   \"3\" = \"3: Not Too Well\", \n                                                                   \"4\" = \"4: Not Well at All\"))"},{"path":"enumeration-2.html","id":"apa-formatted-plot","chapter":"5 Enumeration","heading":"5.4.4 APA-formatted Plot","text":"","code":"\n# Model \nmodel <- output_election$c5_election.out\n\n\n# Title\ntitle <- \"2000 Descriptions of Presidential Candidate George W. Bush; Item Probabilities by Class\"\n\n# Item names\nitem_labels <- c(\"CARESB1\" = \"Caring\",\n                 \"DISHONB1\" = \"Dishonest\",\n                 \"INTELB1\" = \"Intelligent\",\n                 \"KNOWB1\" = \"Knowledgeable\",\n                 \"LEADB1\" = \"Good Leader\",\n                 \"MORALB1\" = \"Moral\")\n\n# Item Category\ncategory_labels <- c(\"1\" = \"1: Extremely well\", \n                     \"2\" = \"2: Quite Well\", \n                     \"3\" = \"3: Not Too Well\", \n                     \"4\" = \"4: Not Well at All\")\n\n# Class labels\nclass_labels <- c(\"1\" = \"Poor Decsription (9.95%)\",\n                  \"2\" = \"Mostly Poor Description (22.40%)\",\n                  \"3\" = \"In-Between (24.06%)\",\n                  \"4\" = \"Mostly Well-Described But Not Intelligent (28.29%)\",\n                  \"5\" = \"Well-Described But Not Intelligent (15.30%)\")\n\n\n#### END EDIT ####"},{"path":"enumeration-2.html","id":"extract-data-needed-for-plotting","chapter":"5 Enumeration","heading":"5.4.4.1 Extract data needed for plotting","text":"","code":"\n# Extract data needed for plotting \nplot_data <- data.frame(model$parameters$probability.scale) %>%\n  dplyr::select(est, LatentClass, param, category) %>%\n  mutate(\n    items = factor(param, labels = item_labels),\n    class = factor(LatentClass, labels = class_labels),\n    cat = factor(category, labels = category_labels)\n  ) %>% \n  mutate(class = factor(class, levels = rev(levels(factor(class))))) "},{"path":"enumeration-2.html","id":"final-grouped-bar-plot","chapter":"5 Enumeration","heading":"5.4.4.2 Final grouped bar plot","text":"Save figure:","code":"\n\n## Plot data\nplot_data %>%\n  ggplot(aes(\n    x = items,\n    y = est,\n    fill = cat,\n    group = cat\n  )) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_text(aes(label = sub(\"^0\\\\.\", \".\", sprintf(\"%.2f\", est))), \n            position = position_dodge(width = 0.9), \n            vjust = -0.5, size = 3) +\n  facet_wrap(~ class) + \n  ylim(0, 1) +\n  scale_x_discrete(\n    \"\",\n    labels = function(x)\n      str_wrap(x, width = 10)\n  ) +\n  labs(title = \"Figure 1\",\n       subtitle = title,\n       y = \"Probability\") +\n  theme_bw(12) +\n  scale_fill_grey(start = 0.8, end = 0.2) + # Gives different shades\n  theme(\n    text = element_text(family = \"sans\", size = 12),\n    legend.text = element_text(family = \"sans\", size = 12, color = \"black\"),\n    legend.title = element_blank(),\n    legend.position = \"bottom\",\n    legend.justification = \"center\", \n    axis.text.x = element_text(vjust = 1),\n    plot.subtitle = element_text(face = \"italic\", size = 15),\n    plot.title = element_text(size = 15),\n    strip.background = element_rect(fill = \"grey90\", color = \"black\", size = 1),\n    strip.text = element_text(size = 12)\n  ) \nggsave(here(\"figures\", \"APA_plot1.png\"), dpi=\"retina\", bg = \"white\", height=9, width=15, units=\"in\")"},{"path":"enumeration-2.html","id":"alternative","chapter":"5 Enumeration","heading":"5.4.4.3 Alternative","text":"Save figure:","code":"\n## Plot data\nplot_data %>%\n  ggplot(aes(\n    x = items,\n    y = est,\n    fill = cat,\n    group = cat\n  )) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_text(aes(label = sub(\"^0\\\\.\", \".\", sprintf(\"%.2f\", est))), \n            position = position_dodge(width = 0.9), \n            vjust = -0.5, size = 3) +\n  facet_wrap(~ class) + \n  ylim(0, 1) +\n  scale_x_discrete(\n    \"\",\n    labels = function(x)\n      str_wrap(x, width = 10)\n  ) +\n  labs(title = \"Figure 1\",\n       subtitle = title,\n       y = \"Probability\") +\n  theme_cowplot(12) +\n  scale_fill_grey(start = 0.8, end = 0.2) + # Gives different shades\n  theme(\n    text = element_text(family = \"sans\", size = 12),\n    legend.text = element_text(family = \"sans\", size = 12, color = \"black\"),\n    legend.title = element_blank(),\n    legend.position = \"bottom\",\n    legend.justification = \"center\", \n    axis.text.x = element_text(vjust = 1),\n    plot.subtitle = element_text(face = \"italic\", size = 15),\n    plot.title = element_text(size = 15),\n    strip.background = element_rect(fill = \"grey90\", color = \"black\", size = 1),\n    strip.text = element_text(size = 12)\n  )\nggsave(here(\"figures\", \"APA_plot2.png\"), dpi=\"retina\", bg = \"white\", height=10, width=17, units=\"in\")"},{"path":"enumeration-4.html","id":"enumeration-4","chapter":"6 Enumeration","heading":"6 Enumeration","text":"Example: PISA Student DataThe first example closely follows vignette used demonstrate tidyLPA package (Rosenberg, 2019).model utilizes PISA data collected U.S. 2015. learn data see .access 2015 US PISA data & documentation R use following code:Variables:broad_interest\ncomposite measure students’ self reported broad interest\ncomposite measure students’ self reported broad interestenjoyment\ncomposite measure students’ self reported enjoyment\ncomposite measure students’ self reported enjoymentinstrumental_mot\ncomposite measure students’ self reported instrumental motivation\ncomposite measure students’ self reported instrumental motivationself_efficacy\ncomposite measure students’ self reported self efficacy\ncomposite measure students’ self reported self efficacy","code":"\n#devtools::install_github(\"jrosen48/pisaUSA15\")\n#library(pisaUSA15)"},{"path":"enumeration-4.html","id":"latent-profile-models","chapter":"6 Enumeration","heading":"6.1 Latent Profile Models","text":"Latent Profile Analysis (LPA) statistical modeling approach estimating distinct profiles variables.\nsocial sciences educational research, profiles represent, example, different youth experience dimensions engaged (.e., cognitively, behaviorally, affectively) time.\nNote LPA works best continuous variables (, cases, ordinal variables), appropriate dichotomous (binary) variables.Many analysts carried LPA using latent variable modeling approach.\napproach, different parameters - means, variances, covariances - freely estimated across profiles, fixed across profiles, constrained zero.\nMPlus software commonly used estimate models (see ) using expectation-maximization (EM) algorithm obtain maximum likelihood estimates parameters.Different models (whether parameters estimated) can specified estimated.\nMPlus widely-used (powerful), costly, closed-source, can difficult use, particularly respect interpreting using output specified models part reproducible workflow.","code":""},{"path":"enumeration-4.html","id":"terminology-for-specifying-variance-covariance-matrix","chapter":"6 Enumeration","heading":"6.2 Terminology for specifying variance-covariance matrix","text":"code used estimate LPA models walkthrough tidyLPA package.\nTidyLPA(source) R package designed estimate latent profile models using tidy framework.\ncan interface Mplus via MplusAutomation package, enabling estimation latent profile models different variance-covariance structures.model 1 Profile-invariant / Diagonal: Equal variances, covariances fixed 0model 2 Profile-varying / Diagonal: Free variances covariances fixed 0model 3 Profile-invariant / Non-Diagonal: Equal variances equal covariances\nNote: alternative Model 3 freely estimating covariances\nNote: alternative Model 3 freely estimating covariancesmodel 4 Free variances, equal covariancesmodel 5 Equal variances, free covariancesmodel 6 Profile Varying / Non-Diagonal: Free variances free covariances","code":""},{"path":"enumeration-4.html","id":"model-1","chapter":"6 Enumeration","heading":"6.2.1 Model 1","text":"Profile-invariant/diagonal:Equal Variances: Variances fixed equality across profiles (.e., variances constrained equal profile).Equal Variances: Variances fixed equality across profiles (.e., variances constrained equal profile).Covariances fixed zero (.e., -diagonal cells matrix zero).Covariances fixed zero (.e., -diagonal cells matrix zero).parsimonious model restricted.\\[\n\\begin{pmatrix}\n\\sigma^2_1 & 0 & 0 \\\\\n0 & \\sigma^2_2 & 0 \\\\\n0 & 0 & \\sigma^2_3 \\\\\n\\end{pmatrix}\n\\]","code":""},{"path":"enumeration-4.html","id":"model-2","chapter":"6 Enumeration","heading":"6.2.2 Model 2","text":"Profile-varying/diagonal:Free variances: Variances parameters freely estimated across profiles (.e., variances vary profile).Free variances: Variances parameters freely estimated across profiles (.e., variances vary profile).Covariances fixed zero (.e., -diagonal cells matrix zero).Covariances fixed zero (.e., -diagonal cells matrix zero).model flexible less parsimonious model 1.\\[\n\\begin{pmatrix}\n\\sigma^2_{1p} & 0 & 0 \\\\\n0 & \\sigma^2_{2p} & 0 \\\\\n0 & 0 & \\sigma^2_{3p} \\\\\n\\end{pmatrix}\n\\]","code":""},{"path":"enumeration-4.html","id":"model-3","chapter":"6 Enumeration","heading":"6.2.3 Model 3","text":"Profile-invariant/ non-diagonal unrestricted:Equal variances: Variances fixed equality across profile.\n(.e., variances constrained profile).Equal variances: Variances fixed equality across profile.\n(.e., variances constrained profile).Equal Covariances: covariances now estimated constrained equal.\nalternative Model 3 freely estimating covariances (Model 5 ).\nEqual Covariances: covariances now estimated constrained equal.alternative Model 3 freely estimating covariances (Model 5 ).\\[\n\\begin{pmatrix}\n\\sigma^2_1 & \\sigma_{12} & \\sigma_{13} \\\\\n\\sigma_{12} & \\sigma^2_2 & \\sigma_{23} \\\\\n\\sigma_{13} & \\sigma_{23} & \\sigma^2_3 \\\\\n\\end{pmatrix}\n\\]","code":""},{"path":"enumeration-4.html","id":"model-4","chapter":"6 Enumeration","heading":"6.2.4 Model 4","text":"Varying means, varying variances, equal covariances:Free variances: Variances parameters freely estimated across profiles (.e., variances vary profile).Free variances: Variances parameters freely estimated across profiles (.e., variances vary profile).Equal Covariances: Covariances constrained equal.Equal Covariances: Covariances constrained equal.model also considered extension Model 3.\\[\n\\begin{pmatrix}\n\\sigma^2_{1p} & \\sigma_{12} & \\sigma_{13} \\\\\n\\sigma_{12} & \\sigma^2_{2p} & \\sigma_{23} \\\\\n\\sigma_{13} & \\sigma_{23} & \\sigma^2_{3p} \\\\\n\\end{pmatrix}\n\\]","code":""},{"path":"enumeration-4.html","id":"model-5","chapter":"6 Enumeration","heading":"6.2.5 Model 5","text":"Varying means, equal variances, varying covariances:Equal variances: Variances fixed equality across profiles.\n(.e., variances constrained profile).Equal variances: Variances fixed equality across profiles.\n(.e., variances constrained profile).Free Covariances: Covariances now freely estimated across profiles.Free Covariances: Covariances now freely estimated across profiles.model also considered extension Model 3.\\[\n\\begin{pmatrix}\n\\sigma^2_{1} & \\sigma_{12p} & \\sigma_{13p} \\\\\n\\sigma_{12p} & \\sigma^2_{2} & \\sigma_{23p} \\\\\n\\sigma_{13p} & \\sigma_{23p} & \\sigma^2_{3} \\\\\n\\end{pmatrix}\n\\]","code":""},{"path":"enumeration-4.html","id":"model-6","chapter":"6 Enumeration","heading":"6.2.6 Model 6","text":"Profile-varying / Non-diagonal:Free variances: Variances parameters freely estimated across profiles (.e., variances vary profile).Free variances: Variances parameters freely estimated across profiles (.e., variances vary profile).Free Covariances: Covariances now freely estimated across profiles.Free Covariances: Covariances now freely estimated across profiles.complex unrestricted model.\nalso least parsimoniousNote: unrestricted model also sometimes known Model 4.\\[\n\\begin{pmatrix}\n\\sigma^2_{1p} & \\sigma_{12p} & \\sigma_{13p} \\\\\n\\sigma_{12p} & \\sigma^2_{2p} & \\sigma_{23p} \\\\\n\\sigma_{13p} & \\sigma_{23p} & \\sigma^2_{3p} \\\\\n\\end{pmatrix}\n\\]","code":""},{"path":"enumeration-4.html","id":"load-packages","chapter":"6 Enumeration","heading":"6.3 Load packages","text":"","code":"\nlibrary(naniar)\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(glue)\nlibrary(MplusAutomation)\nlibrary(here)\nlibrary(janitor)\nlibrary(gt)\nlibrary(tidyLPA)\nlibrary(pisaUSA15)\nlibrary(cowplot)\nlibrary(filesstrings)\nlibrary(patchwork)\nlibrary(RcppAlgos)"},{"path":"enumeration-4.html","id":"prepare-data-3","chapter":"6 Enumeration","heading":"6.4 Prepare Data","text":"","code":"\n\npisa <- pisaUSA15[1:500,] %>%\n  dplyr::select(broad_interest, enjoyment, instrumental_mot, self_efficacy)"},{"path":"enumeration-4.html","id":"descriptive-statistics-3","chapter":"6 Enumeration","heading":"6.5 Descriptive Statistics","text":"Quick SummaryMean TableHistograms","code":"\nsummary(pisa)\n#>  broad_interest    enjoyment    instrumental_mot\n#>  Min.   :1.000   Min.   :1.00   Min.   :1.000   \n#>  1st Qu.:2.200   1st Qu.:2.40   1st Qu.:1.750   \n#>  Median :2.800   Median :3.00   Median :2.000   \n#>  Mean   :2.666   Mean   :2.82   Mean   :2.129   \n#>  3rd Qu.:3.200   3rd Qu.:3.00   3rd Qu.:2.500   \n#>  Max.   :5.000   Max.   :4.00   Max.   :4.000   \n#>  NA's   :23      NA's   :14     NA's   :21      \n#>  self_efficacy  \n#>  Min.   :1.000  \n#>  1st Qu.:1.750  \n#>  Median :2.000  \n#>  Mean   :2.125  \n#>  3rd Qu.:2.500  \n#>  Max.   :4.000  \n#>  NA's   :23\nds <- pisa %>% \n  pivot_longer(broad_interest:self_efficacy, names_to = \"variable\") %>% \n  group_by(variable) %>% \n  summarise(mean = mean(value, na.rm = TRUE),\n            sd = sd(value, na.rm = TRUE)) \n\nds %>% \n  gt () %>% \n  tab_header(title = md(\"**Descriptive Summary**\")) %>%\n  cols_label(\n    variable = \"Variable\",\n    mean = md(\"M\"),\n    sd = md(\"SD\")\n  ) %>%\n  fmt_number(c(2:3),\n             decimals = 2) %>% \n  cols_align(\n    align = \"center\",\n    columns = mean\n  ) \ndata_long <- pisa %>%\n  pivot_longer(broad_interest:self_efficacy, names_to = \"variable\")\n\nggplot(data_long, aes(x = value)) +\n  geom_histogram(binwidth = .3, fill = \"#69b3a2\", color = \"black\") +\n  facet_wrap(~ variable, scales = \"free_x\") +\n  labs(title = \"Histograms of Variables\", x = \"Value\", y = \"Frequency\") +\n  theme_cowplot()"},{"path":"enumeration-4.html","id":"enumeration-5","chapter":"6 Enumeration","heading":"6.6 Enumeration","text":"","code":""},{"path":"enumeration-4.html","id":"tidylpa","chapter":"6 Enumeration","heading":"6.6.1 tidyLPA","text":"Enumerate using estimate_profiles():Estimate models profiles \\(K = 1:5\\)Model 4 continuous indicatorsDefault variance-covariance specifications (model 1)Change variances covariances indicate model want specify, example, estimating six models.","code":"\n\n# Run LPA models \nlpa_fit <- pisa %>% \n    estimate_profiles(1:5,\n                      package = \"MplusAutomation\",\n                      ANALYSIS = \"starts = 500 100;\",\n                      OUTPUT = \"sampstat residual tech11 tech14\",\n                      variances = c(\"equal\", \"varying\", \"equal\", \"varying\", \"equal\", \"varying\"),\n                      covariances = c(\"zero\", \"zero\", \"equal\", \"equal\", \"varying\", \"varying\"),\n                      keepfiles = TRUE)\n\n# Compare fit statistics\nget_fit(lpa_fit)\n\n\n# Move files to folder \nfiles <- list.files(here(), pattern = \"^model\")\nmove_files(files, here(\"lpa\", \"tidyLPA\"), overwrite = TRUE)"},{"path":"enumeration-4.html","id":"mplus","chapter":"6 Enumeration","heading":"6.6.2 Mplus","text":"Alternative method estimate_profiles(): Run enumeration using mplusObject methodYou can change model specification LPA using syntax provided lecture.","code":""},{"path":"enumeration-4.html","id":"model-1-1","chapter":"6 Enumeration","heading":"6.6.2.1 Model 1","text":"estimating LPA Mplus, default variance/covariance specification restricted model (Model 1).\ndon’t specify anything .","code":"\n\nlpa_k14  <- lapply(1:5, function(k) {\n  lpa_enum  <- mplusObject(\n      \n    TITLE = glue(\"Profile {k}\"), \n  \n    VARIABLE = glue(\n    \"usevar = broad_interest-self_efficacy;\n     classes = c({k}); \"),\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 500 100;\",\n  \n  OUTPUT = \"sampstat svalues residual tech11 tech14;\",\n  \n  usevariables = colnames(pisa),\n  rdata = pisa)\n\nlpa_enum_fit <- mplusModeler(lpa_enum, \n                dataout=glue(here(\"lpa\", \"enum_lpa\", \"lpa_pisa\")),\n                modelout=glue(here(\"lpa\", \"enum_lpa\", \"c{k}_lpa_m1.inp\")) ,\n                check=TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"enumeration-4.html","id":"model-2-1","chapter":"6 Enumeration","heading":"6.6.2.2 Model 2","text":", addition loop adds variance/covariance specifications class-specific statement.\nprofile-varying/diagonal specification, must specify variances freely estimated:broad_interest-self_efficacy;reference, Mplus syntax different specifications:Fixed covariance zero (DEFAULT):broad_interest enjoyment@0;Free covariance:broad_interest enjoyment;Equal covariances:%c#1%broad_interest enjoyment (1);%c#2%broad_interest enjoyment (1);Equal variance (DEFAULT):%c#1%broad_interest (1);%c#2%broad_interest (1);Free variance:mth_scor-bio_scor;can also open tidyLPA .inp files see specifications.","code":"\nlpa_m2_k14  <- lapply(1:5, function(k){ \n  \n  # This MODEL section changes the model specification\n  MODEL <- paste(sapply(1:k, function(i) {\n    glue(\"\n    %c#{i}%\n    broad_interest-self_efficacy;      ! variances are freely estimated\n    \")\n  }), collapse = \"\\n\")\n  \n  lpa_enum_m2  <- mplusObject(\n    TITLE = glue(\"Profile {k} - Model 2\"),\n    \n    VARIABLE = glue(\n      \"usevar = broad_interest-self_efficacy;\n     classes = c({k});\"),\n    \n    ANALYSIS = \n      \"estimator = mlr; \n    type = mixture;\n    starts = 500 100;\",\n    \n    MODEL = MODEL,\n    \n    \n    OUTPUT = \"sampstat svalues residual tech11 tech14;\",\n    \n    usevariables = colnames(pisa),\n    rdata = pisa)\n  \n  lpa_m2_fit <- mplusModeler(lpa_enum_m2,\n                             dataout = here(\"lpa\", \"enum_lpa\", \"lpa_pisa\"),\n                             modelout = glue(here(\"lpa\", \"enum_lpa\",\"c{k}_lpa_m2.inp\")),\n                             check = TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"enumeration-4.html","id":"table-of-fit-2","chapter":"6 Enumeration","heading":"6.7 Table of Fit","text":"Evaluate model specification separately using fit indices.examining outputs (increasing random starts necessary), models excluded analysis:Model 2:Profile 4Profile 4Profile 5Profile 5Model 3:Profile 5Model 4:Profile 4Profile 4Profile 5Profile 5Model 5:Profile 3Profile 3Profile 4Profile 4Profile 5Profile 5Model 6:Profile 4Profile 4Profile 5Profile 5","code":"\nsource(here(\"functions\",\"enum_table_lpa.R\"))\n\n# Read in model\noutput_enum <- readModels(here(\"lpa\", \"tidyLPA\"), quiet = TRUE)\n\n# Preview with numbered rows\nenum_fit(output_enum)\n#> # A tibble: 29 × 12\n#>      row Title     Parameters     LL   BIC  aBIC  CAIC   AWE\n#>    <int> <chr>          <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl>\n#>  1     1 Model 1 …          8 -2089. 4227. 4201. 4235. 4300.\n#>  2     2 Model 1 …         13 -1997. 4074. 4032. 4087. 4193.\n#>  3     3 Model 1 …         18 -1953. 4017. 3960. 4035. 4183.\n#>  4     4 Model 1 …         23 -1889. 3921. 3848. 3944. 4133.\n#>  5     5 Model 1 …         28 -1871. 3915. 3826. 3943. 4172.\n#>  6     6 Model 2 …          8 -2089. 4227. 4201. 4235. 4300.\n#>  7     7 Model 2 …         17 -1989. 4083. 4029. 4100. 4239.\n#>  8     8 Model 2 …         26 -1878. 3917. 3834. 3943. 4156.\n#>  9     9 Model 2 …         35 -1851. 3919. 3808. 3954. 4241.\n#> 10    10 Model 2 …         44 -1825. 3923. 3783. 3967. 4327.\n#> # ℹ 19 more rows\n#> # ℹ 4 more variables: BLRT_PValue <dbl>,\n#> #   T11_VLMR_PValue <dbl>, BF <dbl>, cmPk <dbl>\n\n\nselect_models <-LatexSummaryTable(output_enum,                                 \n                keepCols=c(\"Title\", \"Parameters\", \"LL\", \"BIC\", \"aBIC\",\n                           \"BLRT_PValue\", \"T11_VLMR_PValue\",\"Observations\")) %>% \n  slice( # Remove the models that we don't want to consider!!!! Because we looked at every single output, we know which models did not converge, thus we exclude them.\n    # Model 2\n    -9, -10, \n    # Model 3\n    -15,\n    # Model 4\n    -19, -20,\n    # Model 5\n    -23, -24, -25,\n    # Model 6\n    -29, -30\n  )\n\n# Check to make sure that the rows of the models we don't want are removed\n#View(select_models)\n\nenum_table(select_models, 1:5, 6:8, 9:12, 13:15, 16:17, 18:20)"},{"path":"enumeration-4.html","id":"information-criteria-plot-2","chapter":"6 Enumeration","heading":"6.8 Information Criteria Plot","text":"Look “elbow” help profile selectionBased fit indices, choosing following candidate models:Model 1: 2 ProfileModel 2: 3 ProfileModel 3: 4 ProfileModel 4: 3 ProfileModel 5: 2 ProfileModel 6: 2 Profile","code":"\nsource(here(\"functions\",\"ic_plot_lpa.R\"))\nic_plot(select_models)"},{"path":"enumeration-4.html","id":"compare-models","chapter":"6 Enumeration","heading":"6.9 Compare models","text":"","code":""},{"path":"enumeration-4.html","id":"correct-model-probability-cmpk-recalculation","chapter":"6 Enumeration","heading":"6.9.1 Correct Model Probability (cmpK) recalculation","text":"Take candidate models recalculate approximate correct model probabilities (Masyn, 2013)","code":"\n# CmpK recalculation:\nenum_fit1 <- enum_fit(output_enum)\n\nstage2_cmpk <- enum_fit1 %>% \n  slice(2, 8, 14, 18, 22, 27) %>% \n  mutate(SIC = -.5 * BIC,\n         expSIC = exp(SIC - max(SIC)),\n         cmPk = expSIC / sum(expSIC),\n         BF = exp(SIC - lead(SIC))) %>% \n  select(Title, Parameters, BIC:AWE, cmPk, BF)\n\n \n# Format Fit Table\nstage2_cmpk %>%\n  gt() %>% \n  tab_options(column_labels.font.weight = \"bold\") %>%\n  fmt_number(\n    7,\n    decimals = 2,\n    drop_trailing_zeros = TRUE,\n    suffixing = TRUE\n  ) %>%\n  fmt_number(c(3:6),\n             decimals = 2) %>% \n    fmt_number(8,decimals = 2,\n             drop_trailing_zeros=TRUE,\n             suffixing = TRUE) %>% \n  fmt(8, fns = function(x) \n    ifelse(x>100, \">100\",\n           scales::number(x, accuracy = .1))) %>% \n  tab_style(\n    style = list(\n      cell_text(weight = \"bold\")\n      ),\n    locations = list(cells_body(\n     columns = BIC,\n     row = BIC == min(BIC[1:nrow(stage2_cmpk)]) \n    ),\n    cells_body(\n     columns = aBIC,\n     row = aBIC == min(aBIC[1:nrow(stage2_cmpk)])\n    ),\n    cells_body(\n     columns = CAIC,\n     row = CAIC == min(CAIC[1:nrow(stage2_cmpk)])\n    ),\n    cells_body(\n     columns = AWE,\n     row = AWE == min(AWE[1:nrow(stage2_cmpk)])\n    ),\n    cells_body(\n     columns = cmPk,\n     row =  cmPk == max(cmPk[1:nrow(stage2_cmpk)])\n     ),\n    cells_body(\n     columns = BF, \n     row =  BF > 10)\n  )\n)"},{"path":"enumeration-4.html","id":"compare-loglikelihood","chapter":"6 Enumeration","heading":"6.9.2 Compare loglikelihood","text":"can also compare models using nested model testing directly MplusAutomation.\nNote can compare across models profiles must stay ., Model 1 (restricted, fewer parameters) nested Model 2.\nchi-square difference test, assuming nested models, shows significant improvement fit Model 2 Model 1, despite added parameters.","code":"\n# MplusAutomation Method using `compareModels` \n\ncompareModels(output_enum[[\"model_2_class_3.out\"]],\n  output_enum[[\"model_4_class_3.out\"]], diffTest = TRUE)\n#> \n#> ==============\n#> \n#> Mplus model comparison\n#> ----------------------\n#> \n#> ------\n#> Model 1:  C:\\Users\\dnajiarch\\Box\\lca-bookdown\\lpa\\tidyLPA/model_2_class_3.out \n#> Model 2:  C:\\Users\\dnajiarch\\Box\\lca-bookdown\\lpa\\tidyLPA/model_4_class_3.out \n#> ------\n#> \n#> Model Summary Comparison\n#> ------------------------\n#> \n#>              m1                     m2                    \n#> Title        model 2 with 3 classes model 4 with 3 classes\n#> Observations 488                    488                   \n#> Estimator    MLR                    MLR                   \n#> Parameters   26                     32                    \n#> LL           -1877.965              -1859.466             \n#> AIC          3807.929               3782.932              \n#> BIC          3916.877               3917.022              \n#> \n#>   MLR Chi-Square Difference Test for Nested Models Based on Loglikelihood\n#>   -----------------------------------------------------------------------\n#> \n#>   Difference Test Scaling Correction:  1.177167 \n#>   Chi-square difference:  31.4297 \n#>   Diff degrees of freedom:  6 \n#>   P-value:  0 \n#> \n#>   Note: The chi-square difference test assumes that these models are nested.\n#>   It is up to you to verify this assumption.\n#> \n#>   MLR Chi-Square Difference test for nested models\n#>   --------------------------------------------\n#> \n#>   Difference Test Scaling Correction:  \n#>   Chi-square difference:  \n#>   Diff degrees of freedom:  \n#>   P-value:  \n#> \n#> Note: The chi-square difference test assumes that these models are nested.\n#>   It is up to you to verify this assumption.\n#> \n#> =========\n#> \n#> Model parameter comparison\n#> --------------------------\n#>   Parameters present in both models\n#> =========\n#> \n#>   Approximately equal in both models (param. est. diff <= 1e-04)\n#>   ----------------------------------------------\n#>  paramHeader     param LatentClass m1_est m2_est . m1_se\n#>        Means ENJOYMENT           1  2.968  2.968 | 0.012\n#>  m2_se . m1_est_se m2_est_se . m1_pval m2_pval\n#>  0.021 |   238.242   143.432 |       0       0\n#> \n#> \n#>   Parameter estimates that differ between models (param. est. diff > 1e-04)\n#>   ----------------------------------------------\n#>    paramHeader      param                  LatentClass\n#>  BROAD_IN.WITH  ENJOYMENT                            1\n#>  BROAD_IN.WITH  ENJOYMENT                            2\n#>  BROAD_IN.WITH  ENJOYMENT                            3\n#>  BROAD_IN.WITH INSTRUMENT                            1\n#>  BROAD_IN.WITH INSTRUMENT                            2\n#>  BROAD_IN.WITH INSTRUMENT                            3\n#>  BROAD_IN.WITH SELF_EFFIC                            1\n#>  BROAD_IN.WITH SELF_EFFIC                            2\n#>  BROAD_IN.WITH SELF_EFFIC                            3\n#>  ENJOYMEN.WITH INSTRUMENT                            1\n#>  ENJOYMEN.WITH INSTRUMENT                            2\n#>  ENJOYMEN.WITH INSTRUMENT                            3\n#>  ENJOYMEN.WITH SELF_EFFIC                            1\n#>  ENJOYMEN.WITH SELF_EFFIC                            2\n#>  ENJOYMEN.WITH SELF_EFFIC                            3\n#>  INSTRUME.WITH SELF_EFFIC                            1\n#>  INSTRUME.WITH SELF_EFFIC                            2\n#>  INSTRUME.WITH SELF_EFFIC                            3\n#>          Means BROAD_INTE                            1\n#>          Means BROAD_INTE                            2\n#>          Means BROAD_INTE                            3\n#>          Means       C1#1 Categorical.Latent.Variables\n#>          Means       C1#2 Categorical.Latent.Variables\n#>          Means  ENJOYMENT                            2\n#>          Means  ENJOYMENT                            3\n#>          Means INSTRUMENT                            1\n#>          Means INSTRUMENT                            2\n#>          Means INSTRUMENT                            3\n#>          Means SELF_EFFIC                            1\n#>          Means SELF_EFFIC                            2\n#>          Means SELF_EFFIC                            3\n#>      Variances BROAD_INTE                            1\n#>      Variances BROAD_INTE                            2\n#>      Variances BROAD_INTE                            3\n#>      Variances  ENJOYMENT                            1\n#>      Variances  ENJOYMENT                            2\n#>      Variances  ENJOYMENT                            3\n#>      Variances INSTRUMENT                            1\n#>      Variances INSTRUMENT                            2\n#>      Variances INSTRUMENT                            3\n#>      Variances SELF_EFFIC                            1\n#>      Variances SELF_EFFIC                            2\n#>      Variances SELF_EFFIC                            3\n#>  m1_est m2_est . m1_se m2_se . m1_est_se m2_est_se .\n#>   0.000  0.010 | 0.000 0.008 |   999.000     1.300 |\n#>   0.000  0.010 | 0.000 0.008 |   999.000     1.300 |\n#>   0.000  0.010 | 0.000 0.008 |   999.000     1.300 |\n#>   0.000 -0.013 | 0.000 0.024 |   999.000    -0.530 |\n#>   0.000 -0.013 | 0.000 0.024 |   999.000    -0.530 |\n#>   0.000 -0.013 | 0.000 0.024 |   999.000    -0.530 |\n#>   0.000 -0.029 | 0.000 0.023 |   999.000    -1.290 |\n#>   0.000 -0.029 | 0.000 0.023 |   999.000    -1.290 |\n#>   0.000 -0.029 | 0.000 0.023 |   999.000    -1.290 |\n#>   0.000 -0.034 | 0.000 0.015 |   999.000    -2.252 |\n#>   0.000 -0.034 | 0.000 0.015 |   999.000    -2.252 |\n#>   0.000 -0.034 | 0.000 0.015 |   999.000    -2.252 |\n#>   0.000 -0.029 | 0.000 0.008 |   999.000    -3.702 |\n#>   0.000 -0.029 | 0.000 0.008 |   999.000    -3.702 |\n#>   0.000 -0.029 | 0.000 0.008 |   999.000    -3.702 |\n#>   0.000  0.056 | 0.000 0.020 |   999.000     2.777 |\n#>   0.000  0.056 | 0.000 0.020 |   999.000     2.777 |\n#>   0.000  0.056 | 0.000 0.020 |   999.000     2.777 |\n#>   2.912  2.926 | 0.049 0.060 |    59.366    49.094 |\n#>   3.205  3.264 | 0.086 0.126 |    37.165    25.960 |\n#>   2.257  2.159 | 0.082 0.114 |    27.478    18.918 |\n#>  -0.285  0.143 | 0.188 0.341 |    -1.517     0.418 |\n#>  -0.891 -1.085 | 0.183 0.271 |    -4.877    -4.013 |\n#>   3.808  3.948 | 0.048 0.016 |    80.133   251.321 |\n#>   2.305  2.271 | 0.072 0.149 |    32.038    15.214 |\n#>   2.042  1.991 | 0.082 0.051 |    24.923    39.148 |\n#>   1.735  1.801 | 0.092 0.110 |    18.769    16.401 |\n#>   2.357  2.400 | 0.068 0.093 |    34.505    25.743 |\n#>   2.072  2.086 | 0.057 0.044 |    36.269    47.523 |\n#>   1.724  1.758 | 0.067 0.076 |    25.718    23.035 |\n#>   2.330  2.294 | 0.049 0.074 |    47.308    30.997 |\n#>   0.262  0.279 | 0.056 0.053 |     4.671     5.292 |\n#>   0.405  0.384 | 0.104 0.222 |     3.887     1.729 |\n#>   0.594  0.578 | 0.083 0.099 |     7.189     5.844 |\n#>   0.010  0.051 | 0.003 0.019 |     3.283     2.630 |\n#>   0.060  0.011 | 0.016 0.004 |     3.701     3.089 |\n#>   0.397  0.448 | 0.044 0.070 |     9.011     6.373 |\n#>   0.358  0.312 | 0.119 0.061 |     3.008     5.143 |\n#>   0.636  0.797 | 0.134 0.178 |     4.738     4.479 |\n#>   0.560  0.654 | 0.069 0.069 |     8.137     9.507 |\n#>   0.298  0.328 | 0.038 0.032 |     7.771    10.250 |\n#>   0.309  0.377 | 0.048 0.067 |     6.484     5.626 |\n#>   0.435  0.449 | 0.045 0.054 |     9.689     8.370 |\n#>  m1_pval m2_pval\n#>  999.000   0.194\n#>  999.000   0.194\n#>  999.000   0.194\n#>  999.000   0.596\n#>  999.000   0.596\n#>  999.000   0.596\n#>  999.000   0.197\n#>  999.000   0.197\n#>  999.000   0.197\n#>  999.000   0.024\n#>  999.000   0.024\n#>  999.000   0.024\n#>  999.000   0.000\n#>  999.000   0.000\n#>  999.000   0.000\n#>  999.000   0.005\n#>  999.000   0.005\n#>  999.000   0.005\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.129   0.676\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.084\n#>    0.000   0.000\n#>    0.001   0.009\n#>    0.000   0.002\n#>    0.000   0.000\n#>    0.003   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#> \n#> \n#>   P-values that differ between models (p-value diff > 1e-04)\n#>   -----------------------------------\n#>    paramHeader      param                  LatentClass\n#>  BROAD_IN.WITH  ENJOYMENT                            1\n#>  BROAD_IN.WITH  ENJOYMENT                            2\n#>  BROAD_IN.WITH  ENJOYMENT                            3\n#>  BROAD_IN.WITH INSTRUMENT                            1\n#>  BROAD_IN.WITH INSTRUMENT                            2\n#>  BROAD_IN.WITH INSTRUMENT                            3\n#>  BROAD_IN.WITH SELF_EFFIC                            1\n#>  BROAD_IN.WITH SELF_EFFIC                            2\n#>  BROAD_IN.WITH SELF_EFFIC                            3\n#>  ENJOYMEN.WITH INSTRUMENT                            1\n#>  ENJOYMEN.WITH INSTRUMENT                            2\n#>  ENJOYMEN.WITH INSTRUMENT                            3\n#>  ENJOYMEN.WITH SELF_EFFIC                            1\n#>  ENJOYMEN.WITH SELF_EFFIC                            2\n#>  ENJOYMEN.WITH SELF_EFFIC                            3\n#>  INSTRUME.WITH SELF_EFFIC                            1\n#>  INSTRUME.WITH SELF_EFFIC                            2\n#>  INSTRUME.WITH SELF_EFFIC                            3\n#>          Means       C1#1 Categorical.Latent.Variables\n#>      Variances BROAD_INTE                            2\n#>      Variances  ENJOYMENT                            1\n#>      Variances  ENJOYMENT                            2\n#>      Variances INSTRUMENT                            1\n#>  m1_est m2_est . m1_se m2_se . m1_est_se m2_est_se .\n#>   0.000  0.010 | 0.000 0.008 |   999.000     1.300 |\n#>   0.000  0.010 | 0.000 0.008 |   999.000     1.300 |\n#>   0.000  0.010 | 0.000 0.008 |   999.000     1.300 |\n#>   0.000 -0.013 | 0.000 0.024 |   999.000    -0.530 |\n#>   0.000 -0.013 | 0.000 0.024 |   999.000    -0.530 |\n#>   0.000 -0.013 | 0.000 0.024 |   999.000    -0.530 |\n#>   0.000 -0.029 | 0.000 0.023 |   999.000    -1.290 |\n#>   0.000 -0.029 | 0.000 0.023 |   999.000    -1.290 |\n#>   0.000 -0.029 | 0.000 0.023 |   999.000    -1.290 |\n#>   0.000 -0.034 | 0.000 0.015 |   999.000    -2.252 |\n#>   0.000 -0.034 | 0.000 0.015 |   999.000    -2.252 |\n#>   0.000 -0.034 | 0.000 0.015 |   999.000    -2.252 |\n#>   0.000 -0.029 | 0.000 0.008 |   999.000    -3.702 |\n#>   0.000 -0.029 | 0.000 0.008 |   999.000    -3.702 |\n#>   0.000 -0.029 | 0.000 0.008 |   999.000    -3.702 |\n#>   0.000  0.056 | 0.000 0.020 |   999.000     2.777 |\n#>   0.000  0.056 | 0.000 0.020 |   999.000     2.777 |\n#>   0.000  0.056 | 0.000 0.020 |   999.000     2.777 |\n#>  -0.285  0.143 | 0.188 0.341 |    -1.517     0.418 |\n#>   0.405  0.384 | 0.104 0.222 |     3.887     1.729 |\n#>   0.010  0.051 | 0.003 0.019 |     3.283     2.630 |\n#>   0.060  0.011 | 0.016 0.004 |     3.701     3.089 |\n#>   0.358  0.312 | 0.119 0.061 |     3.008     5.143 |\n#>  m1_pval m2_pval\n#>  999.000   0.194\n#>  999.000   0.194\n#>  999.000   0.194\n#>  999.000   0.596\n#>  999.000   0.596\n#>  999.000   0.596\n#>  999.000   0.197\n#>  999.000   0.197\n#>  999.000   0.197\n#>  999.000   0.024\n#>  999.000   0.024\n#>  999.000   0.024\n#>  999.000   0.000\n#>  999.000   0.000\n#>  999.000   0.000\n#>  999.000   0.005\n#>  999.000   0.005\n#>  999.000   0.005\n#>    0.129   0.676\n#>    0.000   0.084\n#>    0.001   0.009\n#>    0.000   0.002\n#>    0.003   0.000\n#> \n#> \n#>   Parameters unique to model 1: 0\n#>   -----------------------------\n#> \n#>   None\n#> \n#> \n#>   Parameters unique to model 2: 0\n#>   -----------------------------\n#> \n#>  None\n#> \n#> \n#> =============="},{"path":"enumeration-4.html","id":"plot-comparison","chapter":"6 Enumeration","heading":"6.9.3 Plot comparison","text":"can also plot comparisons look error bars.NOTE: plotMixtures() function used plotting LPA models (.e., means & variances)","code":"\na <- plotMixtures(output_enum$model_2_class_3.out,\n  ci = 0.95, bw = FALSE) \n\nb <- plotMixtures(output_enum$model_4_class_3.out,\n  ci = 0.95, bw = FALSE) \n\na + labs(title = \"Model 2\") +\n    theme(plot.title = element_text(size = 12)) +\nb + labs(title = \"Model 4\") +\n    theme(plot.title = element_text(size = 12))"},{"path":"enumeration-4.html","id":"visualization","chapter":"6 Enumeration","heading":"6.10 Visualization","text":"","code":""},{"path":"enumeration-4.html","id":"latent-profile-plot","chapter":"6 Enumeration","heading":"6.10.1 Latent Profile Plot","text":"Save figure","code":"\nsource(here(\"functions\", \"plot_lpa.R\"))\n\nplot_lpa(model_name = output_enum$model_3_class_4.out)\nggsave(here(\"figures\", \"model3_profile4.png\"), dpi = \"retina\", bg = \"white\", height=5, width=8, units=\"in\")"},{"path":"enumeration-4.html","id":"plots-means-and-variances","chapter":"6 Enumeration","heading":"6.10.2 Plots Means and Variances","text":"","code":"\nplotMixtures(output_enum$model_3_class_4.out, ci = 0.95, bw = FALSE) + \n  labs(title = \"Model 3: Equal Variances, Equal Covariances\")"},{"path":"enumeration-4.html","id":"model-evaluation","chapter":"6 Enumeration","heading":"6.11 Model Evaluation","text":"","code":""},{"path":"enumeration-4.html","id":"classifications-diagnostics-table","chapter":"6 Enumeration","heading":"6.11.1 Classifications Diagnostics Table","text":"Use Mplus calculate k-class confidence intervals (Note: Change syntax make chosen *k*-class model):Create table","code":"\nclassification  <- mplusObject(\n  \n  TITLE = \"LPA - Calculated k-Class 95% CI\",\n  \n  VARIABLE = \n  \"usevar =  broad_interest-self_efficacy;\n   classes = c1(4);\",\n  \n  ANALYSIS = \n   \"estimator = ml; \n    type = mixture;    \n    starts = 0; \n    processors = 10;\n    optseed = 468036; ! This seed is taken from chosen model output\n    bootstrap = 1000;\",\n  \n  MODEL =\n    \" \n    ! This is copied and pasted from the chosen model input\n  %c1#1%\n  broad_interest (vbroad_interest);\n  enjoyment (venjoyment);\n  instrumental_mot (vinstrumental_mot);\n  self_efficacy (vself_efficacy);\n\n  broad_interest WITH enjoyment (broad_interestWenjoyment);\n  broad_interest WITH instrumental_mot (broad_interestWinstrumental_mot);\n  broad_interest WITH self_efficacy (broad_interestWself_efficacy);\n  enjoyment WITH instrumental_mot (enjoymentWinstrumental_mot);\n  enjoyment WITH self_efficacy (enjoymentWself_efficacy);\n  instrumental_mot WITH self_efficacy (instrumental_motWself_efficacy);\n\n  %c1#2%\n  broad_interest (vbroad_interest);\n  enjoyment (venjoyment);\n  instrumental_mot (vinstrumental_mot);\n  self_efficacy (vself_efficacy);\n\n  broad_interest WITH enjoyment (broad_interestWenjoyment);\n  broad_interest WITH instrumental_mot (broad_interestWinstrumental_mot);\n  broad_interest WITH self_efficacy (broad_interestWself_efficacy);\n  enjoyment WITH instrumental_mot (enjoymentWinstrumental_mot);\n  enjoyment WITH self_efficacy (enjoymentWself_efficacy);\n  instrumental_mot WITH self_efficacy (instrumental_motWself_efficacy);\n\n  %c1#3%\n  broad_interest (vbroad_interest);\n  enjoyment (venjoyment);\n  instrumental_mot (vinstrumental_mot);\n  self_efficacy (vself_efficacy);\n\n  broad_interest WITH enjoyment (broad_interestWenjoyment);\n  broad_interest WITH instrumental_mot (broad_interestWinstrumental_mot);\n  broad_interest WITH self_efficacy (broad_interestWself_efficacy);\n  enjoyment WITH instrumental_mot (enjoymentWinstrumental_mot);\n  enjoyment WITH self_efficacy (enjoymentWself_efficacy);\n  instrumental_mot WITH self_efficacy (instrumental_motWself_efficacy);\n\n  %c1#4%\n  broad_interest (vbroad_interest);\n  enjoyment (venjoyment);\n  instrumental_mot (vinstrumental_mot);\n  self_efficacy (vself_efficacy);\n\n  broad_interest WITH enjoyment (broad_interestWenjoyment);\n  broad_interest WITH instrumental_mot (broad_interestWinstrumental_mot);\n  broad_interest WITH self_efficacy (broad_interestWself_efficacy);\n  enjoyment WITH instrumental_mot (enjoymentWinstrumental_mot);\n  enjoyment WITH self_efficacy (enjoymentWself_efficacy);\n  instrumental_mot WITH self_efficacy (instrumental_motWself_efficacy);\n  \n  \n  !CHANGE THIS SECTION TO YOUR CHOSEN k-CLASS MODEL\n    \n  %OVERALL%\n  [C1#1](c1);\n  [C1#2](c2);\n  [C1#3](c3);\n\n  Model Constraint:\n  New(p1 p2 p3 p4);\n  \n  p1 = exp(c1)/(1+exp(c1)+exp(c2)+exp(c3));\n  p2 = exp(c2)/(1+exp(c1)+exp(c2)+exp(c3));\n  p3 = exp(c3)/(1+exp(c1)+exp(c2)+exp(c3));  \n  p4 = 1/(1+exp(c1)+exp(c2)+exp(c3));\",\n\n  \n  OUTPUT = \"cinterval(bcbootstrap)\",\n  \n  usevariables = colnames(pisa),\n  rdata = pisa)\n\nclassification_fit <- mplusModeler(classification,\n                dataout=here(\"lpa\", \"mplus\", \"class.dat\"),\n                modelout=here(\"lpa\", \"mplus\", \"class.inp\") ,\n                check=TRUE, run = TRUE, hashfilename = FALSE)\nsource(here(\"functions\", \"diagnostics_table.R\"))\n\nclass_output <- readModels(here(\"mplus\", \"class.out\"))\n\ndiagnostics_table(class_output)"},{"path":"enumeration-4.html","id":"profile-homogeneity","chapter":"6 Enumeration","heading":"6.11.2 Profile Homogeneity","text":"Profile homogeneity model-estimated within-profile variances indicator m across k-profiles comparing total overall sample variance:\\[ \\frac{\\theta_{mk}}{\\theta_{m}} \\]","code":"\n# Change this to look at your chosen LPA model\n\nchosen_model <- output_enum$model_3_class_4.out\n\n\n# Extract overall and profile-specifc variances\noverall_variances <- data.frame(chosen_model$sampstat$univariate.sample.statistics) %>% \n  select(Variance) %>% \n  rownames_to_column(var = \"item\") %>% \n  clean_names() %>% \n  mutate(item = str_sub(item, 1, 8))\n\nprofile_variances <- data.frame(chosen_model$parameters$unstandardized) %>% \n  filter(paramHeader == \"Variances\") %>% \n  select(param, est, LatentClass) %>% \n  rename(item = param,\n         variance = est,\n         k = LatentClass) %>% \n  mutate(k = paste0(\"Profile \", k)) %>% \n  mutate(item = str_sub(item, 1, 8))\n\n  \n# Evaluate the ratio\nhomogeneity <- profile_variances %>%\n  left_join(overall_variances, by = \"item\") %>%\n  mutate(homogeneity_ratio = round((variance.x / variance.y),2))\n\n# Create a gt table\nhomogeneity %>%\n  select(homogeneity_ratio, k, item) %>% \n  pivot_wider(\n    names_from = k, \n    values_from = homogeneity_ratio\n  ) %>%\n  gt() %>%\n  cols_label(\n    item = \"Item\"\n  ) %>%\n  fmt_number(\n    columns = everything(),\n    decimals = 3\n  ) %>%\n  tab_header(\n    title = \"Profile Homogeneity Table\"\n  ) %>%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  )"},{"path":"enumeration-4.html","id":"profile-separation","chapter":"6 Enumeration","heading":"6.11.3 Profile Separation","text":"can evaluate degree profile separation assessing actual distance profile-specific means.\nquantify profile separation Profile j Profile k respect particular item m, compute standard mean difference:\\[\\hat{d}_{mjk}= \\frac{{\\hat{\\alpha_{mj}}-\\hat{\\alpha_{mk}}}}{\\sigma_{mjk}}\\] Pooled variance:\\[\\hat{\\sigma}_{mj k} = \\sqrt{\\frac{(\\hat{\\pi}_j)(n)(\\hat{\\theta}_{mj}) + (\\hat{\\pi}_k)(n)(\\hat{\\theta}_{mk})}{(\\hat{\\pi}_j + \\hat{\\pi}_k) n}}\\]","code":"\n# Change this to look at your chosen LPA model\nchosen_model <- output_enum$model_3_class_4.out\n\n\n# Profile-specific means\nprofile_means <- data.frame(chosen_model$parameters$unstandardized) %>% \n  filter(paramHeader == \"Means\") %>%\n  filter(!str_detect(param, \"#\")) %>% \n  select(param, est, LatentClass) %>% \n  rename(item = param,\n         means = est,\n         k = LatentClass) %>% \n  mutate(k = paste0(\"Profile \", k)) %>% \n  mutate(item = str_sub(item, 1, 8))\n\n# Relative profile sizes\nprofile_sizes <- data.frame(chosen_model$class_counts$modelEstimated) %>% \n  rename(size = proportion,\n         k = class) %>% \n  mutate(k = paste0(\"Profile \", k)) %>% \n  select(-count)\n\n# Sample size\nn <- chosen_model$summaries$Observations\n\n# Profile-specific variance\nprofile_variances <- data.frame(chosen_model$parameters$unstandardized) %>% \n  filter(paramHeader == \"Variances\") %>% \n  select(param, est, LatentClass) %>% \n  rename(item = param,\n         variance = est,\n         k = LatentClass) %>% \n  mutate(k = paste0(\"Profile \", k)) %>% \n  mutate(item = str_sub(item, 1, 8))\n\n# Combine profile variances with profile sizes\nvariance_with_sizes <- profile_variances %>%\n  left_join(profile_sizes, by = \"k\")\n\n# Create the combinations\nprofile_combinations <- data.frame(comboGrid(k1 = unique(profile_sizes$k), k2 = unique(profile_sizes$k))) %>%\n  filter(k1 != k2) %>%  \n  arrange(k1, k2)\n\n# Calculate pooled variance for each item across the profiles\ncombined_results <- profile_combinations %>%\n  rowwise() %>%\n  do({\n    pair <- .\n    k1 <- pair$k1\n    k2 <- pair$k2\n    \n    # Filter for the two profiles (variance)\n    data_k1_var <- variance_with_sizes %>% filter(k == k1)\n    data_k2_var <- variance_with_sizes %>% filter(k == k2)\n    \n    # Filter for the two profiles (means)\n    data_k1_mean <- profile_means %>% filter(k == k1)\n    data_k2_mean <- profile_means %>% filter(k == k2)\n    \n    # Combine variance data for the two profiles\n    variance_data <- data_k1_var %>%\n      inner_join(data_k2_var, by = \"item\", suffix = c(\"_k1\", \"_k2\")) %>%\n      mutate(\n        pooled_variance = sqrt(\n          ((size_k1 * n * variance_k1) + (size_k2 * n * variance_k2)) / ((size_k1 + size_k2) * n)\n        ),\n        comparison = paste(k1, \"vs\", k2)\n      ) %>%\n      select(item, pooled_variance, comparison)\n    \n    # Combine mean data for the two profilees\n    mean_data <- data_k1_mean %>%\n      inner_join(data_k2_mean, by = \"item\", suffix = c(\"_k1\", \"_k2\")) %>%\n      mutate(\n        mean_diff = means_k1 - means_k2,\n        comparison = paste(k1, \"vs\", k2)\n      ) %>%\n      select(item, mean_diff, comparison)\n    \n    # Combine both variance and mean differences data\n    combined_data <- variance_data %>%\n      left_join(mean_data, by = c(\"item\", \"comparison\")) %>% \n      mutate(\n         mean_diff_by_pooled_variance = mean_diff / pooled_variance\n      )\n    \n    combined_data\n  }) %>%\n  bind_rows()\n\n# Create a gt table\ncombined_results %>%\n  select(mean_diff_by_pooled_variance, comparison ,item) %>% \n  pivot_wider(\n    names_from = comparison, \n    values_from = mean_diff_by_pooled_variance\n  ) %>%\n  gt() %>%\n  cols_label(\n    item = \"Item\"\n  ) %>%\n  fmt_number(\n    columns = everything(),\n    decimals = 3\n  ) %>%\n  tab_header(\n    title = \"Profile Separation Table\"\n  ) %>%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  )"},{"path":"observed-response-patterns.html","id":"observed-response-patterns","chapter":"7 Observed Response Patterns","heading":"7 Observed Response Patterns","text":"Example: Bullying SchoolsTo demonstrate mixture modeling training program online resource components IES grant utilize Civil Rights Data Collection (CRDC)10 data repository.\nCRDC federally mandated school-level data collection effort occurs every year.\npublic data currently available selected latent class indicators across 4 years (2011, 2013, 2015, 2017) US states.\nexample, use Arizona state sample.\nutilize six focal indicators constitute latent class model example; three variables report harassment/bullying schools based disability, race, sex, three variables full-time equivalent school staff hires (counselor, psychologist, law enforcement).\ndata source also includes covariates variety subjects distal outcomes reported 2018 math/reading assessments graduation rates.Load packages","code":"\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(glue)\nlibrary(MplusAutomation)\nlibrary(here)\nlibrary(janitor)\nlibrary(gt)\nlibrary(cowplot)\nlibrary(DiagrammeR) "},{"path":"observed-response-patterns.html","id":"variable-description-2","chapter":"7 Observed Response Patterns","heading":"7.1 Variable Description","text":"Variables transformed dichotomous indicators using following coding strategyHarassment bullying count variables recoded 1 school reported least one incident harassment (0 indicates reported incidents).\noriginal scale reported CDRC staff variables full time equivalent employees (FTE) represented 1 part time employees represented values 1 0.\nSchools greater one staff designated type represented values greater 1.\nvalues greater zero recorded 1s (e.g., .5, 1,3) indicating school staff present campus least part time.\nSchools staff designated type indicated 0 dichotomous variable.","code":""},{"path":"observed-response-patterns.html","id":"prepare-data-4","chapter":"7 Observed Response Patterns","heading":"7.2 Prepare Data","text":"","code":"\ndf_bully <- read_csv(here(\"data\", \"crdc_lca_data.csv\")) %>% \n  clean_names() %>% \n  dplyr::select(report_dis, report_race, report_sex, counselors_fte, psych_fte, law_fte) "},{"path":"observed-response-patterns.html","id":"descriptive-statistics-4","chapter":"7 Observed Response Patterns","heading":"7.3 Descriptive Statistics","text":"Proportion indicators using R:","code":"\ndframe <- df_bully %>%\n  pivot_longer(\n    c(report_dis, report_race, report_sex, counselors_fte, psych_fte, law_fte),\n    names_to = \"Variable\"\n  ) %>%\n  group_by(Variable) %>%\n  summarise(\n    Count = sum(value == 1, na.rm = TRUE),\n    Total = n(),\n    .groups = \"drop\"\n  ) %>%\n  mutate(`Proportion Endorsed` = round(Count / Total, 3)) %>%\n  select(Variable, `Proportion Endorsed`, Count)\n\ngt(dframe) %>%\n  tab_header(\n    title = md(\"**LCA Indicator Endorsement**\"),\n    subtitle = md(\"&nbsp;\")\n  ) %>%\n  tab_options(\n    column_labels.font.weight = \"bold\",\n    row_group.font.weight = \"bold\"\n  )"},{"path":"observed-response-patterns.html","id":"observed-response-patterns-1","chapter":"7 Observed Response Patterns","heading":"7.4 Observed Response Patterns","text":"Continuining example bookdown, save response frequencies 3-class model response _____.dat SAVEDATA.Read observed response pattern data relabel columnsCreate table top 5 unconditional response pattern, top conditional response pattern modal class assignmentFinally, use gt make nicely formatted tableSave table:","code":"\n\npatterns  <- mplusObject(\n  \n  TITLE = \"C3 LCA - Save response patterns\", \n  \n  VARIABLE = \n  \"categorical = report_dis-law_fte; \n   usevar =  report_dis-law_fte;\n   classes = c(3);\",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 0;\n    processors = 10;\n    optseed = 802779;\",\n  \n  SAVEDATA = \n   \"File=savedata.dat;\n    Save=cprob;\n    \n    ! Code to save response frequency data \n    \n    response is resp_patterns.dat;\",\n  \n  OUTPUT = \"residual patterns tech11 tech14\",\n  \n  usevariables = colnames(df_bully),\n  rdata = df_bully)\n\npatterns_fit <- mplusModeler(patterns,\n                dataout=here(\"mplus\", \"bully.dat\"),\n                modelout=here(\"mplus\", \"patterns.inp\") ,\n                check=TRUE, run = TRUE, hashfilename = FALSE)\n# Read in response frequency data that we just created:\npatterns <- read_table(here(\"mplus\", \"resp_patterns.dat\"),\n                        col_names=FALSE, na = \"*\") \n\n# Extract the column names\nnames <- names(readModels(here(\"mplus\", \"patterns.out\"))[['savedata']]) \n\n# Add the names back to the dataset\ncolnames(patterns) <- c(\"Frequency\", names)  \n# Order responses by highest frequency\norder_highest <- patterns %>% \n  arrange(desc(Frequency)) \n\n# Loop `patterns` data to list top 5 conditional response patterns for each class\nloop_cond  <- lapply(1:max(patterns$C), function(k) {       \norder_cond <- patterns %>%                    \n  filter(C == k) %>%                    \n  arrange(desc(Frequency)) %>%                \n  head(5)                                     \n  })                                          \n\n# Convert loop into data frame\ntable_data <- as.data.frame(bind_rows(loop_cond))\n\n# Combine unconditional and conditional responses patterns\nresponse_patterns <-  rbind(order_highest[1:5,], table_data) \nresp_table <- response_patterns %>% \n  gt() %>%\n    tab_header(\n    title = \"Observed Response Patterns\",\n    subtitle = html(\"Response patterns, estimated frequencies, estimated posterior class probabilities and modal assignments\")) %>% \n    tab_source_note(\n    source_note = md(\"Data Source: **Civil Rights Data Collection (CRDC)**\")) %>%\n    cols_label(\n      Frequency = html(\"<i>f<\/i><sub>r<\/sub>\"),\n    REPORT_D = \"Harrassment: Disability\",\n    REPORT_R = \"Harrassment: Race\",\n    REPORT_S = \"Harrassment: Sex\",\n    COUNSELO = \"Staff: Counselor\",\n    PSYCH_FT = \"Staff: Psychologist\",\n    LAW_FTE = \"Staff: Law Enforcement\",\n    CPROB1 = html(\"P<sub><i>k<\/i><\/sub>=1\"),\n    CPROB2 = html(\"P<sub><i>k<\/i><\/sub>=2\"),\n    CPROB3 = html(\"P<sub><i>k<\/i><\/sub>=3\"),\n    C = md(\"*k*\")) %>% \n  tab_row_group(\n    label = \"Unconditional response patterns\",\n    rows = 1:5) %>%\n  tab_row_group(\n    label = md(\"*k* = 1 Conditional response patterns\"),\n    rows = 6:10) %>% #EDIT THESE VALUES BASED ON THE LAST COLUMN\n  tab_row_group(\n    label = md(\"*k* = 2 Conditional response patterns\"),\n    rows = 11:15)  %>% #EDIT THESE VALUES BASED ON THE LAST COLUMN\n  tab_row_group(\n    label = md(\"*k* = 3 Conditional response patterns\"),\n    rows = 16:20)  %>% #EDIT THESE VALUES BASED ON THE LAST COLUMN  \n    row_group_order(\n      groups = c(\"Unconditional response patterns\",\n                 md(\"*k* = 1 Conditional response patterns\"),\n                 md(\"*k* = 2 Conditional response patterns\"),\n                 md(\"*k* = 3 Conditional response patterns\"))) %>% \n    tab_footnote(\n    footnote = html(\n      \"<i>Note.<\/i> <i>f<\/i><sub>r<\/sub> = response pattern frequency; P<sub><i>k<\/i><\/sub> = posterior class probabilities\"\n    )\n  ) %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"Times New Roman\")\n\nresp_table\ngtsave(resp_table, here(\"figures\",\"resp_table.png\"))"},{"path":"classification-diagnostics.html","id":"classification-diagnostics","chapter":"8 Classification Diagnostics","heading":"8 Classification Diagnostics","text":"Example: Bullying SchoolsTo demonstrate mixture modeling training program online resource components IES grant utilize Civil Rights Data Collection (CRDC)11 data repository.\nCRDC federally mandated school-level data collection effort occurs every year.\npublic data currently available selected latent class indicators across 4 years (2011, 2013, 2015, 2017) US states.\nexample, use Arizona state sample.\nutilize six focal indicators constitute latent class model example; three variables report harassment/bullying schools based disability, race, sex, three variables full-time equivalent school staff hires (counselor, psychologist, law enforcement).\ndata source also includes covariates variety subjects distal outcomes reported 2018 math/reading assessments graduation rates.Load packages","code":"\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(glue)\nlibrary(MplusAutomation)\nlibrary(here)\nlibrary(janitor)\nlibrary(gt)\nlibrary(cowplot)\nlibrary(DiagrammeR) "},{"path":"classification-diagnostics.html","id":"variable-description-3","chapter":"8 Classification Diagnostics","heading":"8.1 Variable Description","text":"Variables transformed dichotomous indicators using following coding strategyHarassment bullying count variables recoded 1 school reported least one incident harassment (0 indicates reported incidents).\noriginal scale reported CDRC staff variables full time equivalent employees (FTE) represented 1 part time employees represented values 1 0.\nSchools greater one staff designated type represented values greater 1.\nvalues greater zero recorded 1s (e.g., .5, 1,3) indicating school staff present campus least part time.\nSchools staff designated type indicated 0 dichotomous variable.","code":""},{"path":"classification-diagnostics.html","id":"prepare-data-5","chapter":"8 Classification Diagnostics","heading":"8.2 Prepare Data","text":"","code":"\ndf_bully <- read_csv(here(\"data\", \"crdc_lca_data.csv\")) %>% \n  clean_names() %>% \n  dplyr::select(report_dis, report_race, report_sex, counselors_fte, psych_fte, law_fte) "},{"path":"classification-diagnostics.html","id":"descriptive-statistics-5","chapter":"8 Classification Diagnostics","heading":"8.3 Descriptive Statistics","text":"Proportion indicators using R:","code":"\ndframe <- df_bully %>%\n  pivot_longer(\n    c(report_dis, report_race, report_sex, counselors_fte, psych_fte, law_fte),\n    names_to = \"Variable\"\n  ) %>%\n  group_by(Variable) %>%\n  summarise(\n    Count = sum(value == 1, na.rm = TRUE),\n    Total = n(),\n    .groups = \"drop\"\n  ) %>%\n  mutate(`Proportion Endorsed` = round(Count / Total, 3)) %>%\n  select(Variable, `Proportion Endorsed`, Count)\n\ngt(dframe) %>%\n  tab_header(\n    title = md(\"**LCA Indicator Endorsement**\"),\n    subtitle = md(\"&nbsp;\")\n  ) %>%\n  tab_options(\n    column_labels.font.weight = \"bold\",\n    row_group.font.weight = \"bold\"\n  )"},{"path":"classification-diagnostics.html","id":"classification-diagnostics-1","chapter":"8 Classification Diagnostics","heading":"8.4 Classification Diagnostics","text":"Continuing example bookdown, use Mplus calculate k-class confidence intervals (Note: Change sytnax make chosen k-class model):Note: Ensure classes shift step (.g., Class 1 enumeration run now Class 4). Evaluate output compare class counts proportions latent classes. Using OPTSEED function ensures replication best loglikelihood value run.Read 3-class model:Now, use gt make nicely formatted table","code":"\nclassification  <- mplusObject(\n  \n  TITLE = \"C3 LCA - Calculated k-Class 95% CI\",\n  \n  VARIABLE =\n    \"categorical = report_dis-law_fte;\n   usevar =  report_dis-law_fte;\n   classes = c(3);\", \n  \n  ANALYSIS =\n    \"estimator = ml;\n    type = mixture;\n    starts = 0; \n    processors = 10;\n    optseed = 802779;\n    bootstrap = 1000;\",\n  \n  MODEL =\n    \"\n  !CHANGE THIS SECTION TO YOUR CHOSEN k-CLASS MODEL\n    \n  %OVERALL%\n  [C#1](c1);\n  \n  [C#2](C2);\n\n  Model Constraint:\n  New(p1 p2 p3);\n  \n  p1 = exp(c1)/(1+exp(c1)+exp(c2));\n  p2 = exp(c2)/(1+exp(c1)+exp(c2));\n  p3 = 1/(1+exp(c1)+exp(c2));\",\n\n  \n  OUTPUT = \"cinterval(bcbootstrap)\",\n  \n  usevariables = colnames(df_bully),\n  rdata = df_bully)\n\nclassification_fit <- mplusModeler(classification,\n                dataout=here(\"mplus\", \"bully.dat\"),\n                modelout=here(\"mplus\", \"class.inp\") ,\n                check=TRUE, run = TRUE, hashfilename = FALSE)\n# Read in the 3-class model and extract information needed\noutput_enum <- readModels(here(\"mplus\", \"class.out\"))\n\n# Entropy\nentropy <- c(output_enum$summaries$Entropy, rep(NA, output_enum$summaries$NLatentClasses-1))\n\n# 95% k-Class and k-class 95% Confidence Intervals\nk_ci <- output_enum$parameters$ci.unstandardized %>% \n  filter(paramHeader == \"New.Additional.Parameters\") %>% \n  unite(CI, c(low2.5,up2.5), sep=\", \", remove = TRUE) %>% \n  mutate(CI = paste0(\"[\", CI, \"]\")) %>% \n  rename(kclass=est) %>% \n  dplyr::select(kclass, CI)\n\n# AvePPk = Average Latent Class Probabilities for Most Likely Latent Class Membership (Row) by Latent Class (Column)\navePPk <- tibble(avePPk = diag(output_enum$class_counts$avgProbs.mostLikely))\n\n# mcaPk = modal class assignment proportion \nmcaPk <- round(output_enum$class_counts$mostLikely,3) %>% \n  mutate(model = paste0(\"Class \", class)) %>% \n  add_column(avePPk, k_ci) %>% \n  rename(mcaPk = proportion) %>% \n  dplyr::select(model, kclass, CI, mcaPk, avePPk)\n\n# OCCk = odds of correct classification\nOCCk <- mcaPk %>% \n  mutate(OCCk = round((avePPk/(1-avePPk))/(kclass/(1-kclass)),3))\n\n# Put everything together\nclass_table <- data.frame(OCCk, entropy)\nclass_table <- class_table %>% \n  gt() %>%\n    tab_header(\n    title = \"Model Classification Diagnostics for the 3-Class Solution\") %>%\n    cols_label(\n      model = md(\"*k*-Class\"),\n      kclass = md(\"*k*-Class Proportions\"),\n      CI = \"95% CI\",\n      mcaPk = html(\"McaP<sub>k<\/sub>\"),\n      avePPk = md(\"AvePP<sub>k<\/sub>\"),\n      OCCk = md(\"OCC<sub>k<\/sub>\"),\n      entropy = \"Entropy\") %>% \n    sub_missing(7,\n              missing_text = \"\") %>%\n    tab_footnote(\n    footnote = html(\n      \"<i>Note.<\/i> McaP<sub>k<\/sub> = Modal class assignment proportion; AvePP<sub>k<\/sub> = Average posterior class probabilities; OCC<sub>k<\/sub> = Odds of correct classification; \"\n    )\n  ) %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"Times New Roman\")\n\nclass_table"},{"path":"a-single-covariate.html","id":"a-single-covariate","chapter":"9 A Single Covariate","heading":"9 A Single Covariate","text":"Data source:first examples utilizes public-use dataset, Longitudinal Survey American Youth (LSAY): See documentation hereThe first examples utilizes public-use dataset, Longitudinal Survey American Youth (LSAY): See documentation hereThe second example utilizes dataset undergraduate Cheating available poLCA package (Dayton, 1998): See documentation hereThe second example utilizes dataset undergraduate Cheating available poLCA package (Dayton, 1998): See documentation ","code":""},{"path":"a-single-covariate.html","id":"load-packages-1","chapter":"9 A Single Covariate","heading":"9.1 Load packages","text":"example Mother’s Education predictor latent class membership Math AttitudesApplication: Longitudinal Study American Youth, Science AttitudesThe data can found data folder called lsay_subset.csv.","code":"\nlibrary(MplusAutomation)\nlibrary(tidyverse) #collection of R packages designed for data science\nlibrary(here) #helps with filepaths\nlibrary(janitor) #clean_names\nlibrary(gt) # create tables\nlibrary(cowplot) # a ggplot theme\nlibrary(DiagrammeR) # create path diagrams\nlibrary(glue) # allows us to paste expressions into R code\nlibrary(data.table) # used for `melt()` function  \nlibrary(poLCA)\nlibrary(reshape2)\nlsay_data <- read_csv(here(\"three_step\",\"data\",\"lsay_subset.csv\")) %>% \n  clean_names() %>%   # make variable names lowercase\n  mutate(female = recode(gender, `1` = 0, `2` = 1)) # relabel values from 1,2 to 0,1"},{"path":"a-single-covariate.html","id":"descriptive-statistics-6","chapter":"9 A Single Covariate","heading":"9.2 Descriptive Statistics","text":"Mother’s Education","code":"\ndframe <- lsay_data %>%\n  pivot_longer(\n    c(enjoy, useful, logical, job, adult),\n    names_to = \"Variable\"\n  ) %>%\n  group_by(Variable) %>%\n  summarise(\n    Count = sum(value == 1, na.rm = TRUE),\n    Total = n(),\n    .groups = \"drop\"\n  ) %>%\n  mutate(`Proportion Endorsed` = round(Count / Total, 3)) %>%\n  dplyr::select(Variable, `Proportion Endorsed`, Count)\n\ngt(dframe) %>%\n  tab_header(\n    title = md(\"**LCA Indicator Endorsement**\"),\n    subtitle = md(\"&nbsp;\")\n  ) %>%\n  tab_options(\n    column_labels.font.weight = \"bold\",\n    row_group.font.weight = \"bold\"\n  )\nsummary(factor(lsay_data$mothed))\n#>    1    2    3    4    5 NA's \n#>  452 1732  304  368  158  102"},{"path":"a-single-covariate.html","id":"manual-ml-three-step","chapter":"9 A Single Covariate","heading":"9.3 Manual ML Three-step","text":"","code":""},{"path":"a-single-covariate.html","id":"step-1---class-enumeration-w-auxiliary-specification","chapter":"9 A Single Covariate","heading":"9.3.1 Step 1 - Class Enumeration w/ Auxiliary Specification","text":"step done class enumeration (selected best latent class model). example, four class model best. Now, re-estimate four-class model using optseed efficiency. difference SAVEDATA command, can save posterior probabilities modal class assignment used steps two three.","code":"\n\nstep1  <- mplusObject(\n  TITLE = \"Step 1 - Three-Step using LSAL\", \n  VARIABLE = \n  \"categorical = enjoy useful logical job adult; \n   usevar = enjoy useful logical job adult;\n    \n   classes = c(4); \n    \n   auxiliary =  mothed  ! covariate \",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 0;\n    optseed = 568405;\",\n  \n  SAVEDATA = \n   \"File=3step_savedata.dat;\n    Save=cprob;\",\n  \n  OUTPUT = \"residual tech11 tech14\",\n  \n  PLOT = \n    \"type = plot3; \n    series = enjoy-adult(*);\",\n  \n  usevariables = colnames(lsay_data),\n  rdata = lsay_data)\n\nstep1_fit <- mplusModeler(step1,\n                            dataout=here(\"three_step\", \"manual_3step\", \"Step1.dat\"),\n                            modelout=here(\"three_step\", \"manual_3step\", \"one_cov.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\nsource(here(\"functions\", \"plot_lca.R\"))\noutput_lsay <- readModels(here(\"three_step\", \"manual_3step\",\"one_cov.out\"))\n\nplot_lca(model_name = output_lsay)"},{"path":"a-single-covariate.html","id":"step-2---determine-measurement-error","chapter":"9 A Single Covariate","heading":"9.3.2 Step 2 - Determine Measurement Error","text":"Extract logits classification probabilities likely latent classExtract saved dataset part mplusObject “step1_fit”Rename column savedata named “C” change “N”","code":"\nlogit_cprobs <- as.data.frame(output_lsay[[\"class_counts\"]]\n                                       [[\"logitProbs.mostLikely\"]])\nsavedata <- as.data.frame(output_lsay[[\"savedata\"]])\ncolnames(savedata)[colnames(savedata)==\"C\"] <- \"N\""},{"path":"a-single-covariate.html","id":"step-3---lca-auxiliary-variable-model-with-1-covariate","chapter":"9 A Single Covariate","heading":"9.3.3 Step 3 - LCA Auxiliary Variable Model with 1 Covariate","text":"","code":"\nstep3  <- mplusObject(\n  TITLE = \"Step3 - 3step LSAY\", \n  \n  VARIABLE = \n \"nominal=N;\n  usevar = n;\n  \n  classes = c(4);\n  \n  usevar = mothed;\" ,\n  \n  ANALYSIS = \n \"estimator = mlr; \n  type = mixture; \n  starts = 0;\",\n \n  DEFINE = \n   \"center mothed (grandmean);\",\n  \n  MODEL =\n  glue(\n \" %OVERALL%\n \n  C on mothed;        ! covariate as predictor of C\n\n     %C#1%\n  [n#1@{logit_cprobs[1,1]}]; ! MUST EDIT if you do not have a 4-class model. \n  [n#2@{logit_cprobs[1,2]}];\n  [n#3@{logit_cprobs[1,3]}];\n\n  %C#2%\n  [n#1@{logit_cprobs[2,1]}];\n  [n#2@{logit_cprobs[2,2]}];\n  [n#3@{logit_cprobs[2,3]}];\n\n  \n  %C#3%\n  [n#1@{logit_cprobs[3,1]}];\n  [n#2@{logit_cprobs[3,2]}];\n  [n#3@{logit_cprobs[3,3]}];\n\n\n  %C#4%\n  [n#1@{logit_cprobs[4,1]}];\n  [n#2@{logit_cprobs[4,2]}];\n  [n#3@{logit_cprobs[4,3]}];\"),\n  \n \n  usevariables = colnames(savedata), \n  rdata = savedata)\n\nstep3_fit <- mplusModeler(step3,\n               dataout=here(\"three_step\", \"manual_3step\", \"Step3.dat\"), \n               modelout=here(\"three_step\", \"manual_3step\", \"three_cov.inp\"), \n               check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"a-single-covariate.html","id":"covariates-relations","chapter":"9 A Single Covariate","heading":"9.3.3.1 Covariates Relations","text":"","code":"\nmodelParams <- readModels(here(\"three_step\", \"manual_3step\", \"three_cov.out\"))\n\n# Extract information as data frame\ncov <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n   filter(str_detect(paramHeader, \"^C#\\\\d+\\\\.ON$\")) %>% \n#  mutate(param = str_replace(param, \"FEMALE\", \"Gender\")) %>% # Change this to your own covariates\n  mutate(param = str_replace(param, \"MOTHED\", \"Mother's Education\")) %>%\n  mutate(est = format(round(est, 3), nsmall = 3),\n         se = round(se, 2),\n         pval = round(pval, 3)) %>% \n  mutate(latent_class = str_replace(paramHeader, \"^C#(\\\\d+)\\\\.ON$\", \"Class \\\\1\")) %>% \n  dplyr::select(param, est, se, pval, latent_class) %>% \n  mutate(se = paste0(\"(\", format(round(se,2), nsmall =2), \")\")) %>% \n  unite(logit, est, se, sep = \" \") %>% \n  dplyr::select(param, logit, pval, latent_class) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001)))) \n\nor <- as.data.frame(modelParams[[\"parameters\"]][[\"odds\"]]) %>%\n  filter(str_detect(paramHeader, \"^C#\\\\d+\\\\.ON$\")) %>% \n # mutate(param = str_replace(param, \"FEMALE\", \"Gender\")) %>% # Change this to your own covariates\n  mutate(param = str_replace(param, \"MOTHED\", \"Mother's Education\")) %>%\n  mutate(est = format(round(est, 3), nsmall = 3)) %>% \n  mutate(latent_class = str_replace(paramHeader, \"^C#(\\\\d+)\\\\.ON$\", \"Class \\\\1\")) %>% \n  mutate(CI = paste0(\"[\", format(round(lower_2.5ci, 3), nsmall = 3), \", \", format(round(upper_2.5ci, 3), nsmall = 3), \"]\")) %>% \n  dplyr::select(param, est, CI, latent_class) %>% \n  rename(or = est)\n  \ncombined <- or %>% \n  full_join(cov) %>% \n  dplyr::select(param, latent_class, logit, pval, or, CI)\n\n\n# Create table\n\ncombined %>% \n  gt(groupname_col = \"latent_class\", rowname_col = \"param\") %>%\n  tab_header(\n    title = \"Covariate Results: Mother's Education on Class\") %>%\n  cols_label(\n    logit = md(\"Logit (*se*)\"),\n    or = md(\"Odds Ratio\"),\n    CI = md(\"95% CI\"),\n    pval = md(\"*p*-value\")) %>% \n  sub_missing(1:3,\n              missing_text = \"\") %>%\n  sub_values(values = c(\"999.000\"), replacement = \"-\") %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\") %>%   \n  tab_footnote(\n    footnote = \"Reference Class: 4\",\n    locations = cells_title(groups = \"title\")\n  )"},{"path":"a-single-covariate.html","id":"automated-three-step","chapter":"9 A Single Covariate","heading":"9.4 Automated Three-Step","text":"Application: Undergraduate Cheating behavior“Dichotomous self-report responses 319 undergraduates four questions cheating behavior” (poLCA, 2016).Prepare data","code":"\n\ndata(cheating)\n\ncheating <- cheating %>% clean_names() \n\ndf_cheat <-  cheating %>%                                  \n  dplyr::select(1:4) %>%                                  \n  mutate_all(funs(.-1)) %>%                                \n  mutate(gpa = cheating$gpa)\n\n# Detaching packages that mask the dpylr functions \ndetach(package:poLCA, unload = TRUE)\ndetach(package:MASS, unload = TRUE)"},{"path":"a-single-covariate.html","id":"r3step","chapter":"9 A Single Covariate","heading":"9.4.1 R3STEP","text":"R3STEP incorporates latent class predictors mixture models. However, recommended use manual three-step.","code":""},{"path":"a-single-covariate.html","id":"run-the-r3step-model-with-gpa-as-the-latent-class-predictor","chapter":"9 A Single Covariate","heading":"9.4.1.1 Run the R3STEP model with gpa as the latent class predictor","text":"","code":"\n\nm_stepr  <- mplusObject(\n  TITLE = \"R3STEP - GPA as Predictor\", \n  VARIABLE = \n   \"categorical = lieexam-copyexam; \n    usevar = lieexam-copyexam;\n    auxiliary = gpa (R3STEP);\n    classes = c(2);\",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 500 100; \n    processors = 10;\",\n  \n  OUTPUT = \"sampstat patterns tech11 tech14;\",\n  \n  PLOT = \n    \"type = plot3; \n     series = lieexam-copyexam(*);\",\n  \n  usevariables = colnames(df_cheat),\n  rdata = df_cheat)\n\nm_stepr_fit <- mplusModeler(m_stepr, \n                            dataout=here(\"three_step\", \"auto_3step\", \"r3step.dat\"),\n                            modelout=here(\"three_step\", \"auto_3step\", \"c2_r3step.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"a-single-covariate.html","id":"regression-slopes-and-odds-ratios","chapter":"9 A Single Covariate","heading":"9.4.1.2 Regression slopes and odds ratios","text":"","code":"TESTS OF CATEGORICAL LATENT VARIABLE MULTINOMIAL LOGISTIC REGRESSIONS USING\nTHE 3-STEP PROCEDURE\n\n   WARNING:  LISTWISE DELETION IS APPLIED TO THE AUXILIARY VARIABLES IN THE\n   ANALYSIS.  TO AVOID LISTWISE DELETION, DATA IMPUTATION CAN BE USED\n   FOR THE AUXILIARY VARIABLES FOLLOWED BY ANALYSIS WITH TYPE=IMPUTATION.\n   NUMBER OF DELETED OBSERVATIONS:  4\n   NUMBER OF OBSERVATIONS USED:  315\n\n                                                    Two-Tailed\n                    Estimate       S.E.  Est./S.E.    P-Value\n\n C#1        ON\n    GPA               -0.698      0.255     -2.739      0.006\n\n Intercepts\n    C#1               -0.241      0.460     -0.523      0.601\n\nParameterization using Reference Class 1\n\n C#2        ON\n    GPA                0.698      0.255      2.739      0.006\n\n Intercepts\n    C#2                0.241      0.460      0.523      0.601\n\n\nODDS RATIOS FOR TESTS OF CATEGORICAL LATENT VARIABLE MULTINOMIAL LOGISTIC REGRESSIONS\nUSING THE 3-STEP PROCEDURE\n\n                                                95% C.I.\n                    Estimate       S.E. Lower 2.5% Upper 2.5%\n\n C#1        ON\n    GPA                0.498      0.127      0.302      0.820\n\n\nParameterization using Reference Class 1\n\n C#2        ON\n    GPA                2.009      0.512      1.220      3.310"},{"path":"a-single-distal-outcome.html","id":"a-single-distal-outcome","chapter":"10 A Single Distal Outcome","heading":"10 A Single Distal Outcome","text":"Data source:first examples utilizes public-use dataset, Longitudinal Survey American Youth (LSAY): See documentation hereThe first examples utilizes public-use dataset, Longitudinal Survey American Youth (LSAY): See documentation hereThe second example utilizes dataset undergraduate Cheating available poLCA package (Dayton, 1998): See documentation hereThe second example utilizes dataset undergraduate Cheating available poLCA package (Dayton, 1998): See documentation ","code":""},{"path":"a-single-distal-outcome.html","id":"load-packages-2","chapter":"10 A Single Distal Outcome","heading":"10.1 Load packages","text":"example Math IRT Score distal outcome Math Attitude classesApplication: Longitudinal Study American Youth, Math AttitudesThe data can found data folder called lsay_subset.csv.","code":"\nlibrary(MplusAutomation)\nlibrary(tidyverse) #collection of R packages designed for data science\nlibrary(here) #helps with filepaths\nlibrary(janitor) #clean_names\nlibrary(gt) # create tables\nlibrary(cowplot) # a ggplot theme\nlibrary(DiagrammeR) # create path diagrams\nlibrary(glue) # allows us to paste expressions into R code\nlibrary(data.table) # used for `melt()` function  \nlibrary(poLCA)\nlibrary(reshape2)\nlsay_data <- read_csv(here(\"three_step\",\"data\",\"lsay_subset.csv\")) %>% \n  clean_names() %>%   # make variable names lowercase\n  mutate(female = recode(gender, `1` = 0, `2` = 1)) # relabel values from 1,2 to 0,1"},{"path":"a-single-distal-outcome.html","id":"descriptive-statistics-7","chapter":"10 A Single Distal Outcome","heading":"10.2 Descriptive Statistics","text":"Math IRT Score","code":"\ndframe <- lsay_data %>%\n  pivot_longer(\n    c(enjoy, useful, logical, job, adult),\n    names_to = \"Variable\"\n  ) %>%\n  group_by(Variable) %>%\n  summarise(\n    Count = sum(value == 1, na.rm = TRUE),\n    Total = n(),\n    .groups = \"drop\"\n  ) %>%\n  mutate(`Proportion Endorsed` = round(Count / Total, 3)) %>%\n  dplyr::select(Variable, `Proportion Endorsed`, Count)\n\ngt(dframe) %>%\n  tab_header(\n    title = md(\"**LCA Indicator Endorsement**\"),\n    subtitle = md(\"&nbsp;\")\n  ) %>%\n  tab_options(\n    column_labels.font.weight = \"bold\",\n    row_group.font.weight = \"bold\"\n  )\nsummary(lsay_data$math_irt)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   26.57   50.00   59.30   58.81   68.21   94.19     875"},{"path":"a-single-distal-outcome.html","id":"manual-ml-three-step-1","chapter":"10 A Single Distal Outcome","heading":"10.3 Manual ML Three-step","text":"","code":""},{"path":"a-single-distal-outcome.html","id":"step-1---class-enumeration-w-auxiliary-specification-1","chapter":"10 A Single Distal Outcome","heading":"10.3.1 Step 1 - Class Enumeration w/ Auxiliary Specification","text":"step done class enumeration (selected best latent class model). example, four class model best. Now, re-estimate four-class model using optseed efficiency. difference SAVEDATA command, can save posterior probabilities modal class assignment used steps two three.","code":"\n\nstep1  <- mplusObject(\n  TITLE = \"Step 1 - Three-Step using LSAL\", \n  VARIABLE = \n  \"categorical = enjoy useful logical job adult; \n   usevar = enjoy useful logical job adult;\n    \n   classes = c(4); \n    \n   auxiliary =  math_irt  ! distal outcome \",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 0;\n    optseed = 568405;\",\n  \n  SAVEDATA = \n   \"File=3step_savedata.dat;\n    Save=cprob;\",\n  \n  OUTPUT = \"residual tech11 tech14\",\n  \n  PLOT = \n    \"type = plot3; \n    series = enjoy-adult(*);\",\n  \n  usevariables = colnames(lsay_data),\n  rdata = lsay_data)\n\nstep1_fit <- mplusModeler(step1,\n                            dataout=here(\"three_step\", \"manual_3step\", \"Step1.dat\"),\n                            modelout=here(\"three_step\", \"manual_3step\", \"one_dis.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\nsource(here(\"functions\", \"plot_lca.R\"))\noutput_lsay <- readModels(here(\"three_step\", \"manual_3step\",\"one_dis.out\"))\n\nplot_lca(model_name = output_lsay)"},{"path":"a-single-distal-outcome.html","id":"step-2---determine-measurement-error-1","chapter":"10 A Single Distal Outcome","heading":"10.3.2 Step 2 - Determine Measurement Error","text":"Extract logits classification probabilities likely latent classExtract saved dataset part mplusObject “step1_fit”Rename column savedata named “C” change “N”","code":"\nlogit_cprobs <- as.data.frame(output_lsay[[\"class_counts\"]]\n                                       [[\"logitProbs.mostLikely\"]])\nsavedata <- as.data.frame(output_lsay[[\"savedata\"]])\ncolnames(savedata)[colnames(savedata)==\"C\"] <- \"N\""},{"path":"a-single-distal-outcome.html","id":"step-3---lca-auxiliary-variable-model-with-1-distal-outcome","chapter":"10 A Single Distal Outcome","heading":"10.3.3 Step 3 - LCA Auxiliary Variable Model with 1 Distal Outcome","text":"","code":"\nstep3  <- mplusObject(\n  TITLE = \"Step3 - 3step LSAY\", \n  \n  VARIABLE = \n \"nominal=N;\n  usevar = n;\n  \n  classes = c(4);\n  \n  usevar = math_irt;\" ,\n  \n  ANALYSIS = \n \"estimator = mlr; \n  type = mixture; \n  starts = 0;\",\n  \n  MODEL =\n  glue(\n \" %OVERALL%\n \n\n     %C#1%\n  [n#1@{logit_cprobs[1,1]}]; ! MUST EDIT if you do not have a 4-class model. \n  [n#2@{logit_cprobs[1,2]}];\n  [n#3@{logit_cprobs[1,3]}];\n  \n  [math_irt](m1);    ! conditional distal mean \n  math_irt;          ! conditional distal variance (freely estimated)\n\n  %C#2%\n  [n#1@{logit_cprobs[2,1]}];\n  [n#2@{logit_cprobs[2,2]}];\n  [n#3@{logit_cprobs[2,3]}];\n  \n  [math_irt](m2);\n  math_irt;\n  \n  %C#3%\n  [n#1@{logit_cprobs[3,1]}];\n  [n#2@{logit_cprobs[3,2]}];\n  [n#3@{logit_cprobs[3,3]}];\n  \n  [math_irt](m3);\n  math_irt;\n\n  %C#4%\n  [n#1@{logit_cprobs[4,1]}];\n  [n#2@{logit_cprobs[4,2]}];\n  [n#3@{logit_cprobs[4,3]}];\n  \n  [math_irt](m4);\n  math_irt; \"),\n  \n  MODELCONSTRAINT = \n   \"New (diff12 diff13 diff23 \n    diff14 diff24 diff34);\n  \n    diff12 = m1-m2;  ! test pairwise distal mean differences\n    diff13 = m1-m3;\n    diff23 = m2-m3;\n    diff14 = m1-m4;\n    diff24 = m2-m4;\n    diff34 = m3-m4;\",\n  \n  MODELTEST = \"     ! omnibus test of distal means \n    m1=m2;\n    m2=m3;\n    m3=m4;\",\n  \n \n  usevariables = colnames(savedata), \n  rdata = savedata)\n\nstep3_fit <- mplusModeler(step3,\n               dataout=here(\"three_step\", \"manual_3step\", \"Step3.dat\"), \n               modelout=here(\"three_step\", \"manual_3step\", \"three_dis.inp\"), \n               check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"a-single-distal-outcome.html","id":"wald-test-table","chapter":"10 A Single Distal Outcome","heading":"10.3.3.1 Wald Test Table","text":"testing relation latent class variable distal outcome (mathirt)Save figure","code":"\nmodelParams <- readModels(here(\"three_step\", \"manual_3step\", \"three_dis.out\"))\n\n# Extract information as data frame\nwald <- as.data.frame(modelParams[[\"summaries\"]]) %>%\n  dplyr::select(WaldChiSq_Value:WaldChiSq_PValue) %>% \n  mutate(WaldChiSq_DF = paste0(\"(\", WaldChiSq_DF, \")\")) %>% \n  unite(wald_test, WaldChiSq_Value, WaldChiSq_DF, sep = \" \") %>% \n  rename(pval = WaldChiSq_PValue) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001))))\n\n# Create table\n\nwald_table <- wald %>% \n  gt() %>%\n    tab_header(\n    title = \"Wald Test Distal Means (Math IRT Scores)\") %>%\n    cols_label(\n      wald_test = md(\"Wald Test (*df*)\"), \n      pval = md(\"*p*-value\")) %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\")\n\nwald_table\ngtsave(wald_table, here(\"figures\",\"wald_table.docx\"))"},{"path":"a-single-distal-outcome.html","id":"table-of-pairwise-distal-outcome-differences","chapter":"10 A Single Distal Outcome","heading":"10.3.3.2 Table of Pairwise Distal Outcome Differences","text":"","code":"\nmodelParams <- readModels(here(\"three_step\", \"manual_3step\", \"three_dis.out\"))\n\n# Extract information as data frame\ndiff <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n  filter(grepl(\"DIFF\", param)) %>% \n  dplyr::select(param:pval) %>% \n  mutate(se = paste0(\"(\", format(round(se,2), nsmall =2), \")\")) %>% \n  unite(estimate, est, se, sep = \" \") %>% \n  mutate(param = str_remove(param, \"DIFF\"),\n         param = as.numeric(param)) %>% \n  separate(param, into = paste0(\"Group\", 1:2), sep = 1) %>% \n  mutate(class = paste0(\"Class \", Group1, \" vs \", Group2)) %>% \n  dplyr::select(class, estimate, pval) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001))))\n\n# Create table\n\ndiff %>% \n  gt() %>%\n    tab_header(\n    title = \"Distal Outcome Differences\") %>%\n    cols_label(\n      class = \"Class\",\n      estimate = md(\"Mean (*se*)\"),\n      pval = md(\"*p*-value\")) %>% \n    sub_missing(1:3,\n              missing_text = \"\") %>%\n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\")"},{"path":"a-single-distal-outcome.html","id":"plot-distal-outcome-means","chapter":"10 A Single Distal Outcome","heading":"10.3.3.3 Plot Distal Outcome Means","text":"","code":"\nmodelParams <- readModels(here(\"three_step\", \"manual_3step\", \"three_dis.out\"))\n\n# Extract class size \nc_size <- as.data.frame(modelParams[[\"class_counts\"]][[\"modelEstimated\"]][[\"proportion\"]]) %>% \n  rename(\"cs\" = 1) %>% \n  mutate(cs = round(cs*100, 2))\n\nc_size_val <- paste0(\"C\", 1:nrow(c_size), glue(\" ({c_size[1:nrow(c_size),]}%)\"))\n\n\n# Extract information as data frame\nestimates <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n  filter(paramHeader == \"Means\") %>%\n  dplyr::select(param, est, se) %>% \n  filter(param == \"MATH_IRT\") %>% \n  mutate(across(c(est, se), as.numeric)) %>% \n  mutate(LatentClass = c_size_val)\n\n# Add labels (NOTE: You must change the labels to match the significance testing!!) \n#value_labels <- paste0(estimates$est, c(\"a\",\" bc\",\" abd\",\" cd\"))\n\nestimates$LatentClass <- fct_inorder(estimates$LatentClass)\n\n# Plot bar graphs\nestimates %>%\n  ggplot(aes(x=LatentClass, y = est, fill = LatentClass)) +\n  geom_col(position = \"dodge\", color = \"black\") +\n  geom_errorbar(aes(ymin=est-se, ymax=est+se),\n                linewidth=.3,    # Thinner lines\n                width=.2,\n                position=position_dodge(.9)) +\n  geom_text(aes(label = est), \n            family = \"serif\", size = 4,\n            position=position_dodge(.9),\n            vjust = 8) +  \n # scale_fill_grey(start = .4, end = .7) + # Remove for colorful bars\n  labs(y=\"Math Scores\", x=\"\") +\n  theme_cowplot() +\n  theme(text = element_text(family = \"serif\", size = 15),\n        axis.text.x = element_text(size=15),\n        legend.position=\"none\")\n\n# Save plot\nggsave(here(\"figures\",\"ManualDistal_Plot.jpeg\"),              \n       dpi=300, width=10, height = 7, units=\"in\") "},{"path":"a-single-distal-outcome.html","id":"automated-three-step-1","chapter":"10 A Single Distal Outcome","heading":"10.4 Automated Three-Step","text":"Application: Undergraduate Cheating behavior“Dichotomous self-report responses 319 undergraduates four questions cheating behavior” (poLCA, 2016).Prepare data","code":"\n\ndata(cheating)\n\ncheating <- cheating %>% clean_names() \n\ndf_cheat <-  cheating %>%                                  \n  dplyr::select(1:4) %>%                                  \n  mutate_all(funs(.-1)) %>%                                \n  mutate(gpa = cheating$gpa)\n\n# Detaching packages that mask the dpylr functions \ndetach(package:poLCA, unload = TRUE)\ndetach(package:MASS, unload = TRUE)"},{"path":"a-single-distal-outcome.html","id":"du3step","chapter":"10 A Single Distal Outcome","heading":"10.4.1 DU3STEP","text":"DU3STEP incorporates distal outcome variables (assumed unequal means variances) mixture models.","code":""},{"path":"a-single-distal-outcome.html","id":"run-the-du3step-model-with-gpa-as-distal-outcome","chapter":"10 A Single Distal Outcome","heading":"10.4.1.1 Run the DU3step model with gpa as distal outcome","text":"","code":"\n\nm_stepdu  <- mplusObject(\n  TITLE = \"DU3STEP - GPA as Distal\", \n  VARIABLE = \n   \"categorical = lieexam-copyexam; \n    usevar = lieexam-copyexam;\n    auxiliary = gpa (du3step);\n    classes = c(2);\",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 500 100; \n    processors = 10;\",\n  \n  OUTPUT = \"sampstat patterns tech11 tech14;\",\n  \n  PLOT = \n    \"type = plot3; \n     series = lieexam-copyexam(*);\",\n  \n  usevariables = colnames(df_cheat),\n  rdata = df_cheat)\n\nm_stepdu_fit <- mplusModeler(m_stepdu, \n                            dataout=here(\"three_step\", \"auto_3step\", \"du3step.dat\"),\n                            modelout=here(\"three_step\", \"auto_3step\", \"c2_du3step.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"a-single-distal-outcome.html","id":"plot-distal-outcome-mean-differences","chapter":"10 A Single Distal Outcome","heading":"10.4.1.2 Plot Distal Outcome mean differences","text":"","code":"\nmodelParams <- readModels(here(\"three_step\", \"auto_3step\", \"c2_du3step.out\"))\n\n# Extract class size \nc_size <- as.data.frame(modelParams[[\"class_counts\"]][[\"modelEstimated\"]][[\"proportion\"]]) %>% \n  rename(\"cs\" = 1) %>% \n  mutate(cs = round(cs*100, 2))\n\nc_size_val <- paste0(\"C\", 1:nrow(c_size), glue(\" ({c_size[1:nrow(c_size),]}%)\"))\n\n\n# Extract information as data frame\nestimates <- as.data.frame(modelParams[[\"lcCondMeans\"]][[\"overall\"]]) %>%\n  reshape2::melt(id.vars = \"var\") %>%\n  mutate(variable = as.character(variable),\n         LatentClass = case_when(\n           endsWith(variable, \"1\") ~ c_size_val[1],\n           endsWith(variable, \"2\") ~ c_size_val[2])) %>% #Add to this based on the number of classes you have\n  head(-3) %>% \n  pivot_wider(names_from = variable, values_from = value) %>% \n  unite(\"mean\", contains(\"m\"), na.rm = TRUE) %>% \n  unite(\"se\", contains(\"se\"), na.rm = TRUE) %>% \n  mutate(across(c(mean, se), as.numeric))\n\n# Add labels (NOTE: You must change the labels to match the significance testing!!) \nvalue_labels <- paste0(estimates$mean, c(\" a\",\" b\"))\n\n# Plot bar graphs\nestimates %>%\n  ggplot(aes(fill = LatentClass, y = mean, x = LatentClass)) +\n  geom_bar(position = \"dodge\", stat = \"identity\", color = \"black\") +\n  geom_errorbar(aes(ymin=mean-se, ymax=mean+se),\n                size=.3,    \n                width=.2,\n                position=position_dodge(.9)) +\n  geom_text(aes(y = mean, label = value_labels), \n            family = \"serif\", size = 4,\n            position=position_dodge(.9),\n            vjust = 8) +\n  #scale_fill_grey(start = .5, end = .7) +\n  labs(y=\"GPA\", x=\"\") +\n  theme_cowplot() +\n  theme(text = element_text(family = \"serif\", size = 12),\n        axis.text.x = element_text(size=12),\n        legend.position=\"none\") +\n  coord_cartesian(expand = FALSE, \n                  ylim=c(0,max(estimates$mean*1.5))) # Change ylim based on distal outcome rang\n\n\n# Save plot\nggsave(here(\"figures\",\"Du3STEP_plot.jpeg\"),                \n       dpi=300, width=10, height = 7, units=\"in\")  "},{"path":"two-covariates-and-a-single-distal-outcome.html","id":"two-covariates-and-a-single-distal-outcome","chapter":"11 Two Covariates and a Single Distal Outcome","heading":"11 Two Covariates and a Single Distal Outcome","text":"Data source:example utilizes public-use dataset, Longitudinal Survey American Youth (LSAY): See documentation ","code":""},{"path":"two-covariates-and-a-single-distal-outcome.html","id":"load-packages-3","chapter":"11 Two Covariates and a Single Distal Outcome","heading":"11.1 Load packages","text":"example uses Gender Mother’s Education predictors latent class membership Math IRT scores distal outcome single model.Application: Longitudinal Study American Youth, Science AttitudesThe data can found data folder called lsay_subset.csv.","code":"\nlibrary(MplusAutomation)\nlibrary(tidyverse) #collection of R packages designed for data science\nlibrary(here) #helps with filepaths\nlibrary(janitor) #clean_names\nlibrary(gt) # create tables\nlibrary(cowplot) # a ggplot theme\nlibrary(DiagrammeR) # create path diagrams\nlibrary(glue) # allows us to paste expressions into R code\nlibrary(data.table) # used for `melt()` function  \nlibrary(poLCA)\nlibrary(reshape2)\nlsay_data <- read_csv(here(\"three_step\",\"data\",\"lsay_subset.csv\")) %>% \n  clean_names() %>%   # make variable names lowercase\n  mutate(female = recode(gender, `1` = 0, `2` = 1)) # relabel values from 1,2 to 0,1"},{"path":"two-covariates-and-a-single-distal-outcome.html","id":"descriptive-statistics-8","chapter":"11 Two Covariates and a Single Distal Outcome","heading":"11.2 Descriptive Statistics","text":"GenderMother’s EducationMath IRT Score","code":"\ndframe <- lsay_data %>%\n  pivot_longer(\n    c(enjoy, useful, logical, job, adult),\n    names_to = \"Variable\"\n  ) %>%\n  group_by(Variable) %>%\n  summarise(\n    Count = sum(value == 1, na.rm = TRUE),\n    Total = n(),\n    .groups = \"drop\"\n  ) %>%\n  mutate(`Proportion Endorsed` = round(Count / Total, 3)) %>%\n  dplyr::select(Variable, `Proportion Endorsed`, Count)\n\ngt(dframe) %>%\n  tab_header(\n    title = md(\"**LCA Indicator Endorsement**\"),\n    subtitle = md(\"&nbsp;\")\n  ) %>%\n  tab_options(\n    column_labels.font.weight = \"bold\",\n    row_group.font.weight = \"bold\"\n  )\nsummary(factor(lsay_data$mothed))\n#>    1    2    3    4    5 NA's \n#>  452 1732  304  368  158  102\nsummary(factor(lsay_data$female))\n#>    0    1 \n#> 1490 1626\nsummary(lsay_data$math_irt)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   26.57   50.00   59.30   58.81   68.21   94.19     875"},{"path":"two-covariates-and-a-single-distal-outcome.html","id":"manual-ml-three-step-2","chapter":"11 Two Covariates and a Single Distal Outcome","heading":"11.3 Manual ML Three-step","text":"","code":""},{"path":"two-covariates-and-a-single-distal-outcome.html","id":"step-1---class-enumeration-w-auxiliary-specification-2","chapter":"11 Two Covariates and a Single Distal Outcome","heading":"11.3.1 Step 1 - Class Enumeration w/ Auxiliary Specification","text":"step done class enumeration (selected best latent class model). example, four class model best. Now, re-estimate four-class model using optseed efficiency. difference SAVEDATA command, can save posterior probabilities modal class assignment used steps two three.","code":"\n\nstep1  <- mplusObject(\n  TITLE = \"Step 1 - Three-Step using LSAL\", \n  VARIABLE = \n  \"categorical = enjoy useful logical job adult; \n   usevar = enjoy useful logical job adult;\n    \n   classes = c(4); \n    \n   auxiliary =   ! list all potential covariates and distals here\n   female  mothed      ! covariate\n   math_irt;      ! distal math test score in 12th grade \",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 0;\n    optseed = 568405;\",\n  \n  SAVEDATA = \n   \"File=3step_savedata.dat;\n    Save=cprob;\",\n  \n  OUTPUT = \"residual tech11 tech14\",\n  \n  PLOT = \n    \"type = plot3; \n    series = enjoy-adult(*);\",\n  \n  usevariables = colnames(lsay_data),\n  rdata = lsay_data)\n\nstep1_fit <- mplusModeler(step1,\n                            dataout=here(\"three_step\", \"manual_3step\", \"Step1.dat\"),\n                            modelout=here(\"three_step\", \"manual_3step\", \"one.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\nsource(here(\"functions\", \"plot_lca.R\"))\noutput_lsay <- readModels(here(\"three_step\", \"manual_3step\",\"one.out\"))\n\nplot_lca(model_name = output_lsay)"},{"path":"two-covariates-and-a-single-distal-outcome.html","id":"step-2---determine-measurement-error-2","chapter":"11 Two Covariates and a Single Distal Outcome","heading":"11.3.2 Step 2 - Determine Measurement Error","text":"Extract logits classification probabilities likely latent classExtract saved dataset part mplusObject “step1_fit”Rename column savedata named “C” change “N”","code":"\nlogit_cprobs <- as.data.frame(output_lsay[[\"class_counts\"]]\n                                       [[\"logitProbs.mostLikely\"]])\nsavedata <- as.data.frame(output_lsay[[\"savedata\"]])\ncolnames(savedata)[colnames(savedata)==\"C\"] <- \"N\""},{"path":"two-covariates-and-a-single-distal-outcome.html","id":"step-3---lca-auxiliary-variable-model-with-2-covariates-and-1-distal-outcome","chapter":"11 Two Covariates and a Single Distal Outcome","heading":"11.3.3 Step 3 - LCA Auxiliary Variable Model with 2 covariates and 1 distal outcome","text":"Model 2 covariates (gender mother’s education) 1 distal outcome (math IRT scores)","code":"\nstep3  <- mplusObject(\n  TITLE = \"Step3 - 3step LSAY\", \n  \n  VARIABLE = \n \"nominal=N;\n  usevar = n;\n  \n  classes = c(4);\n  \n  usevar = female mothed math_irt;\" ,\n  \n  ANALYSIS = \n \"estimator = mlr; \n  type = mixture; \n  starts = 0;\",\n \n  DEFINE = \n   \"center female mothed (grandmean);\",\n  \n  MODEL =\n  glue(\n \" %OVERALL%\n \n  math_irt on female mothed; ! covariate as a related to the distal outcome\n  C on female (f1-f3);\n  c on mothed (e1-e3);      ! covariate as predictor of C\n\n     %C#1%\n  [n#1@{logit_cprobs[1,1]}]; ! MUST EDIT if you do not have a 4-class model. \n  [n#2@{logit_cprobs[1,2]}];\n  [n#3@{logit_cprobs[1,3]}];\n  \n  [math_irt](m1);    ! conditional distal mean \n  math_irt;          ! conditional distal variance (freely estimated)\n\n  %C#2%\n  [n#1@{logit_cprobs[2,1]}];\n  [n#2@{logit_cprobs[2,2]}];\n  [n#3@{logit_cprobs[2,3]}];\n  \n  [math_irt](m2);\n  math_irt;\n  \n  %C#3%\n  [n#1@{logit_cprobs[3,1]}];\n  [n#2@{logit_cprobs[3,2]}];\n  [n#3@{logit_cprobs[3,3]}];\n  \n  [math_irt](m3);\n  math_irt;\n\n  %C#4%\n  [n#1@{logit_cprobs[4,1]}];\n  [n#2@{logit_cprobs[4,2]}];\n  [n#3@{logit_cprobs[4,3]}];\n  \n  [math_irt](m4);\n  math_irt; \"),\n  \n  MODELCONSTRAINT = \n   \"New (diff12 diff13 diff23 \n    diff14 diff24 diff34\n    \n    d_fem_12 d_fem_13 \n    d_fem_23\n \n    d_ed_12 d_ed_13 \n    d_ed_23 \n    );\n  \n    diff12 = m1-m2;  ! test pairwise distal mean differences\n    diff13 = m1-m3;\n    diff23 = m2-m3;\n    diff14 = m1-m4;\n    diff24 = m2-m4;\n    diff34 = m3-m4;\n \n    d_fem_12 = f1-f2;\n    d_fem_13 = f1-f3;\n    d_fem_23 = f2-f3; \n\n    d_ed_12 = e1-e2;\n    d_ed_13 = e1-e3;\n    d_ed_23 = e2-e3; \n\n \",\n  \n  MODELTEST = \"     ! omnibus test of distal means \n  !  m1=m2;\n  !  m2=m3;\n  !  m3=m4;\n \n  ! f1=f2;       ! omnibus test of covariate logits (female)  \n  !  f1=f3;\n \n    e1=e2;       ! omnibus test of covariate logits (mothers ed)  \n    e1=e3;\n   \",\n \n  usevariables = colnames(savedata), \n  rdata = savedata)\n\nstep3_fit <- mplusModeler(step3,\n               dataout=here(\"three_step\", \"manual_3step\", \"Step3.dat\"), \n               modelout=here(\"three_step\", \"manual_3step\", \"three.inp\"), \n               check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"two-covariates-and-a-single-distal-outcome.html","id":"wald-test-table-1","chapter":"11 Two Covariates and a Single Distal Outcome","heading":"11.3.3.1 Wald Test Table","text":"testing relation latent class variable distal outcome (mathirt)Save figure","code":"\nmodelParams <- readModels(here(\"three_step\", \"manual_3step\", \"three.out\"))\n\n# Extract information as data frame\nwald <- as.data.frame(modelParams[[\"summaries\"]]) %>%\n  dplyr::select(WaldChiSq_Value:WaldChiSq_PValue) %>% \n  mutate(WaldChiSq_DF = paste0(\"(\", WaldChiSq_DF, \")\")) %>% \n  unite(wald_test, WaldChiSq_Value, WaldChiSq_DF, sep = \" \") %>% \n  rename(pval = WaldChiSq_PValue) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001))))\n\n# Create table\n\nwald_table <- wald %>% \n  gt() %>%\n    tab_header(\n    title = \"Wald Test Distal Means (Math IRT Scores)\") %>%\n    cols_label(\n      wald_test = md(\"Wald Test (*df*)\"), \n      pval = md(\"*p*-value\")) %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\")\n\nwald_table\ngtsave(wald_table, here(\"figures\",\"wald_table.docx\"))"},{"path":"two-covariates-and-a-single-distal-outcome.html","id":"table-of-pairwise-distal-outcome-differences-1","chapter":"11 Two Covariates and a Single Distal Outcome","heading":"11.3.3.2 Table of Pairwise Distal Outcome Differences","text":"","code":"\nmodelParams <- readModels(here(\"three_step\", \"manual_3step\", \"three.out\"))\n\n# Extract information as data frame\ndiff <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n  filter(grepl(\"DIFF\", param)) %>% \n  dplyr::select(param:pval) %>% \n  mutate(se = paste0(\"(\", format(round(se,2), nsmall =2), \")\")) %>% \n  unite(estimate, est, se, sep = \" \") %>% \n  mutate(param = str_remove(param, \"DIFF\"),\n         param = as.numeric(param)) %>% \n  separate(param, into = paste0(\"Group\", 1:2), sep = 1) %>% \n  mutate(class = paste0(\"Class \", Group1, \" vs \", Group2)) %>% \n  dplyr::select(class, estimate, pval) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001))))\n\n# Create table\n\ndiff %>% \n  gt() %>%\n    tab_header(\n    title = \"Distal Outcome Differences\") %>%\n    cols_label(\n      class = \"Class\",\n      estimate = md(\"Mean (*se*)\"),\n      pval = md(\"*p*-value\")) %>% \n    sub_missing(1:3,\n              missing_text = \"\") %>%\n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\")"},{"path":"two-covariates-and-a-single-distal-outcome.html","id":"plot-distal-outcome-means-1","chapter":"11 Two Covariates and a Single Distal Outcome","heading":"11.3.3.3 Plot Distal Outcome Means","text":"","code":"\nmodelParams <- readModels(here(\"three_step\", \"manual_3step\", \"three.out\"))\n\n# Extract class size \nc_size <- as.data.frame(modelParams[[\"class_counts\"]][[\"modelEstimated\"]][[\"proportion\"]]) %>% \n  rename(\"cs\" = 1) %>% \n  mutate(cs = round(cs*100, 2))\n\nc_size_val <- paste0(\"C\", 1:nrow(c_size), glue(\" ({c_size[1:nrow(c_size),]}%)\"))\n\n\n# Extract information as data frame\nestimates <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n  filter(paramHeader == \"Intercepts\") %>%\n  dplyr::select(param, est, se) %>% \n  filter(param == \"MATH_IRT\") %>% \n  mutate(across(c(est, se), as.numeric)) %>% \n  mutate(LatentClass = c_size_val)\n\n# Add labels (NOTE: You must change the labels to match the significance testing!!) \n#value_labels <- paste0(estimates$est, c(\"a\",\" bc\",\" abd\",\" cd\"))\n\nestimates$LatentClass <- fct_inorder(estimates$LatentClass)\n\n# Plot bar graphs\nestimates %>%\n  ggplot(aes(x=LatentClass, y = est, fill = LatentClass)) +\n  geom_col(position = \"dodge\", stat = \"identity\", color = \"black\") +\n  geom_errorbar(aes(ymin=est-se, ymax=est+se),\n                size=.3,    # Thinner lines\n                width=.2,\n                position=position_dodge(.9)) +\n  geom_text(aes(label = est), \n            family = \"serif\", size = 4,\n            position=position_dodge(.9),\n            vjust = 8) +  \n # scale_fill_grey(start = .4, end = .7) + # Remove for colorful bars\n  labs(y=\"Math Scores\", x=\"\") +\n  theme_cowplot() +\n  theme(text = element_text(family = \"serif\", size = 15),\n        axis.text.x = element_text(size=15),\n        legend.position=\"none\")\n\n# Save plot\nggsave(here(\"figures\",\"ManualDistal_Plot.jpeg\"),              \n       dpi=300, width=10, height = 7, units=\"in\") "},{"path":"two-covariates-and-a-single-distal-outcome.html","id":"covariates-relations-1","chapter":"11 Two Covariates and a Single Distal Outcome","heading":"11.3.3.4 Covariates Relations","text":"","code":"\nmodelParams <- readModels(here(\"three_step\", \"manual_3step\", \"three.out\"))\n\n# Extract information as data frame\ncov <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n   filter(str_detect(paramHeader, \"^C#\\\\d+\\\\.ON$\")) %>% \n  mutate(param = str_replace(param, \"FEMALE\", \"Gender\")) %>% # Change this to your own covariates\n  mutate(param = str_replace(param, \"MOTHED\", \"Mother's Education\")) %>%\n  mutate(est = format(round(est, 3), nsmall = 3),\n         se = round(se, 2),\n         pval = round(pval, 3)) %>% \n  mutate(latent_class = str_replace(paramHeader, \"^C#(\\\\d+)\\\\.ON$\", \"Class \\\\1\")) %>% \n  dplyr::select(param, est, se, pval, latent_class) %>% \n  mutate(se = paste0(\"(\", format(round(se,2), nsmall =2), \")\")) %>% \n  unite(logit, est, se, sep = \" \") %>% \n  dplyr::select(param, logit, pval, latent_class) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001)))) \n\nor <- as.data.frame(modelParams[[\"parameters\"]][[\"odds\"]]) %>%\n  filter(str_detect(paramHeader, \"^C#\\\\d+\\\\.ON$\")) %>% \n  mutate(param = str_replace(param, \"FEMALE\", \"Gender\")) %>% # Change this to your own covariates\n  mutate(param = str_replace(param, \"MOTHED\", \"Mother's Education\")) %>%\n  mutate(est = format(round(est, 3), nsmall = 3)) %>% \n  mutate(latent_class = str_replace(paramHeader, \"^C#(\\\\d+)\\\\.ON$\", \"Class \\\\1\")) %>% \n  mutate(CI = paste0(\"[\", format(round(lower_2.5ci, 3), nsmall = 3), \", \", format(round(upper_2.5ci, 3), nsmall = 3), \"]\")) %>% \n  dplyr::select(param, est, CI, latent_class) %>% \n  rename(or = est)\n  \ncombined <- or %>% \n  full_join(cov) %>% \n  dplyr::select(param, latent_class, logit, pval, or, CI)\n\n\n# Create table\n\ncombined %>% \n  gt(groupname_col = \"latent_class\", rowname_col = \"param\") %>%\n  tab_header(\n    title = \"Predictors of Class Membership\") %>%\n  cols_label(\n    logit = md(\"Logit (*se*)\"),\n    or = md(\"Odds Ratio\"),\n    CI = md(\"95% CI\"),\n    pval = md(\"*p*-value\")) %>% \n  sub_missing(1:3,\n              missing_text = \"\") %>%\n  sub_values(values = c(\"999.000\"), replacement = \"-\") %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\") %>%   \n  tab_footnote(\n    footnote = \"Reference Class: 4\",\n    locations = cells_title(groups = \"title\")\n  )"},{"path":"two-covariates-and-a-single-distal-outcome.html","id":"distal-outcome-regressed-on-the-covariate","chapter":"11 Two Covariates and a Single Distal Outcome","heading":"11.3.3.5 Distal outcome regressed on the covariate","text":"relation distal outcome (Math IRT Scores) covariate (Gender)?","code":"\nmodelParams <- readModels(here(\"three_step\", \"manual_3step\", \"three.out\"))\n\n# Extract information as data frame\ndonx <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n  filter(param %in% c(\"FEMALE\", \"MOTHED\")) %>% \n  mutate(param = str_replace(param, \"FEMALE\", \"Gender\")) %>% \n  mutate(param = str_replace(param, \"MOTHED\", \"Mother's Education\")) %>%\n  mutate(LatentClass = sub(\"^\",\"Class \", LatentClass)) %>%  \n  dplyr::select(!paramHeader) %>% \n  mutate(se = paste0(\"(\", format(round(se,2), nsmall =2), \")\")) %>% \n    unite(estimate, est, se, sep = \" \") %>% \n  dplyr::select(param, estimate, pval) %>% \n  distinct(param, .keep_all=TRUE) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001))))\n\n\n# Create table\n\ndonx %>% \n  gt(groupname_col = \"LatentClass\", rowname_col = \"param\") %>%\n  tab_header(\n    title = \"Gender Predicting Math Scores\") %>%\n  cols_label(\n    estimate = md(\"Estimate (*se*)\"),\n    pval = md(\"*p*-value\")) %>% \n  sub_missing(1:3,\n              missing_text = \"\") %>%\n  sub_values(values = c(\"999.000\"), replacement = \"-\") %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\")"},{"path":"moderation.html","id":"moderation","chapter":"12 Moderation","heading":"12 Moderation","text":"Example: Longitudinal Study American YouthData source: : See documentation ","code":""},{"path":"moderation.html","id":"load-packages-4","chapter":"12 Moderation","heading":"12.1 Load Packages","text":"","code":"\nlibrary(MplusAutomation)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(glue)\nlibrary(gt)\nlibrary(cowplot)\nlibrary(kableExtra)\nlibrary(psych)\nlibrary(float)\nlibrary(janitor)"},{"path":"moderation.html","id":"moderation-path-diagram","chapter":"12 Moderation","heading":"12.2 Moderation Path Diagram","text":"Read LSAY dataset","code":"\ndata <- read_csv(here(\"data\",\"lsay_subset.csv\")) %>% \n  clean_names() %>%   # make variable names lowercase\n  mutate(female = recode(gender, `1` = 0, `2` = 1)) # relabel values from 1,2 to 0,1"},{"path":"moderation.html","id":"descriptive-statistics-9","chapter":"12 Moderation","heading":"12.3 Descriptive Statistics","text":"","code":""},{"path":"moderation.html","id":"descriptive-statistics-using-r","chapter":"12 Moderation","heading":"12.3.1 Descriptive Statistics using R:","text":"Quick view relevant variables:Proportion indicators using R:","code":"\ndata %>% \n  select(enjoy, useful, logical, job, adult, female, math_irt) %>% \n  describe()\n# Set up data to find proportions of binary indicators\nds <- data %>% \n  pivot_longer(c(enjoy, useful, logical, job, adult), names_to = \"Variable\") \n\n# Create table of variables and counts\ntab <- table(ds$Variable, ds$value)\n\n# Find proportions and round to 3 decimal places\nprop <- prop.table(tab, margin = 1) %>% \n  round(3)\n\n# Combine everything to one table \ndframe <- data.frame(Variables=rownames(tab), Proportion=prop[,2], Count=tab[,2])\n#remove row names\nrow.names(dframe) <- NULL\ngt(dframe) %>% \ntab_header(title = md(\"**LCA Indicator Proportions**\"), subtitle = md(\"&nbsp;\")) %>%\ntab_options(column_labels.font.weight = \"bold\", row_group.font.weight = \"bold\") "},{"path":"moderation.html","id":"descriptive-statistics-using-mplusautomation","chapter":"12 Moderation","heading":"12.3.2 Descriptive Statistics using MplusAutomation:","text":"View .file:, view descriptive statistics using get_sampstat():","code":"\nbasic_mplus  <- mplusObject(\n  TITLE = \"LSAL Descriptive Statistics;\",\n  \n  VARIABLE =\n    \"usevar = enjoy, useful, logical, job, adult, female, math_irt;\n    categorical = enjoy, useful, logical, job, adult, female;\",\n\n  ANALYSIS = \"TYPE=basic;\",\n  \n  OUTPUT = \"sampstat;\",  \n  \n  usevariables = colnames(data),\n  rdata = data)\n\nbasic_mplus_fit <- mplusModeler(basic_mplus, \n                            dataout = here(\"moderation\", \"LSAL_data.dat\"),\n                            modelout = here(\"moderation\",\"basic.inp\"),\n                            check = TRUE, run = TRUE, hashfilename = FALSE)\n# Using MplusAutomation\nMplusAutomation::get_sampstat(basic_mplus_fit)\n\n# Using base R\nsummary(data)"},{"path":"moderation.html","id":"enumeration-6","chapter":"12 Moderation","heading":"12.4 Enumeration","text":"code uses mplusObject function MplusAutomation package saves model runs mplus_enum folder.IMPORTANT: moving forward, make sure examine output document ensure models estimated normally. example, last model (6-class models) produce reliable output excluded.","code":"\n\nlca_enum_6  <- lapply(1:6, function(k) {\n  lca_enum  <- mplusObject(\n    \n    TITLE = glue(\"{k}-Class\"), \n    \n    VARIABLE = glue(\n      \"categorical = enjoy, useful, logical, job, adult; \n     usevar = enjoy, useful, logical, job, adult;\n     classes = c({k});\"),\n    \n    ANALYSIS = \n      \"estimator = mlr; \n    type = mixture;\n    processors = 12;\n    starts = 500 100;\",\n    \n    OUTPUT = \"sampstat residual tech11 tech14;\",\n\n    usevariables = colnames(data),\n    rdata = data)\n  \n  lca_enum_fit <- mplusModeler(lca_enum, \n                               dataout=glue(here(\"moderation\",\"enum\", \"LSAY_data.dat\")),\n                               modelout=glue(here(\"moderation\",\"enum\", \"c{k}_lsal.inp\")) ,\n                               check=TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"moderation.html","id":"table-of-fit-3","chapter":"12 Moderation","heading":"12.4.1 Table of Fit","text":"First, extract data:","code":"\nsource(here(\"functions\", \"extract_mplus_info.R\"))\n\n# Define the directory where all of the .out files are located.\noutput_dir <- here(\"moderation\",\"enum\")\n\n# Get all .out files\noutput_files <- list.files(output_dir, pattern = \"\\\\.out$\", full.names = TRUE)\n\n# Process all .out files into one dataframe\nfinal_data <- map_dfr(output_files, extract_mplus_info_extended)\n\n# Extract Sample_Size from final_data\nsample_size <- unique(final_data$Sample_Size)"},{"path":"moderation.html","id":"examine-mplus-warnings-1","chapter":"12 Moderation","heading":"12.4.1.1 Examine Mplus Warnings","text":"","code":"\nsource(here(\"functions\", \"extract_warnings.R\"))\n\nwarnings_table <- extract_warnings(final_data)\nwarnings_table\n\n# Save the warnings table\n#gtsave(warnings_table, here(\"figures\", \"warnings_table.png\"))"},{"path":"moderation.html","id":"examine-mplus-errors-1","chapter":"12 Moderation","heading":"12.4.1.2 Examine Mplus Errors","text":"","code":"\nsource(here(\"functions\", \"error_visualization.R\"))\n\n# Process errors\nerror_table_data <- process_error_data(final_data)\nerror_table_data\n\n# Save the errors table\n#gtsave(error_table, here(\"figures\", \"error_table.png\"))"},{"path":"moderation.html","id":"examine-convergence-and-loglikelihood-replications-1","chapter":"12 Moderation","heading":"12.4.1.3 Examine Convergence and Loglikelihood Replications","text":"N = 2675Random StartsFinal starting value sets convergingLL ReplicationSmallest ClassModelBest LLnparInitialFinalf%f%f%1-Class-8,150.3515500100100100%100100.0%2,675100.0%2-Class-7,191.87811500100100100%100100.0%80330.0%3-Class-7,124.921175001006868%6494.1%37213.9%4-Class-7,095.123235001003535%3085.7%2629.8%5-Class-7,091.946295001003434%617.6%30611.4%6-Class-7,090.886355001004141%922.0%1385.2%","code":"\nsource(here(\"functions\", \"summary_table.R\"))\n\n# Print Table with Superheader & Heatmap\nsummary_table <- create_flextable(final_data, sample_size)\nsummary_table\n\n# Save the flextable as a PNG image\n#invisible(save_as_image(summary_table, path = here(\"figures\", \"housekeeping.png\")))"},{"path":"moderation.html","id":"final-fit-table","chapter":"12 Moderation","heading":"12.4.1.4 Final Fit Table","text":"First, extract data:Save table:","code":"\nsource(here(\"functions\", \"enum_table_lca.R\"))\n\noutput_enum <- readModels(here(\"moderation\", \"enum\"), quiet = TRUE)\n\nfit_table <- fit_table_lca(output_enum, final_data)\nfit_table\ngtsave(fit_table, here(\"figures\", \"fit_table_lca.png\"))"},{"path":"moderation.html","id":"information-criteria-plot-3","chapter":"12 Moderation","heading":"12.4.2 Information Criteria Plot","text":"Save figure:","code":"\nsource(here(\"functions\", \"ic_plot_lca.R\"))\nic_plot(output_enum)\nggsave(here(\"figures\", \"info_criteria_moderation.png\"),  dpi = \"retina\", bg = \"white\", height=5, width=7, units=\"in\")"},{"path":"moderation.html","id":"compare-class-solutions-1","chapter":"12 Moderation","heading":"12.4.3 Compare Class Solutions","text":"Compare probability plots \\(K = 1:5\\) class solutionsSave figure:","code":"\nmodel_results <- data.frame()\n\nfor (i in 1:length(output_enum)) {\n  temp <- output_enum[[i]]$parameters$probability.scale %>%\n    mutate(model = paste0(i, \"-Class Model\"))\n  \n  model_results <- rbind(model_results, temp)\n}\n\ncompare_plot <-\n  model_results %>%\n  filter(category == 2) %>%\n  dplyr::select(est, model, LatentClass, param) %>%\n  filter(model != \"6-Class Model\") #Remove from plot\n\ncompare_plot$param <- fct_inorder(compare_plot$param)\n\nggplot(\n  compare_plot,\n  aes(\n    x = param,\n    y = est,\n    color = LatentClass,\n    shape = LatentClass,\n    group = LatentClass,\n    lty = LatentClass\n  )\n) +\n  geom_point() +\n  geom_line() +\n  scale_colour_viridis_d() +\n  facet_wrap(~ model, ncol = 2) +\n  labs(title = \"Math Attitude Items\", x = \" \", y = \"Probability\") +\n  theme_minimal() +\n  theme(panel.grid.major.y = element_blank(),\n        axis.text.x = element_text(angle = -45, hjust = -.1))                            \nggsave(here(\"figures\", \"compare_kclass_plot_mod.png\"), dpi = \"retina\", bg = \"white\", height=5, width=7, units=\"in\")"},{"path":"moderation.html","id":"class-probability-plot-2","chapter":"12 Moderation","heading":"12.4.4 4-Class Probability Plot","text":"Use plot_lca function provided folder plot item probability plot. function requires one argument:\n- model_name: name Mplus readModels object (e.g., output_lsal$c4_lsal.)Save figure:","code":"\nsource(here(\"functions\",\"plot_lca.R\"))\n\nplot_lca(model_name = output_enum$c4_lsal.out)\nggsave(here(\"figures\", \"probability_plot_mod.png\"), dpi = \"retina\", bg = \"white\", height=5, width=7, units=\"in\")"},{"path":"moderation.html","id":"lca-moderation---ml-three-step","chapter":"12 Moderation","heading":"12.5 LCA Moderation - ML Three-Step","text":"","code":""},{"path":"moderation.html","id":"step-1---estimate-unconditional-model-w-auxiliary-specification","chapter":"12 Moderation","heading":"12.5.1 Step 1 - Estimate Unconditional Model w/ Auxiliary Specification","text":"Note: Ensure classes shift step (.g., Class 1 enumeration run now Class 4). Evaluate output compare class counts proportions latent classes. Using OPTSEED function ensures replication best loglikelihood value run.selecting latent class model, add class labels item probability plot using plot_lca_labels function. function requires three arguments:model_name: Mplus readModels object (e.g., output_lsal$c4_lsal.)item_labels: item labels x-axis (e.g.,c(“Enjoy”,“Useful”,“Logical”,“Job”,“Adult”))class_labels: class labels (e.g., c(“Pro-Science w/ Elevated Utility Value”, “Ambivalent w/ Minimal Utility Value”, “Ambivalent w/ Elevated Utility Value”, “Anti-Science w/ Minimal Utility Value”))Note: Use \\n add return label lengthy.","code":"\n\nstep1  <- mplusObject(\n  TITLE = \"Step 1 - Unconditional Model w/ Auxiliary Specification\", \n  VARIABLE = \"categorical = enjoy, useful, logical, job, adult;\n  usevar =  enjoy, useful, logical, job, adult;\n  classes = c(4);\n  AUXILIARY = female math_irt;\",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 0;\n    OPTSEED = 813779;\",\n  \n  SAVEDATA = \n   \"File=savedata.dat;\n    Save=cprob;\n    format=free;\",\n  \n  OUTPUT = \"sampstat residual tech11 tech14\",\n  \n  usevariables = colnames(data),\n  rdata = data)\n\nstep1_fit <- mplusModeler(step1,\n                            dataout=here(\"moderation\", \"three_step\", \"new.dat\"),\n                            modelout=here(\"moderation\", \"three_step\", \"one.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\nsource(here(\"functions\",\"plot_lca_labels.R\"))\n\n# Read in output from step 1.\noutput_one <- readModels(here(\"moderation\",\"three_step\",\"one.out\"))\n\n# Plot Title\ntitle <- \"LCA Probability Plot - LSAL\"\n\n#Identify item and class labels (Make sure they are in the order presented in the plot above)\nitem_labels <-  c(\n  \"I Enjoy \\nScience\",\n  \"Science is Useful \\nin Everyday Problems\",\n  \"Science Helps \\nLogical Thinking\",\n  \"Need Science for \\na Good Job\",\n  \"Will Use Science \\nOften as an Adult\"\n)\n\nclass_labels <- c(\n\n  \"Ambivalent w/ \\nElevated Utility Value\",\n  \"Anti-Science w/ \\nMinimal Utility Value\",\n  \"Ambivalent w/ \\nMinimal Utility Value\",\n  \"Pro-Science w/ \\nElevated Utility Value\"\n)\n\n# Plot LCA plot\nplot_lca_labels(model_name = output_one, item_labels, class_labels, title)\n\n# Save\nggsave(here(\"figures\", \"final_probability_plot_mod.png\"), dpi = \"retina\", bg = \"white\", height=7, width=10, units=\"in\")"},{"path":"moderation.html","id":"step-2---determine-measurement-error-3","chapter":"12 Moderation","heading":"12.5.2 Step 2 - Determine Measurement Error","text":"Extract logits classification probabilities likely latent class:Extract saved dataset step one:Rename column savedata named “C” change “N”:","code":"\nlogit_cprobs <- as.data.frame(output_one[[\"class_counts\"]]\n                                       [[\"logitProbs.mostLikely\"]])\nsavedata <- as.data.frame(output_one[[\"savedata\"]])\ncolnames(savedata)[colnames(savedata)==\"C\"] <- \"N\""},{"path":"moderation.html","id":"step-3---add-auxiliary-variables","chapter":"12 Moderation","heading":"12.5.3 Step 3 - Add Auxiliary Variables","text":"Build moderation model:","code":"\nstep3mod  <- mplusObject(\n  TITLE = \"LCA Moderation\", \n  \n  VARIABLE = \n \"nominal=N;\n  usevar = n;\n\n  classes = c(4);\n  \n  usevar = female math_irt;\" ,\n  \n  ANALYSIS = \n \"estimator = mlr; \n  type = mixture; \n  starts = 0;\",\n \n  MODEL =\n  glue(\n \"!DISTAL = math_irt, COVARIATE = female, MODERATOR = C\n \n  %OVERALL%\n  math_irt on female;\n  math_irt;\n\n     %C#1%\n  [n#1@{logit_cprobs[1,1]}];\n  [n#2@{logit_cprobs[1,2]}];\n  [n#3@{logit_cprobs[1,3]}];\n  \n  math_irt on female(s1);  ! conditional slope (class 1)\n  [math_irt](m1);          ! conditional distal mean\n  math_irt;                ! conditional distal variance (freely estimated)\n\n  %C#2%\n  [n#1@{logit_cprobs[2,1]}];\n  [n#2@{logit_cprobs[2,2]}];\n  [n#3@{logit_cprobs[2,3]}];\n  \n  math_irt on female(s2);\n  [math_irt](m2);\n  math_irt;\n  \n  %C#3%\n  [n#1@{logit_cprobs[3,1]}];\n  [n#2@{logit_cprobs[3,2]}];\n  [n#3@{logit_cprobs[3,3]}];\n  \n  math_irt on female(s3);\n  [math_irt](m3);\n  math_irt;\n\n  %C#4%\n  [n#1@{logit_cprobs[4,1]}];\n  [n#2@{logit_cprobs[4,2]}];\n  [n#3@{logit_cprobs[4,3]}];\n  \n  math_irt on female(s4);\n  [math_irt](m4);\n  math_irt; \"),\n  \n  MODELCONSTRAINT = \n   \"New (\n   diff12 diff13 diff23\n   diff14 diff24 diff34\n   slope12 slope13 slope23 \n    slope14 slope24 slope34);\n    \n    diff12 = m1-m2;  ! test distal outcome differences\n    diff13 = m1-m3;\n    diff23 = m2-m3;\n    diff14 = m1-m4;\n    diff24 = m2-m4;\n    diff34 = m3-m4;\n  \n    slope12 = s1-s2;  ! test pairwise slope differences\n    slope13 = s1-s3;\n    slope23 = s2-s3;\n    slope14 = s1-s4;\n    slope24 = s2-s4;\n    slope34 = s3-s4;\",\n  \n  MODELTEST = \" ! can run only a single Omnibus test per model \n    s1=s2;\n    s2=s3;\n    s3=s4;\",\n \n  usevariables = colnames(savedata), \n  rdata = savedata)\n\nstep3mod_fit <- mplusModeler(step3mod,\n               dataout=here(\"moderation\", \"three_step\", \"mod.dat\"), \n               modelout=here(\"moderation\", \"three_step\", \"three.inp\"), \n               check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"moderation.html","id":"wald-test-table-2","chapter":"12 Moderation","heading":"12.5.3.1 Wald Test Table","text":"","code":"\nmodelParams <- readModels(here(\"moderation\", \"three_step\", \"three.out\"))\n\n# Extract information as data frame\nwald <- as.data.frame(modelParams[[\"summaries\"]]) %>%\n  dplyr::select(WaldChiSq_Value:WaldChiSq_PValue) %>% \n  mutate(WaldChiSq_DF = paste0(\"(\", WaldChiSq_DF, \")\")) %>% \n  unite(wald_test, WaldChiSq_Value, WaldChiSq_DF, sep = \" \") %>% \n  rename(pval = WaldChiSq_PValue) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\".001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001))))\n\n# Create table\n\nwald %>% \n  gt() %>%\n    tab_header(\n    title = \"Wald Test of Paramter Constraints (Slope)\") %>%\n    cols_label(\n      wald_test = md(\"Wald Test (*df*)\"),\n      pval = md(\"*p*-value\")) %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\")"},{"path":"moderation.html","id":"table-of-slope-and-intercept-values-across-classes","chapter":"12 Moderation","heading":"12.5.3.2 Table of Slope and Intercept Values Across Classes","text":"","code":"\nmodelParams <- readModels(here(\"moderation\", \"three_step\", \"three.out\"))\n\n# Change these to how the variables are written in Mplus\nx <- \"FEMALE\"\ny <- \"MATH_IRT\"\n\n# Extract information as data frame\nvalues <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n  filter(param %in% c(x, y),\n         paramHeader != \"Residual.Variances\") %>% \n  mutate(param = str_replace(param, pattern = x, replacement = \"Slope\"),\n         param = str_replace(param, pattern = y, replacement = \"Intercept\")) %>% \n  mutate(LatentClass = sub(\"^\",\"Class \", LatentClass)) %>%  \n  dplyr::select(!paramHeader) %>% \n  mutate(se = paste0(\"(\", format(round(se,2), nsmall =2), \")\")) %>% \n    unite(estimate, est, se, sep = \" \") %>% \n  select(!est_se) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001))))\n\n\n# Create table\n\nvalues %>% \n  gt(groupname_col = \"LatentClass\", rowname_col = \"param\") %>%\n  tab_header(\n    title = \"Slope and Intercept Values Across Science Attitudes Classes\") %>%\n  cols_label(\n    estimate = md(\"Estimate (*se*)\"),\n    pval = md(\"*p*-value\")) %>% \n  sub_values(values = \"999.000\", replacement = \"-\") %>% \n  sub_missing(1:3,\n              missing_text = \"\") %>%\n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\")"},{"path":"moderation.html","id":"table-of-distal-outcome-differences","chapter":"12 Moderation","heading":"12.5.3.3 Table of Distal Outcome Differences","text":"","code":"\nmodelParams <- readModels(here(\"moderation\", \"three_step\", \"three.out\"))\n\n# Extract information as data frame\ndiff <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n  filter(grepl(\"DIFF\", param)) %>% \n  dplyr::select(param:pval) %>% \n  mutate(se = paste0(\"(\", format(round(se,2), nsmall =2), \")\")) %>% \n  unite(estimate, est, se, sep = \" \") %>% \n  mutate(param = str_remove(param, \"DIFF\"),\n         param = as.numeric(param)) %>% \n  separate(param, into = paste0(\"Group\", 1:2), sep = 1) %>% \n  mutate(class = paste0(\"Class \", Group1, \" vs \", Group2)) %>% \n  select(class, estimate, pval)\n\n\n# Create table\n\ndiff %>% \n  gt() %>%\n    tab_header(\n    title = \"Distal Outcome Differences\") %>%\n    cols_label(\n      class = \"Class\",\n      estimate = md(\"Mean (*se*)\"),\n      pval = md(\"*p*-value\")) %>% \n    sub_missing(1:3,\n              missing_text = \"\") %>%\n  fmt(3,\n    fns = function(x)\n      ifelse(x<0.05, paste0(scales::number(x, accuracy = .001), \"*\"),\n             scales::number(x, accuracy = .001))\n  ) %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\")"},{"path":"moderation.html","id":"table-of-slope-differences","chapter":"12 Moderation","heading":"12.5.3.4 Table of Slope Differences","text":"","code":"\nmodelParams <- readModels(here(\"moderation\", \"three_step\", \"three.out\"))\n\n# Extract information as data frame\ndiff2 <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n  filter(grepl(\"SLOPE\", param)) %>% \n  dplyr::select(param:pval) %>% \n  mutate(se = paste0(\"(\", format(round(se,2), nsmall =2), \")\")) %>% \n  unite(estimate, est, se, sep = \" \") %>% \n  mutate(param = str_remove(param, \"SLOPE\"),\n         param = as.numeric(param)) %>% \n  separate(param, into = paste0(\"Group\", 1:2), sep = 1) %>% \n  mutate(class = paste0(\"Class \", Group1, \" vs \", Group2)) %>% \n  select(class, estimate, pval) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001))))\n\n\n# Create table\n\ndiff2 %>% \n  gt() %>%\n    tab_header(\n    title = \"Slope Differences\") %>%\n    cols_label(\n      class = \"Class\",\n      estimate = md(\"Mean (*se*)\"),\n      pval = md(\"*p*-value\")) %>% \n    sub_missing(1:3,\n              missing_text = \"\") %>%\n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\")"},{"path":"moderation.html","id":"table-of-covariates","chapter":"12 Moderation","heading":"12.5.3.5 Table of Covariates","text":"","code":"\nmodelParams <- readModels(here(\"moderation\", \"three_step\", \"three.out\"))\n\nx <- \"FEMALE\"\nrename <- \"Gender\"\n\n# Extract information as data frame\ncov <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n  filter(param %in% c(x)) %>% \n  mutate(param = str_replace(param, x, rename)) %>% \n  mutate(LatentClass = sub(\"^\",\"Class \", LatentClass)) %>%  \n  dplyr::select(!paramHeader) %>% \n  mutate(se = paste0(\"(\", format(round(se,2), nsmall =2), \")\")) %>% \n    unite(estimate, est, se, sep = \" \") %>% \n  select(param, estimate, pval) %>% \n  distinct(param, .keep_all=TRUE)  %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001))))\n\n\n# Create table\n\ncov %>% \n  gt(groupname_col = \"LatentClass\", rowname_col = \"param\") %>%\n  tab_header(\n    title = \"Relations Between the Covariates and Distal Outcome\") %>%\n  cols_label(\n    estimate = md(\"Estimate (*se*)\"),\n    pval = md(\"*p*-value\")) %>% \n  sub_missing(1:3,\n              missing_text = \"\") %>%\n  sub_values(values = c(999.000), replacement = \"-\") %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\")"},{"path":"moderation.html","id":"plot-distal-outcome","chapter":"12 Moderation","heading":"12.5.3.6 Plot Distal Outcome","text":"","code":"\nmodelParams <- readModels(here(\"moderation\", \"three_step\", \"three.out\"))\n\ny <- \"MATH_IRT\"\n\n# Extract class size \nc_size <- as.data.frame(modelParams[[\"class_counts\"]][[\"modelEstimated\"]][[\"proportion\"]]) %>% \n  rename(\"cs\" = 1) %>% \n  mutate(cs = round(cs*100, 2))\n\n# Keep this code if you want a generic label for the classes\n#c_size_val <- paste0(\"C\", 1:nrow(c_size), glue(\" ({c_size[1:nrow(c_size),]}%)\"))\n\n# Otherwise use this:\nc_size_val <- paste0(class_labels, glue(\" ({c_size[1:nrow(c_size),]}%)\"))\n\n\n# Extract information as data frame\nestimates <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n  filter(paramHeader == \"Intercepts\") %>%\n  dplyr::select(param, est, se) %>% \n  filter(param == y) %>% \n  mutate(across(c(est, se), as.numeric)) %>% \n  mutate(LatentClass = c_size_val)\n\n# Plot bar graphs\nestimates %>%\n  ggplot(aes(x=LatentClass, y = est, fill = LatentClass)) +\n  geom_col(position = \"dodge\", stat = \"identity\", color = \"black\") +\n  geom_errorbar(aes(ymin=est-se, ymax=est+se),\n                size=.3,    # Thinner lines\n                width=.2,\n                position=position_dodge(.9)) +\n  geom_text(aes(label = est), \n            family = \"serif\", size = 4,\n            position=position_dodge(.9),\n            vjust = 8) +  \n  #scale_fill_grey(start = .4, end = .7) +\n  labs(y=\"Math Score\", x=\"\") +\n  theme_cowplot() +\n  theme(text = element_text(family = \"serif\", size = 12),\n        axis.text.x = element_text(size=12),\n        legend.position=\"none\") +\n  coord_cartesian(ylim=c(0,80),  # Change ylim based on distal outcome range\n                  expand = FALSE) \n\n\n# Save plot\nggsave(here(\"figures\",\"distal_plot_mod.jpeg\"), dpi = \"retina\", bg = \"white\", width=10, height = 7, units=\"in\") "},{"path":"moderation.html","id":"plot-interaction","chapter":"12 Moderation","heading":"12.5.4 Plot Interaction","text":"","code":""},{"path":"moderation.html","id":"bar-plot","chapter":"12 Moderation","heading":"12.5.4.1 Bar plot","text":"","code":"\nmodelParams <- readModels(here(\"moderation\", \"three_step\", \"three.out\"))\n\nx <- \"FEMALE\"\n\n# Extract information as data frame\ndesc <- as.data.frame(modelParams$sampstat$univariate.sample.statistics) %>% \n  rownames_to_column(\"Variables\")\n\n# Select min amd max values of covariate\nxmin <- desc %>% \n  filter(Variables == x) %>% \n  dplyr::select(Minimum) %>% \n  as.numeric()\nxmax <- desc %>% \n  filter(Variables == x) %>% \n  dplyr::select(Maximum) %>% \n  as.numeric()\n\n# Add slope and intercept, Min and Max values \nline <- as.data.frame(modelParams$parameters$unstandardized) %>% \n  filter(str_detect(paramHeader, 'ON|Inter')) %>% \n  unite(\"param\", paramHeader:param, remove = TRUE) %>% \n  mutate(param = replace(param,agrep(\".ON\",param),\"slope\"),\n         param = replace(param,agrep(\"Inter\", param), \"intercept\"),\n         LatentClass = factor(LatentClass, labels = c_size_val)) %>% \n  dplyr::select(param, est, LatentClass) %>% \n  pivot_wider(names_from = param, values_from = est) %>%  \n  add_column(x_max = xmax,\n         x_min = xmin)\n\n\n# Add column with y values\ndata <- line %>% \n  mutate(y_min = (slope*xmin) + intercept,\n         y_max = (slope*xmax) + intercept) %>% \n  dplyr::select(-slope, -intercept) %>% \n  pivot_longer(-LatentClass, \n               names_to = c(\"xvalues\", \"yvalues\"), \n               names_sep=\"_\" ) %>% \n  pivot_wider(names_from = xvalues, values_from = value) %>% \n  dplyr::select(-yvalues) %>% \n  mutate(x = case_when(\n           x == 1 ~ \"Female\", ## Change these names\n           x == 0 ~ \"Male\")) ## Change these names\n  \n# Plot bar graphs\ndata %>%\n  ggplot(aes(x=factor(x), y = y, fill = LatentClass)) +\n  geom_col(position = \"dodge\", stat = \"identity\", color = \"black\") +\n  geom_text(aes(label = y), \n            family = \"serif\", size = 4,\n            position=position_dodge(.9),\n            vjust = -.5) +  \n  #scale_fill_grey(start = .4, end = .7) +\n  labs(y=\"Math Score\", x=\"\") +\n  theme_cowplot() +\n  theme(text = element_text(family = \"serif\", size = 12),\n        axis.text.x = element_text(size=12)) +\n  coord_cartesian(ylim=c(0,80),  # Change ylim based on distal outcome range\n                  expand = FALSE) \n\n# Save plot\nggsave(here(\"figures\",\"interaction_mod.jpeg\"), dpi = \"retina\", bg = \"white\", width=10, height = 7, units=\"in\") "},{"path":"moderation.html","id":"line-plot","chapter":"12 Moderation","heading":"12.5.4.2 Line plot","text":"can visualize slopes.’s also important report slope coefficients. ones assume significant based plots?","code":"\n\nx <- \"FEMALE\"\n\n# Minimum and Maximum Values\ndesc <- as.data.frame(modelParams$sampstat$univariate.sample.statistics) %>% \n  rownames_to_column(\"Variables\")\n\n\n# Select min amd max values of covariate\nxmin <- desc %>% \n  filter(Variables == x) %>% \n  dplyr::select(Minimum) %>% \n  as.numeric()\nxmax <- desc %>% \n  filter(Variables == x) %>% \n  dplyr::select(Maximum) %>% \n  as.numeric()\n\n# Add slope and intercept, Min and Max values \nline <- as.data.frame(modelParams$parameters$unstandardized) %>% \n  filter(str_detect(paramHeader, 'ON|Inter')) %>% \n  unite(\"param\", paramHeader:param, remove = TRUE) %>% \n  mutate(param = replace(param,agrep(\".ON\",param),\"slope\"),\n         param = replace(param,agrep(\"Inter\", param), \"intercept\"),\n         LatentClass = factor(LatentClass, labels = c_size_val)) %>% \n  dplyr::select(param, est, LatentClass) %>% \n  pivot_wider(names_from = param, values_from = est) %>%  \n  add_column(x_max = xmax,\n         x_min = xmin)\n\n\n# Add column with y values\ndata <- line %>% \n  mutate(y_min = (slope*xmin) + intercept,\n         y_max = (slope*xmax) + intercept) %>% \n  dplyr::select(-slope, -intercept) %>% \n  pivot_longer(-LatentClass, \n               names_to = c(\"xvalues\", \"yvalues\"), \n               names_sep=\"_\" ) %>% \n  pivot_wider(names_from = xvalues, values_from = value) %>% \n  dplyr::select(-yvalues) %>% \n  mutate(x = case_when(\n           x == 1 ~ \"Female\",\n           x == 0 ~ \"Male\"))\n  \n# Plot \ndata %>%\n  ggplot(aes(\n    x = factor(x),\n    y = y,\n    color = LatentClass,\n    group = LatentClass,\n    lty = LatentClass,\n    shape = LatentClass\n  )) +\n  geom_point(size = 4) +\n  geom_line(aes(group = LatentClass), size = 1) +\n  labs(x = \"\",\n       y = \"Math Score\") +\n  #scale_colour_grey(start = .3, end = .6) +\n  theme_cowplot() +\n  theme(\n    text = element_text(family = \"serif\", size = 12),\n    axis.text.x = element_text(size = 12),\n    legend.text = element_text(family = \"serif\", size = 10),\n    legend.position = \"top\",\n    legend.title = element_blank()\n  ) \n\n# Save\nggsave(here(\"figures\",\"slope_plot_mod.jpeg\"),  dpi = \"retina\", bg = \"white\", width=10, height = 7, units=\"in\")   "},{"path":"lta.html","id":"lta","chapter":"13 Latent Transition Analysis","heading":"13 Latent Transition Analysis","text":"Data Source: data used illustrate analyses include elementary school student Science Attitude survey items collected 7th 10th grades Longitudinal Study American Youth (LSAY; Miller, 2015).install package {rhdf5}Load packagesRead LSAY data file, lsay_new.csv.","code":"\nif (!requireNamespace(\"BiocManager\", quietly = TRUE)) \n  install.packages(\"BiocManager\")\n\n#BiocManager::install(\"rhdf5\")\nlibrary(MplusAutomation)\nlibrary(rhdf5)\nlibrary(tidyverse)       \nlibrary(here)            \nlibrary(glue)            \nlibrary(janitor)            \nlibrary(gt) \nlibrary(reshape2)\nlibrary(cowplot)\nlibrary(ggrepel)\nlibrary(haven)\nlibrary(modelsummary)\nlibrary(corrplot)\nlibrary(DiagrammeR)\nlibrary(filesstrings)\nlibrary(PNWColors)\n\nlsay_data <- read_csv(here(\"data\",\"lsay_lta.csv\"), na = c(\"9999\")) %>% \n    mutate(across(everything(), as.numeric))"},{"path":"lta.html","id":"descriptive-statistics-10","chapter":"13 Latent Transition Analysis","heading":"13.1 Descriptive Statistics","text":"","code":""},{"path":"lta.html","id":"data-summary","chapter":"13 Latent Transition Analysis","heading":"13.1.1 Data Summary","text":"","code":"\ndata <- lsay_data\n\nselect_data <- data %>% \n select(female, minority, ab39m:ga33l)\n\nf <- All(select_data) ~ Mean + SD + Min + Median + Max + Histogram\ndatasummary(f, data, output=\"markdown\")"},{"path":"lta.html","id":"correlation-table","chapter":"13 Latent Transition Analysis","heading":"13.1.2 Correlation Table","text":"","code":"\nselect_data %>% \n  datasummary_correlation(output = \"markdown\")"},{"path":"lta.html","id":"correlation-plot","chapter":"13 Latent Transition Analysis","heading":"13.1.3 Correlation Plot","text":"","code":"\nf_cor <- data %>% \n select(female, minority, ab39m:ga33l) %>% \n  cor(use = \"pairwise.complete.obs\")\n\ncorrplot(f_cor, \n         method = \"circle\",\n         type = \"upper\", \n         tl.col=\"black\", \n         tl.srt=45)"},{"path":"lta.html","id":"enumeration-7","chapter":"13 Latent Transition Analysis","heading":"13.2 Enumeration","text":"","code":""},{"path":"lta.html","id":"enumerate-time-point-1-7th-grade","chapter":"13 Latent Transition Analysis","heading":"13.2.1 Enumerate Time Point 1 (7th grade)","text":"NEXT STEP Check output (.) files check convergence warnings syntax errors.","code":"\n# NOTE CHANGE: '1:6' indicates the number of k-class models to estimate\n# User can change this number to fit research context\n# In this example, the code loops or iterates over values 1 through 6 ( '{k}' )\n#\nt1_enum_k_16  <- lapply(1:6, function(k) { \n  enum_t1  <- mplusObject(                 \n    \n# The 'glue' function inserts R code within a string or \"quoted green text\" using the syntax {---}\n#\n    TITLE = glue(\"Class-{k}_Time1\"), \n  \n    VARIABLE = glue( \n    \"!!! NOTE CHANGE: List of the five 7th grade science attitude indicators !!!\n     categorical = ab39m-ab39x; \n          usevar = ab39m-ab39x;\n     \n     classes = c({k});\"),\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    !!! NOTE CHANGE: The intial and final start values. Reduce to speed up estimation time. !!!\n    starts = 500 100;           \n    processors=10;\",\n  \n  OUTPUT = \"sampstat residual tech11 tech14;\",\n  \n  PLOT = \n    \"type = plot3; \n    series = ab39m-ab39x(*);\",\n  \n  usevariables = colnames(lsay_data),\n  rdata = lsay_data)\n\n# NOTE CHANGE: Fix to match appropriate sub-folder name\n# See after `here` function (e.g., \"enum_LCA_time1\")\nenum_t1_fit <- mplusModeler(enum_t1,\n                 dataout=here(\"lta\",\"enum_t1\",\"t1.dat\"), \n                 modelout=glue(here(\"lta\",\"enum_t1\",\"c{k}_lca_enum_time1.inp\")),\n                 check=TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"lta.html","id":"enumerate-time-point-2-10th-grade","chapter":"13 Latent Transition Analysis","heading":"13.2.2 Enumerate Time Point 2 (10th grade)","text":"","code":"\n\nt2_enum_k_16  <- lapply(1:6, function(k) { \n  enum_t2  <- mplusObject(                 \n      \n    TITLE = glue(\"Class-{k}_Time2\"), \n  \n    VARIABLE = \n  glue( \n    \"!!! NOTE CHANGE: List of the five 10th grade science attitude indicators !!!\n     categorical = ga33a-ga33l; \n          usevar = ga33a-ga33l;\n    \n     classes = c({k});\"),\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 500 100;\n    processors=10;\",\n  \n  OUTPUT = \"sampstat residual tech11 tech14;\",\n  \n  PLOT = \n    \"type = plot3; \n    series = ga33a-ga33l(*);\",\n  \n  usevariables = colnames(lsay_data),\n  rdata = lsay_data)\n\nenum_t2_fit <- mplusModeler(enum_t2, \n                 dataout=here(\"lta\",\"enum_t2\",\"t2.dat\"),\n                 modelout=glue(here(\"lta\",\"enum_t2\",\"c{k}_lca_enum_time2.inp\")),\n                 check=TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"lta.html","id":"create-model-fit-summary-table","chapter":"13 Latent Transition Analysis","heading":"13.2.3 Create Model Fit Summary Table","text":"Read models enumeration tableExtract model fit data","code":"\noutput_enum_t1 <- readModels(here(\"lta\",\"enum_t1\"), quiet = TRUE)\noutput_enum_t2 <- readModels(here(\"lta\",\"enum_t2\"), quiet = TRUE)\n\nenum_extract1 <- LatexSummaryTable(output_enum_t1,                                 \n                keepCols=c(\"Title\", \"Parameters\", \"LL\", \"BIC\", \"aBIC\",\n                           \"BLRT_PValue\", \"T11_VLMR_PValue\",\"Observations\"))   \n\nenum_extract2 <- LatexSummaryTable(output_enum_t2,                                 \n                keepCols=c(\"Title\", \"Parameters\", \"LL\", \"BIC\", \"aBIC\",\n                           \"BLRT_PValue\", \"T11_VLMR_PValue\",\"Observations\")) "},{"path":"lta.html","id":"calculate-indices-derived-from-the-log-likelihood-ll","chapter":"13 Latent Transition Analysis","heading":"13.2.4 Calculate Indices Derived from the Log Likelihood (LL)","text":"","code":"\n                           \nallFit1 <- enum_extract1 %>% \n  mutate(aBIC = -2*LL+Parameters*log((Observations+2)/24)) %>% \n  mutate(CAIC = -2*LL+Parameters*(log(Observations)+1)) %>% \n  mutate(AWE = -2*LL+2*Parameters*(log(Observations)+1.5)) %>%\n  mutate(SIC = -.5*BIC) %>% \n  mutate(expSIC = exp(SIC - max(SIC))) %>% \n  mutate(BF = exp(SIC-lead(SIC))) %>% \n  mutate(cmPk = expSIC/sum(expSIC)) %>% \n  select(1:5,9:10,6:7,13,14) %>% \n  arrange(Parameters)\n\nallFit2 <- enum_extract2 %>% \n  mutate(aBIC = -2*LL+Parameters*log((Observations+2)/24)) %>% \n  mutate(CAIC = -2*LL+Parameters*(log(Observations)+1)) %>% \n  mutate(AWE = -2*LL+2*Parameters*(log(Observations)+1.5)) %>%\n  mutate(SIC = -.5*BIC) %>% \n  mutate(expSIC = exp(SIC - max(SIC))) %>% \n  mutate(BF = exp(SIC-lead(SIC))) %>% \n  mutate(cmPk = expSIC/sum(expSIC)) %>% \n  select(1:5,9:10,6:7,13,14) %>% \n  arrange(Parameters)\n\nallFit <- full_join(allFit1,allFit2)"},{"path":"lta.html","id":"format-fit-table","chapter":"13 Latent Transition Analysis","heading":"13.3 Format Fit Table","text":"","code":"\nrows_m1 <- 1:6\nrows_m2 <- 7:12\n\n\nallFit %>% \n  mutate(Title = str_remove(Title, \"_Time*\")) %>% \n  gt() %>%\n  tab_header(\n    title = md(\"**Model Fit Summary Table**\")) %>% \n  cols_label(\n    Title = \"Classes\",\n    Parameters = md(\"Par\"),\n    LL = md(\"*LL*\"),\n    T11_VLMR_PValue = \"VLMR\",\n    BLRT_PValue = \"BLRT\",\n    BF = md(\"BF\"),\n    cmPk = md(\"*cmP_k*\")) %>%\n  tab_footnote(\n    footnote = md(\n    \"*Note.* Par = Parameters; *LL* = model log likelihood;\n      BIC = Bayesian information criterion;\n      aBIC = sample size adjusted BIC; CAIC = consistent Akaike information criterion;\n      AWE = approximate weight of evidence criterion;\n      BLRT = bootstrapped likelihood ratio test p-value;\n      VLMR = Vuong-Lo-Mendell-Rubin adjusted likelihood ratio test p-value;\n      cmPk = approximate correct model probability.\"), \n    locations = cells_title()) %>% \n  tab_options(column_labels.font.weight = \"bold\") %>% \n  fmt_number(10,decimals = 2,\n             drop_trailing_zeros=TRUE,\n             suffixing = TRUE) %>% \n  fmt_number(c(3:9,11), \n             decimals = 2) %>% \n  fmt_missing(1:11,\n              missing_text = \"--\") %>% \n  fmt(c(8:9,11),\n    fns = function(x) \n    ifelse(x<0.001, \"<.001\",\n           scales::number(x, accuracy = 0.01))) %>%\n  fmt(10, fns = function(x) \n    ifelse(x>100, \">100\",\n           scales::number(x, accuracy = .1))) %>%\n  tab_row_group(\n    group = \"Time-1\",\n    rows = 1:6) %>%\n  tab_row_group(\n    group = \"Time-2\",\n    rows = 7:12) %>% \n  row_group_order(\n      groups = c(\"Time-1\",\"Time-2\")\n      ) %>% \n   tab_style(\n    style = list(\n      cell_text(weight = \"bold\")\n      ),\n    locations = list(cells_body(\n     columns = BIC,\n     row = BIC == min(BIC[rows_m1]) # Model 1\n    ),\n    cells_body(\n     columns = aBIC,\n     row = aBIC == min(aBIC[rows_m1])\n    ),\n    cells_body(\n     columns = CAIC,\n     row = CAIC == min(CAIC[rows_m1])\n    ),\n    cells_body(\n     columns = AWE,\n     row = AWE == min(AWE[rows_m1])\n    ),\n    cells_body(\n     columns = cmPk,\n     row =  cmPk == max(cmPk[rows_m1])\n     ),   \n    cells_body(\n     columns = BIC,\n     row = BIC == min(BIC[rows_m2]) # Model 2\n    ),\n    cells_body(\n     columns = aBIC,\n     row = aBIC == min(aBIC[rows_m2])\n    ),\n    cells_body(\n     columns = CAIC,\n     row = CAIC == min(CAIC[rows_m2])\n    ),\n    cells_body(\n     columns = AWE,\n     row = AWE == min(AWE[rows_m2])\n    ),\n    cells_body(\n     columns = cmPk,\n     row =  cmPk == max(cmPk[rows_m2])\n     ),  \n    cells_body(\n     columns = BF, \n     row =  BF > 10),\n    cells_body(\n     columns =  BLRT_PValue,\n     row =  ifelse(BLRT_PValue < .05 & lead(BLRT_PValue) > .05, BLRT_PValue < .05, NA)),\n    cells_body(\n     columns =  T11_VLMR_PValue,\n     row =  ifelse(T11_VLMR_PValue < .05 & lead(T11_VLMR_PValue) > .05, T11_VLMR_PValue < .05, NA))\n  )\n)"},{"path":"lta.html","id":"compare-time-1-time-2-lca-plots","chapter":"13 Latent Transition Analysis","heading":"13.4 Compare Time 1 & Time 2 LCA Plots","text":"Read models plotting (4-class models)","code":"\nmodel_t1_c4 <- output_enum_t1$c4_lca_enum_time1.out\nmodel_t2_c4 <- output_enum_t2$c4_lca_enum_time2.out"},{"path":"lta.html","id":"create-a-function-plot_lca_function-that-requires-5-arguments","chapter":"13 Latent Transition Analysis","heading":"13.4.1 Create a function plot_lca_function that requires 5 arguments:","text":"model_name: name Mplus model object (e.g., model_t1_c4)item_num: number items LCA measurement model (e.g., 5)class_num: number classes (k) LCA model (e.g., 4)item_labels: item labels x-axis (e.g., c(\"Enjoy\",\"Useful\",\"Logical\",\"Job\",\"Adult\"))plot_title: include title plot (e.g., \"Time 1 LCA Conditional Item Probability Plot\")","code":"\n\nplot_lca_function <- function(model_name,item_num,class_num,item_labels,plot_title){\n\nmplus_model <- as.data.frame(model_name$gh5$means_and_variances_data$estimated_probs$values)\nplot_t1 <- mplus_model[seq(2, 2*item_num, 2),]\n\nc_size <- as.data.frame(model_name$class_counts$modelEstimated$proportion)\ncolnames(c_size) <- paste0(\"cs\")\nc_size <- c_size %>% mutate(cs = round(cs*100, 2))\ncolnames(plot_t1) <- paste0(\"C\", 1:class_num, glue(\" ({c_size[1:class_num,]}%)\"))\n\nplot_t1 <- cbind(Var = paste0(\"U\", 1:item_num), plot_t1)\nplot_t1$Var <- factor(plot_t1$Var,\n               labels = item_labels)\nplot_t1$Var <- fct_inorder(plot_t1$Var)\npd_long_t1 <- melt(plot_t1, id.vars = \"Var\") \n\np <- pd_long_t1 %>%\n  ggplot(aes(x = as.integer(Var), y = value,\n  shape = variable, colour = variable, lty = variable)) +\n  geom_point(size = 4) + geom_line() + \n  scale_x_continuous(\"\", breaks = 1:5, labels = plot_t1$Var) + \n  scale_colour_grey() + \n  labs(title = plot_title, y = \"Probability\") +\n  theme_cowplot() +\n  theme(legend.title = element_blank(), \n        legend.position = \"top\")\n\np\nreturn(p)\n}"},{"path":"lta.html","id":"time-1-lca---conditional-item-probability-plot","chapter":"13 Latent Transition Analysis","heading":"13.4.2 Time 1 LCA - Conditional Item Probability Plot","text":"","code":"\n\nplot_lca_function(\n  model_name = model_t1_c4, \n  item_num = 5,\n  class_num = 4,\n  item_labels = c(\"Enjoy\",\"Useful\",\"Logical\",\"Job\",\"Adult\"),\n  plot_title = \"Time 1 LCA Conditional Item Probability Plot\"\n  )\nggsave(here(\"figures\", \"t1_c4_lca_plot.png\"), dpi=300, height=5, width=7, units=\"in\")"},{"path":"lta.html","id":"time-2-lca---conditional-item-probability-plot","chapter":"13 Latent Transition Analysis","heading":"13.4.3 Time 2 LCA - Conditional Item Probability Plot","text":"","code":"\nplot_lca_function(\n  model_name = model_t2_c4,\n  item_num = 5,         \n  class_num = 4,\n  item_labels = c(\"Enjoy\",\"Useful\",\"Logical\",\"Job\",\"Adult\"),\n  plot_title = \"Time 2 LCA Conditional Item Probability Plot\"\n  )\nggsave(here(\"figures\", \"t2_c4_lca_plot.png\"), dpi=300, height=5, width=7, units=\"in\")"},{"path":"lta.html","id":"estimate-latent-transition-analysis-lta-model","chapter":"13 Latent Transition Analysis","heading":"13.5 Estimate Latent Transition Analysis (LTA) Model","text":"","code":""},{"path":"lta.html","id":"estimate-invariant-lta-model","chapter":"13 Latent Transition Analysis","heading":"13.5.1 Estimate Invariant LTA Model","text":"","code":"\n\nlta_inv <- mplusObject(\n  \n  TITLE = \n    \"Invariant LTA\", \n  \n  VARIABLE = \n     \"usev = ab39m ab39t ab39u ab39w ab39x  ! 7th grade indicators\n             ga33a ga33h ga33i ga33k ga33l; ! 10th grade indicators\n      \n      categorical = ab39m-ab39x ga33a-ga33l;\n\n      classes = c1(4) c2(4);\",\n    \n  ANALYSIS = \n     \"estimator = mlr;\n      type = mixture;\n      starts = 500 100;\n      processors=10;\",\n\n  MODEL = \n     \"%overall%\n      c2 on c1;\n\n      MODEL c1: \n      %c1#1%\n      [AB39M$1-AB39X$1] (1-5);  !!! labels that are repeated will constrain parameters to equality !!!\n      %c1#2%\n      [AB39M$1-AB39X$1] (6-10);\n      %c1#3%\n      [AB39M$1-AB39X$1] (11-15);\n      %c1#4%\n      [AB39M$1-AB39X$1] (16-20);\n\n      MODEL c2:\n      %c2#1%\n      [GA33A$1-GA33L$1] (1-5);\n      %c2#2%\n      [GA33A$1-GA33L$1] (6-10);\n      %c2#3%\n      [GA33A$1-GA33L$1] (11-15);\n      %c2#4%\n      [GA33A$1-GA33L$1] (16-20);\",\n   \n  SAVEDATA = \n   \"file = LTA_Inv_CPROBS.dat;\n    save = cprob;\n    missflag = 9999;\",\n\n  OUTPUT = \"tech1 tech15 svalues;\",\n  \n  usevariables = colnames(lsay_data),\n  rdata = lsay_data)\n\nlta_inv_fit <- mplusModeler(lta_inv,\n                 dataout=here(\"lta\",\"lta_model\",\"lta.dat\"),\n                 modelout=here(\"lta\",\"lta_model\",\"4-class-invariant.inp\"),\n                 check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"lta.html","id":"estimate-non-invariant-estimated-lta-model","chapter":"13 Latent Transition Analysis","heading":"13.5.2 Estimate Non-Invariant Estimated LTA Model","text":"","code":"\n\nlta_non_inv <- mplusObject(\n  \n  TITLE = \n    \"Non-Invariant LTA\", \n  \n  VARIABLE = \n     \"usev = ab39m ab39t ab39u ab39w ab39x  ! 7th grade indicators\n             ga33a ga33h ga33i ga33k ga33l; ! 10th grade indicators\n      \n      categorical = ab39m-ab39x ga33a-ga33l;\n\n      classes = c1(4) c2(4);\",\n    \n  ANALYSIS = \n     \"estimator = mlr;\n      type = mixture;\n      starts = 500 100;\n      processors=10;\",\n\n  MODEL = \n     \"%overall%\n      c2 on c1; !!! estimate all multinomial logistic regressions !!!\n      \n      !!! The above syntax can also be written as: !!!\n               ! c2#1 on c1#1 c1#2 c1#3; !  \n               ! c2#2 on c1#1 c1#2 c1#3; !\n               ! c2#3 on c1#1 c1#2 c1#3; !\n\n      MODEL c1: !!! the following syntax will allow item thresholds to be estimated for each class (e.g. noninvariance) !!!\n      \n      %c1#1%\n      [AB39M$1-AB39X$1]; \n      %c1#2%\n      [AB39M$1-AB39X$1];\n      %c1#3%\n      [AB39M$1-AB39X$1];\n      %c1#4%\n      [AB39M$1-AB39X$1];\n\n      MODEL c2:\n      %c2#1%\n      [GA33A$1-GA33L$1];\n      %c2#2%\n      [GA33A$1-GA33L$1];\n      %c2#3%\n      [GA33A$1-GA33L$1];\n      %c2#4%\n      [GA33A$1-GA33L$1];\",\n\n  OUTPUT = \"tech1 tech15 svalues;\",\n  \n  usevariables = colnames(lsay_data),\n  rdata = lsay_data)\n\nlta_non_inv_fit <- mplusModeler(lta_non_inv,\n                     dataout=here(\"lta\",\"lta_model\",\"lta.dat\"),\n                     modelout=here(\"lta\",\"lta_model\",\"4-class-non-invariant.inp\"),\n                     check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"lta.html","id":"conduct-sattorra-bentler-adjusted-log-likelihood-ratio-difference-testing","chapter":"13 Latent Transition Analysis","heading":"13.5.3 Conduct Sattorra-Bentler adjusted Log Likelihood Ratio Difference Testing","text":"non-invariant (comparison): model parameters.non-invariant (comparison): model parameters.invariant (nested): model less parameters.invariant (nested): model less parameters.RESULT: Log Likelihood \\(\\chi^2\\) difference test comparing invariant non-invariant LTA models , \\(\\chi^2 (20) = 21.542, p = .624\\).ReferenceRead invariance model extract parameters (intercepts multinomial regression coefficients)Manual method calculate transition probabilities:Although possible extract transition probabilities directly output following code illustrates parameters used calculate transition. useful conducting advanced LTA model specifications making specific constraints within transition matrices, testing equivalence specific transition probabilities.","code":"\n\n# *0 = null or nested model & *1 = comparison  or parent model\n\nlta_models <- readModels(here(\"lta\",\"lta_model\"), quiet = TRUE)\n\n# Log Likelihood Values\nL0 <- lta_models[[\"X4.class.invariant.out\"]][[\"summaries\"]][[\"LL\"]]\nL1 <- lta_models[[\"X4.class.non.invariant.out\"]][[\"summaries\"]][[\"LL\"]] \n\n# LRT equation\nlr <- -2*(L0-L1) \n\n# Parameters\np0 <- lta_models[[\"X4.class.invariant.out\"]][[\"summaries\"]][[\"Parameters\"]] \np1 <- lta_models[[\"X4.class.non.invariant.out\"]][[\"summaries\"]][[\"Parameters\"]]\n\n# Scaling Correction Factors\nc0 <- lta_models[[\"X4.class.invariant.out\"]][[\"summaries\"]][[\"LLCorrectionFactor\"]]\nc1 <- lta_models[[\"X4.class.non.invariant.out\"]][[\"summaries\"]][[\"LLCorrectionFactor\"]]\n\n# Difference Test Scaling correction\ncd <- ((p0*c0)-(p1*c1))/(p0-p1)\n\n# Chi-square difference test(TRd)\nTRd <- (lr)/(cd)\n\n# Degrees of freedom\ndf <- abs(p0 - p1)\n\n\n# Significance test\n(p_diff <- pchisq(TRd, df, lower.tail=FALSE))\n#> [1] 0.6245173\n\nlta_inv1 <- readModels(here(\"lta\",\"lta_model\",\"4-Class-Invariant.out\" ), quiet = TRUE)\n\npar <- as_tibble(lta_inv1[[\"parameters\"]][[\"unstandardized\"]]) %>% \n  select(1:3) %>% \n  filter(grepl('ON|Means', paramHeader)) %>% \n  mutate(est = as.numeric(est))\n# Name each parameter individually to make the subsequent calculations more readable\na1 <- unlist(par[13,3]); a2 <- unlist(par[14,3]); a3 <- unlist(par[15,3]); b11 <- unlist(par[1,3]);\nb21 <- unlist(par[4,3]); b31 <- unlist(par[7,3]); b12 <- unlist(par[2,3]); b22 <- unlist(par[5,3]);\nb32 <- unlist(par[8,3]); b13 <- unlist(par[3,3]); b23 <- unlist(par[6,3]); b33 <- unlist(par[9,3])\n\n# Calculate transition probabilities from the logit parameters\nt11 <- exp(a1+b11)/(exp(a1+b11)+exp(a2+b21)+exp(a3+b31)+exp(0))\nt12 <- exp(a2+b21)/(exp(a1+b11)+exp(a2+b21)+exp(a3+b31)+exp(0))\nt13 <- exp(a3+b31)/(exp(a1+b11)+exp(a2+b21)+exp(a3+b31)+exp(0))\nt14 <- 1 - (t11 + t12 + t13)\n\nt21 <- exp(a1+b12)/(exp(a1+b12)+exp(a2+b22)+exp(a3+b32)+exp(0))\nt22 <- exp(a2+b22)/(exp(a1+b12)+exp(a2+b22)+exp(a3+b32)+exp(0))\nt23 <- exp(a3+b32)/(exp(a1+b12)+exp(a2+b22)+exp(a3+b32)+exp(0))\nt24 <- 1 - (t21 + t22 + t23)\n\nt31 <- exp(a1+b13)/(exp(a1+b13)+exp(a2+b23)+exp(a3+b33)+exp(0))\nt32 <- exp(a2+b23)/(exp(a1+b13)+exp(a2+b23)+exp(a3+b33)+exp(0))\nt33 <- exp(a3+b33)/(exp(a1+b13)+exp(a2+b23)+exp(a3+b33)+exp(0))\nt34 <- 1 - (t31 + t32 + t33)\n\nt41 <- exp(a1)/(exp(a1)+exp(a2)+exp(a3)+exp(0))\nt42 <- exp(a2)/(exp(a1)+exp(a2)+exp(a3)+exp(0))\nt43 <- exp(a3)/(exp(a1)+exp(a2)+exp(a3)+exp(0))\nt44 <- 1 - (t41 + t42 + t43)"},{"path":"lta.html","id":"create-transition-table","chapter":"13 Latent Transition Analysis","heading":"13.5.4 Create Transition Table","text":"","code":"\n\nt_matrix <- tibble(\n  \"Time1\" = c(\"C1=Anti-Science\",\"C1=Amb. w/ Elevated\",\"C1=Amb. w/ Minimal\",\"C1=Pro-Science\"),\n  \"C2=Anti-Science\" = c(t11,t21,t31,t41),\n  \"C2=Amb. w/ Elevated\" = c(t12,t22,t32,t42),\n  \"C2=Amb. w/ Minimal\" = c(t13,t23,t33,t43),\n  \"C2=Pro-Science\" = c(t14,t24,t34,t44))\n\nt_matrix %>% \n  gt(rowname_col = \"Time1\") %>%\n  tab_stubhead(label = \"7th grade\") %>% \n  tab_header(\n    title = md(\"**Student transitions from 7th grade (rows) to 10th grade (columns)**\"),\n    subtitle = md(\"&nbsp;\")) %>% \n  fmt_number(2:5,decimals = 2) %>% \n  tab_spanner(label = \"10th grade\",columns = 2:5) %>% \n  tab_footnote(\n    footnote = md(\n    \"*Note.* Transition matrix values are the identical to Table 5, however Table 5 \n    has the values rearranged by class for interpretation purposes. Classes may be arranged\n    directly through Mplus syntax using start values.\"), \n    locations = cells_title())"},{"path":"lta-with-covariates.html","id":"lta-with-covariates","chapter":"14 LTA with Covariates","heading":"14 LTA with Covariates","text":"Data Source: data used illustrate analyses include elementary school student Science Attitude survey items collected 7th 10th grades Longitudinal Study American Youth (LSAY; Miller, 2015).install package {rhdf5}Load packagesRead LSAY data file, lsay_new.csv.","code":"\nif (!requireNamespace(\"BiocManager\", quietly = TRUE)) \n  install.packages(\"BiocManager\")\n\n#BiocManager::install(\"rhdf5\")\nlibrary(MplusAutomation)\nlibrary(rhdf5)\nlibrary(tidyverse)       \nlibrary(here)            \nlibrary(glue)            \nlibrary(janitor)            \nlibrary(gt) \nlibrary(reshape2)\nlibrary(cowplot)\nlibrary(ggrepel)\nlibrary(haven)\nlibrary(modelsummary)\nlibrary(corrplot)\nlibrary(DiagrammeR)\nlibrary(filesstrings)\nlibrary(PNWColors)\n\nlsay_data <- read_csv(here(\"data\",\"lsay_lta.csv\"), na = c(\"9999\")) %>% \n    mutate(across(everything(), as.numeric))"},{"path":"lta-with-covariates.html","id":"descriptive-statistics-11","chapter":"14 LTA with Covariates","heading":"14.1 Descriptive Statistics","text":"","code":""},{"path":"lta-with-covariates.html","id":"data-summary-1","chapter":"14 LTA with Covariates","heading":"14.1.1 Data Summary","text":"","code":"\ndata <- lsay_data\n\nselect_data <- data %>% \n select(female, minority, ab39m:ga33l)\n\nf <- All(select_data) ~ Mean + SD + Min + Median + Max + Histogram\ndatasummary(f, data, output=\"markdown\")"},{"path":"lta-with-covariates.html","id":"adding-covariates","chapter":"14 LTA with Covariates","heading":"14.2 Adding Covariates","text":"Continuing previous section 13, use ML three-step method estimate LTA models predictors distal outcomes). Estimate unconditional model latent variable predictors included auxiliary option least one models.Covariatessci_issues7: Interest science issues (1 = interested, 2 = Moderately Interested, 3 = interested)sci_irt7: 7th Grade Science IRT Score (Continuous)female: Gender (0 = Male, 1 = Female)","code":""},{"path":"lta-with-covariates.html","id":"step-1---estimate-unconditional-model-w-auxiliary-specification-1","chapter":"14 LTA with Covariates","heading":"14.2.1 Step 1 - Estimate Unconditional Model w/ Auxiliary Specification","text":"7th Grade10th Gradeplot_lca_function requires 5 arguments:model_name: name Mplus model object (e.g., model_t1_c4)item_num: number items LCA measurement model (e.g., 5)class_num: number classes (k) LCA model (e.g., 4)item_labels: item labels x-axis (e.g., c(\"Enjoy\",\"Useful\",\"Logical\",\"Job\",\"Adult\"))plot_title: include title plot (e.g., \"Time 1 LCA Conditional Item Probability Plot\")Plot Time 1Plot Time 2","code":"\n\nstep1  <- mplusObject(\n  TITLE = \"Step 1 - T1\", \n  VARIABLE = \n  \"usevar = ab39m ab39t ab39u ab39w ab39x;\n  categorical = ab39m ab39t ab39u ab39w ab39x;\n    \n   classes = c(4); \n    \n   auxiliary = sci_issues7 sci_irt7 female;\n  \n   idvariable = casenum;\",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 0;\n    optseed = 534483;\",\n  \n  SAVEDATA = \n   \"File=3step_t1.dat;\n    Save=cprob;\",\n  \n  OUTPUT = \"residual tech11 tech14 svalues\",\n  \n  PLOT = \n    \"type = plot3; \n    series = ab39m-ab39x(*);\",\n  \n  usevariables = colnames(lsay_data),\n  rdata = lsay_data)\n\nstep1_fit <- mplusModeler(step1,\n                            dataout=here(\"lta\",\"cov_model\",\"t1.dat\"),\n                            modelout=here(\"lta\",\"cov_model\",\"one_T1.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\n\nstep1  <- mplusObject(\n  TITLE = \"Step 1 - T1\", \n  VARIABLE = \n  \"usevar =  ga33a ga33h ga33i ga33k ga33l;\n  categorical =  ga33a ga33h ga33i ga33k ga33l;\n    \n   classes = c(4); \n    \n   !auxiliary = sci_issues7 sci_irt7 female;\n  \n   idvariable = casenum;\",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 0;\n    optseed = 392418;\",\n  \n  SAVEDATA = \n   \"File=3step_t2.dat;\n    Save=cprob;\",\n  \n  OUTPUT = \"residual tech11 tech14 svalues\",\n\n  PLOT = \n    \"type = plot3; \n    series = ga33a-ga33l(*);\",\n  \n  usevariables = colnames(lsay_data),\n  rdata = lsay_data)\n\nstep1_fit <- mplusModeler(step1,\n                            dataout=here(\"lta\",\"cov_model\",\"t2.dat\"),\n                            modelout=here(\"lta\",\"cov_model\",\"one_T2.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\n\nplot_lca_function <- function(model_name,item_num,class_num,item_labels,plot_title){\n\nmplus_model <- as.data.frame(model_name$gh5$means_and_variances_data$estimated_probs$values)\nplot_t1 <- mplus_model[seq(2, 2*item_num, 2),]\n\nc_size <- as.data.frame(model_name$class_counts$modelEstimated$proportion)\ncolnames(c_size) <- paste0(\"cs\")\nc_size <- c_size %>% mutate(cs = round(cs*100, 2))\ncolnames(plot_t1) <- paste0(\"C\", 1:class_num, glue(\" ({c_size[1:class_num,]}%)\"))\n\nplot_t1 <- cbind(Var = paste0(\"U\", 1:item_num), plot_t1)\nplot_t1$Var <- factor(plot_t1$Var,\n               labels = item_labels)\nplot_t1$Var <- fct_inorder(plot_t1$Var)\npd_long_t1 <- melt(plot_t1, id.vars = \"Var\") \n\np <- pd_long_t1 %>%\n  ggplot(aes(x = as.integer(Var), y = value,\n  shape = variable, colour = variable, lty = variable)) +\n  geom_point(size = 4) + geom_line() + \n  scale_x_continuous(\"\", breaks = 1:5, labels = plot_t1$Var) + \n  scale_colour_grey() + \n  labs(title = plot_title, y = \"Probability\") +\n  theme_cowplot() +\n  theme(legend.title = element_blank(), \n        legend.position = \"top\")\n\np\nreturn(p)\n}\n\noutput_T1 <- readModels(here(\"lta\",\"cov_model\",\"one_T1.out\"))\n\nplot_lca_function(\n  model_name = output_T1, \n  item_num = 5,\n  class_num = 4,\n  item_labels = c(\"Enjoy\",\"Useful\",\"Logical\",\"Job\",\"Adult\"),\n  plot_title = \"Time 1 LCA Conditional Item Probability Plot\"\n  )\n\noutput_T2 <- readModels(here(\"lta\",\"cov_model\",\"one_T2.out\"))\n\nplot_lca_function(\n  model_name = output_T2, \n  item_num = 5,\n  class_num = 4,\n  item_labels = c(\"Enjoy\",\"Useful\",\"Logical\",\"Job\",\"Adult\"),\n  plot_title = \"Time 2 LCA Conditional Item Probability Plot\"\n  )"},{"path":"lta-with-covariates.html","id":"step-2---determine-measurement-error-4","chapter":"14 LTA with Covariates","heading":"14.2.2 Step 2 - Determine Measurement Error","text":"Extract logits classification probabilities likely latent class:Extract saved dataset:Rename column savedata named “C” change “N”","code":"\n\nlogit_cprobs_T1 <- as.data.frame(output_T1[[\"class_counts\"]]\n                                       [[\"logitProbs.mostLikely\"]])\n\nlogit_cprobs_T2 <- as.data.frame(output_T2[[\"class_counts\"]]\n                                       [[\"logitProbs.mostLikely\"]])\n\nsavedata_T1 <- as.data.frame(output_T1[[\"savedata\"]])\nsavedata_T2 <- as.data.frame(output_T2[[\"savedata\"]])\n\ncolnames(savedata_T1)[colnames(savedata_T1)==\"C\"] <- \"N_T1\"\ncolnames(savedata_T2)[colnames(savedata_T2)==\"C\"] <- \"N_T2\"\n\nsavedata <- savedata_T1 %>% \n  full_join(savedata_T2, by = \"CASENUM\")"},{"path":"lta-with-covariates.html","id":"step-3---add-auxiliary-variables-1","chapter":"14 LTA with Covariates","heading":"14.2.3 Step 3 - Add Auxiliary Variables","text":"","code":"\nstep3  <- mplusObject(\n  TITLE = \"ML Three Step LTA Model\", \n  \n  VARIABLE = \n \"nominal=N_T1 N_T2;\n  usevar = N_T1 N_T2 SCI_IRT7 FEMALE;\n  classes = c1(4) c2(4);\" ,\n  \n  ANALYSIS = \n \"estimator = mlr; \n  type = mixture; \n  starts = 0;\",\n \n  MODEL =\n  glue(\n \" %OVERALL%\n      \n   c2 on c1; \n   c1 c2 on  SCI_IRT7 FEMALE;\n\n  MODEL c1:\n  \n  %c1#1%\n  [N_T1#1@{logit_cprobs_T1[1,1]}];\n  [N_T1#2@{logit_cprobs_T1[1,2]}];\n  [N_T1#3@{logit_cprobs_T1[1,3]}];\n\n  %c1#2%\n  [N_T1#1@{logit_cprobs_T1[2,1]}];\n  [N_T1#2@{logit_cprobs_T1[2,2]}];\n  [N_T1#3@{logit_cprobs_T1[2,3]}];\n  \n  %c1#3%\n  [N_T1#1@{logit_cprobs_T1[3,1]}]; \n  [N_T1#2@{logit_cprobs_T1[3,2]}];\n  [N_T1#3@{logit_cprobs_T1[3,3]}];\n  \n  %c1#4%\n  [N_T1#1@{logit_cprobs_T1[4,1]}]; \n  [N_T1#2@{logit_cprobs_T1[4,2]}];\n  [N_T1#3@{logit_cprobs_T1[4,3]}];\n\n \n  MODEL c2:\n  \n  %c2#1%\n  [N_T2#1@{logit_cprobs_T2[1,1]}]; \n  [N_T2#2@{logit_cprobs_T2[1,2]}];\n  [N_T2#3@{logit_cprobs_T2[1,3]}];\n\n  %c2#2%\n  [N_T2#1@{logit_cprobs_T2[2,1]}];\n  [N_T2#2@{logit_cprobs_T2[2,2]}];\n  [N_T2#3@{logit_cprobs_T2[2,3]}];\n\n  \n  %c2#3%\n  [N_T2#1@{logit_cprobs_T2[3,1]}];\n  [N_T2#2@{logit_cprobs_T2[3,2]}];\n  [N_T2#3@{logit_cprobs_T2[3,3]}];\n \n   %c2#4%\n  [N_T2#1@{logit_cprobs_T2[4,1]}];\n  [N_T2#2@{logit_cprobs_T2[4,2]}];\n  [N_T2#3@{logit_cprobs_T2[4,3]}];\"),\n \n \n  OUTPUT = \"tech15;\",\n  \n  usevariables = colnames(savedata), \n  rdata = savedata)\n\nstep3_fit <- mplusModeler(step3,\n               dataout=here(\"lta\",\"cov_model\",\"three.dat\"), \n               modelout=here(\"lta\",\"cov_model\",\"three.inp\"), \n               check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"lta-with-covariates.html","id":"lta-transition-plot","chapter":"14 LTA with Covariates","heading":"14.2.3.1 LTA Transition Plot","text":"code adapted source code plotLTA function found NOTE: function found plot_transitions_function.R specific model 2 time-points 4-classes & must updated accommodate models.Table:","code":"\nsource(here(\"functions\",\"plot_transitions_function.R\"))\n\nlta_model <- readModels(here(\"lta\",\"cov_model\",\"three.out\"))\n\nplot_transitions_function(\n  model_name = lta_model,\n  color_pallete = pnw_palette(\"Bay\", n=4, type = \"discrete\"),\n  facet_labels =c(\n    `1` = \"Transitions to 10th Grade from the Pro-Science w/ Elevated Utility Class\",\n    `2` = \"Transitions to 10th Grade from the Ambivalent w/ Elevated Utility Class\",\n    `3` = \"Transitions to 10th Grade from the Ambivalent w/ Minimal Utility Class\",\n    `4` = \"Transitions to 10th Grade from the Anti-Science w/ Minimal Utility Class\"),\n  timepoint_labels = c('1' = \"7th Grade\", '2' = \"10th Grade\"),\n  class_labels = c(\n    \"Pro-Science\",\n    \"Amb. / Elev. Utility\",\n    \"Amb. / Min. Utility\",\n    \"Anti-Science\")\n  )\nlta_prob <- as.data.frame(lta_model$class_counts$transitionProbs$probability)\n\n\nt_matrix <- tibble(\n  \"7th Grade\" = c(\"Pro-Science\",\"Amb. / Elev. Utility\",\"Amb. / Min. Utility\",\"Anti-Science\"),\n  \"Pro-Science\" = c(lta_prob[1,1],lta_prob[2,1],lta_prob[3,1],lta_prob[4,1]),\n  \"Amb. / Elev. Utility\" = c(lta_prob[5,1],lta_prob[6,1],lta_prob[7,1],lta_prob[8,1]),\n  \"Amb. / Min. Utility\" = c(lta_prob[9,1],lta_prob[10,1],lta_prob[11,1],lta_prob[12,1]),\n  \"Anti-Science\" = c(lta_prob[13,1],lta_prob[14,1],lta_prob[15,1],lta_prob[16,1]))\n\nt_matrix %>% \n  gt(rowname_col = \"7th Grade\") %>%\n  tab_stubhead(label = \"7th Grade\") %>% \n  tab_header(\n    title = md(\"**Transition Probabilities**\")) %>% \n  fmt_number(2:5,decimals = 2) %>% \n  tab_spanner(label = \"10th Grade\",columns = 2:5)#%>% \n  #gtsave(\"matrix.docx\")"},{"path":"lta-with-covariates.html","id":"covariate-table","chapter":"14 LTA with Covariates","heading":"14.2.3.2 Covariate Table","text":"","code":"\n# REFERENCE CLASS 4\ncov <- as.data.frame(lta_model[[\"parameters\"]][[\"unstandardized\"]]) %>%\n  filter(param %in% c(\"SCI_IRT7\", \"FEMALE\")) %>% \n  mutate(param = case_when(\n            param == \"SCI_IRT7\" ~ \"Science IRT Score\",\n            param == \"FEMALE\" ~ \"Gender\"),\n    se = paste0(\"(\", format(round(se,2), nsmall =2), \")\")) %>% \n  separate(paramHeader, into = c(\"Time\", \"Class\"), sep = \"#\") %>% \n  mutate(Class = case_when(\n            Class == \"1.ON\" ~ \"Pro-Science\",\n            Class == \"2.ON\" ~ \"Amb. / Elev. Utility\",\n            Class == \"3.ON\" ~ \"Amb. / Min. Utility\"),\n         Time = case_when(\n            Time == \"C1\" ~ \"7th Grade (T1)\",\n            Time == \"C2\" ~ \"10th Grade (T2)\",\n         )\n         ) %>% \n  unite(estimate, est, se, sep = \" \") %>% \n  select(Time:pval, -est_se) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001))))\n\n\n# Create table\n\ncov_m1 <- cov %>% \n  group_by(param, Class) %>% \n  gt() %>% \n  tab_header(\n    title = \"Relations Between the Covariates and Latent Class\") %>%\n  tab_footnote(\n    footnote = md(\n      \"Reference Group: Anti-Science\"\n    ),\nlocations = cells_title()\n  ) %>% \n  cols_label(\n    param = md(\"Covariate\"),\n    estimate = md(\"Estimate (*se*)\"),\n    pval = md(\"*p*-value\")) %>% \n  sub_missing(1:3,\n              missing_text = \"\") %>%\n  sub_values(values = c(999.000), replacement = \"-\") %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\") \n\ncov_m1"},{"path":"lta-with-covariates.html","id":"manually-calculate-transition-probabilities-by-covariate","chapter":"14 LTA with Covariates","heading":"14.2.3.3 Manually calculate transition probabilities by covariate","text":"Read invariance model extract parameters (intercepts multinomial regression coefficients)Manual method calculate transition probabilities covariate:Create table","code":"\nstep3  <- mplusObject(\n  TITLE = \"LTA (invariant)\", \n  \n  VARIABLE = \n \"usevar = ab39m ab39t ab39u ab39w ab39x  ! 7th grade indicators\n               ga33a ga33h ga33i ga33k ga33l FEMALE;\n  categorical = ab39m-ab39x ga33a-ga33l;\n  classes = c1(4) c2(4);\" ,\n  \n  ANALYSIS = \n \"estimator = mlr; \n  type = mixture; \n  starts = 500 100;\n processors = 10;\",\n \n  MODEL =\n  \"%overall%\n\n    c2 c1 on FEMALE;\n\n      c2#1 on c1#1 (b11);\n      c2#2 on c1#1 (b21);\n      c2#3 on c1#1 (b31);\n    \n      c2#1 on c1#2 (b12);\n      c2#2 on c1#2 (b22);\n      c2#3 on c1#2 (b32);    \n\n      c2#1 on c1#3 (b13);\n      c2#2 on c1#3 (b23);\n      c2#3 on c1#3 (b33);\n\n      [c2#1] (a1);\n      [c2#2] (a2);\n      [c2#3] (a3);\n\n     c2#1 ON female (b212);\n     c2#2 ON female (b222);\n     c2#3 ON female (b232);\n     c1#1 ON female (b112);\n     c1#2 ON female (b122);\n     c1#3 ON female (b132);\n\n      MODEL c1: \n      %c1#1%\n      [AB39M$1-AB39X$1] (1-5);  !!! labels that are repeated will constrain parameters to equality !!!\n      %c1#2%\n      [AB39M$1-AB39X$1] (6-10);\n      %c1#3%\n      [AB39M$1-AB39X$1] (11-15);\n      %c1#4%\n      [AB39M$1-AB39X$1] (16-20);\n\n      MODEL c2:\n      %c2#1%\n      [GA33A$1-GA33L$1] (1-5);\n      %c2#2%\n      [GA33A$1-GA33L$1] (6-10);\n      %c2#3%\n      [GA33A$1-GA33L$1] (11-15);\n      %c2#4%\n      [GA33A$1-GA33L$1] (16-20);\",\n \n \n  OUTPUT = \"tech1 tech15 svalues;\",\n \n  MODELCONSTRAINT = \"  ! Compute joint and marginal probabilities:\n        New(\n        t11 t12 t13 t14\n        t21 t22 t23 t24\n        t31 t32 t33 t34\n        t41 t42 t43 t44\n\n        t11B t12B t13B t14B\n        t21B t22B t23B t24B\n        t31B t32B t33B t34B\n        t41B t42B t43B t44B\n        \n        diff_11_22 x\n        \n       );\n\n        t11 = exp(a1 +b11)/(exp(a1+b11)+exp(a2+b21)+exp(a3+b31)+exp(0));\n        t12 = exp(a2 +b21)/(exp(a1+b11)+exp(a2+b21)+exp(a3+b31)+exp(0));\n        t13 = exp(a3 +b31)/(exp(a1+b11)+exp(a2+b21)+exp(a3+b31)+exp(0));\n        t14 = 1 - (t11+t12+t13);\n\n        t21 = exp(a1 +b12)/(exp(a1+b12)+exp(a2+b22)+exp(a3+b32)+exp(0));\n        t22 = exp(a2 +b22)/(exp(a1+b12)+exp(a2+b22)+exp(a3+b32)+exp(0));\n        t23 = exp(a3 +b32)/(exp(a1+b12)+exp(a2+b22)+exp(a3+b32)+exp(0));\n        t24 = 1 - (t21+t22+t23);\n\n        t31 = exp(a1 +b13)/(exp(a1+b13)+exp(a2+b23)+exp(a3+b33)+exp(0));\n        t32 = exp(a2 +b23)/(exp(a1+b13)+exp(a2+b23)+exp(a3+b33)+exp(0));\n        t33 = exp(a3 +b33)/(exp(a1+b13)+exp(a2+b23)+exp(a3+b33)+exp(0));\n        t34 = 1 - (t31+t32+t33);\n\n        t41 = exp(a1)/(exp(a1)+exp(a2)+exp(a3)+exp(0));\n        t42 = exp(a2)/(exp(a1)+exp(a2)+exp(a3)+exp(0));\n        t43 = exp(a3)/(exp(a1)+exp(a2)+exp(a3)+exp(0));\n        t44 = 1 - (t41+t42+t43);\n        \n        \n     !c1#3 ON female (b132);\n\n x= 1  ; ! x=1 is female, x=0 males\n\nt11B = exp(a1 +b11+b212*x)/(exp(a1+b11+b212*x)+exp(a2+b21+b222*x)+exp(a3+b31+b232*x)\n+exp(0));\nt12B = exp(a2 +b21+b222*x)/(exp(a1+b11+b212*x)+exp(a2+b21+b222*x)+exp(a3+b31+b232*x)\n+exp(0));\nt13B = exp(a3 +b31+b232*x)/(exp(a1+b11+b212*x)+exp(a2+b21+b222*x)+exp(a3+b31+b232*x)\n+exp(0));\nt14B = 1 - (t11B+t12B+t13B);\n\nt21B = exp(a1 +b12+b212*x)/(exp(a1+b12+b212*x)+exp(a2+b22+b222*x)+exp(a3+b32+b232*x)\n+exp(0));\nt22B = exp(a2 +b22+b222*x)/(exp(a1+b12+b212*x)+exp(a2+b22+b222*x)+exp(a3+b32+b232*x)\n+exp(0));\nt23B = exp(a3 +b32+b232*x)/(exp(a1+b12+b212*x)+exp(a2+b22+b222*x)+exp(a3+b32+b232*x)\n+exp(0));\nt24B = 1 - (t21B+t22B+t23B);\n\nt31B = exp(a1 +b13+b212*x)/(exp(a1+b13+b212*x)+exp(a2+b23+b222*x)+exp(a3+b33+b232*x)\n+exp(0));\nt32B = exp(a2 +b23+b222*x)/(exp(a1+b13+b212*x)+exp(a2+b23+b222*x)+exp(a3+b33+b232*x)\n+exp(0));\nt33B = exp(a3 +b33+b232*x)/(exp(a1+b13+b212*x)+exp(a2+b23+b222*x)+exp(a3+b33+b232*x)\n+exp(0));\nt34B = 1 - (t31B+t32B+t33B);\n\nt41B = exp(a1+b212*x)/(exp(a1+b212*x)+exp(a2+b222*x)+exp(a3+b232*x)+exp(0));\nt42B = exp(a2+b222*x)/(exp(a1+b212*x)+exp(a2+b222*x)+exp(a3+b232*x)+exp(0));\nt43B = exp(a3+b232*x)/(exp(a1+b212*x)+exp(a2+b222*x)+exp(a3+b232*x)+exp(0));\nt44B = 1 - (t41B+t42B+t43B);\n\n        diff_11_22= t11-t11B;\",\n  \n  usevariables = colnames(savedata), \n  rdata = savedata)\n\nstep3_fit <- mplusModeler(step3,\n               dataout=here(\"lta\",\"cov_model\",\"calc_tran.dat\"), \n               modelout=here(\"lta\",\"cov_model\",\"calc_tran.inp\"), \n               check=TRUE, run = TRUE, hashfilename = FALSE)\n\nlta_inv1 <- readModels(here(\"lta\",\"cov_model\",\"calc_tran.out\" ), quiet = TRUE)\n\npar <- as_tibble(lta_inv1[[\"parameters\"]][[\"unstandardized\"]]) %>% \n  select(1:3) %>% \n  filter(grepl('ON|Means|Intercept', paramHeader)) %>% \n  mutate(est = as.numeric(est),\n         label = c(\"b11\", \"b12\", \"b13\", \"b21\", \"b22\", \"b23\", \"b31\", \"b32\", \"b33\", \"b212\", \"b222\", \"b232\", \"b112\", \"b122\", \"b132\", \"a11\", \"a21\", \"a31\", \"a12\", \"a22\", \"a32\"))\n# Name each parameter individually to make the subsequent calculations more readable\na1 <- unlist(par[19,3]); \na2 <- unlist(par[20,3]); \na3 <- unlist(par[21,3]);\n\nb11 <- unlist(par[1,3]);\nb21 <- unlist(par[4,3]); \nb31 <- unlist(par[7,3]); \n\nb12 <- unlist(par[2,3]); \nb22 <- unlist(par[5,3]);\nb32 <- unlist(par[8,3]); \n\nb13 <- unlist(par[3,3]); \nb23 <- unlist(par[6,3]); \nb33 <- unlist(par[9,3]);\n\nb212 <- unlist(par[10,3]);\nb222 <- unlist(par[11,3]);\nb232 <- unlist(par[12,3]);\n\nb112 <- unlist(par[13,3]);\nb122 <- unlist(par[14,3]);\nb132 <- unlist(par[15,3]);\n\nx <- 0 # x=1 is female, x=0 males\n\n# Calculate transition probabilities from the logit parameters\nt11B <- exp(a1 + b11 + b212*x) / (exp(a1 + b11 + b212*x) + exp(a2 + b21 + b222*x) + exp(a3 + b31 + b232*x) + exp(0))\nt12B <- exp(a2 + b21 + b222*x) / (exp(a1 + b11 + b212*x) + exp(a2 + b21 + b222*x) + exp(a3 + b31 + b232*x) + exp(0))\nt13B <- exp(a3 + b31 + b232*x) / (exp(a1 + b11 + b212*x) + exp(a2 + b21 + b222*x) + exp(a3 + b31 + b232*x) + exp(0))\nt14B <- 1 - (t11B + t12B + t13B)\n\nt21B <- exp(a1 + b12 + b212*x) / (exp(a1 + b12 + b212*x) + exp(a2 + b22 + b222*x) + exp(a3 + b32 + b232*x) + exp(0))\nt22B <- exp(a2 + b22 + b222*x) / (exp(a1 + b12 + b212*x) + exp(a2 + b22 + b222*x) + exp(a3 + b32 + b232*x) + exp(0))\nt23B <- exp(a3 + b32 + b232*x) / (exp(a1 + b12 + b212*x) + exp(a2 + b22 + b222*x) + exp(a3 + b32 + b232*x) + exp(0))\nt24B <- 1 - (t21B + t22B + t23B)\n\nt31B <- exp(a1 + b13 + b212*x) / (exp(a1 + b13 + b212*x) + exp(a2 + b23 + b222*x) + exp(a3 + b33 + b232*x) + exp(0))\nt32B <- exp(a2 + b23 + b222*x) / (exp(a1 + b13 + b212*x) + exp(a2 + b23 + b222*x) + exp(a3 + b33 + b232*x) + exp(0))\nt33B <- exp(a3 + b33 + b232*x) / (exp(a1 + b13 + b212*x) + exp(a2 + b23 + b222*x) + exp(a3 + b33 + b232*x) + exp(0))\nt34B <- 1 - (t31B + t32B + t33B)\n\nt41B <- exp(a1 + b212*x) / (exp(a1 + b212*x) + exp(a2 + b222*x) + exp(a3 + b232*x) + exp(0))\nt42B <- exp(a2 + b222*x) / (exp(a1 + b212*x) + exp(a2 + b222*x) + exp(a3 + b232*x) + exp(0))\nt43B <- exp(a3 + b232*x) / (exp(a1 + b212*x) + exp(a2 + b222*x) + exp(a3 + b232*x) + exp(0))\nt44B <- 1 - (t41B + t42B + t43B)\n\nx <- 1 # x=1 is female, x=0 males\n\n# Calculate transition probabilities from the logit parameters\nt11 <- exp(a1 + b11 + b212*x) / (exp(a1 + b11 + b212*x) + exp(a2 + b21 + b222*x) + exp(a3 + b31 + b232*x) + exp(0))\nt12 <- exp(a2 + b21 + b222*x) / (exp(a1 + b11 + b212*x) + exp(a2 + b21 + b222*x) + exp(a3 + b31 + b232*x) + exp(0))\nt13 <- exp(a3 + b31 + b232*x) / (exp(a1 + b11 + b212*x) + exp(a2 + b21 + b222*x) + exp(a3 + b31 + b232*x) + exp(0))\nt14 <- 1 - (t11 + t12 + t13)\n\nt21 <- exp(a1 + b12 + b212*x) / (exp(a1 + b12 + b212*x) + exp(a2 + b22 + b222*x) + exp(a3 + b32 + b232*x) + exp(0))\nt22 <- exp(a2 + b22 + b222*x) / (exp(a1 + b12 + b212*x) + exp(a2 + b22 + b222*x) + exp(a3 + b32 + b232*x) + exp(0))\nt23 <- exp(a3 + b32 + b232*x) / (exp(a1 + b12 + b212*x) + exp(a2 + b22 + b222*x) + exp(a3 + b32 + b232*x) + exp(0))\nt24 <- 1 - (t21 + t22 + t23)\n\nt31 <- exp(a1 + b13 + b212*x) / (exp(a1 + b13 + b212*x) + exp(a2 + b23 + b222*x) + exp(a3 + b33 + b232*x) + exp(0))\nt32 <- exp(a2 + b23 + b222*x) / (exp(a1 + b13 + b212*x) + exp(a2 + b23 + b222*x) + exp(a3 + b33 + b232*x) + exp(0))\nt33 <- exp(a3 + b33 + b232*x) / (exp(a1 + b13 + b212*x) + exp(a2 + b23 + b222*x) + exp(a3 + b33 + b232*x) + exp(0))\nt34 <- 1 - (t31 + t32 + t33)\n\nt41 <- exp(a1 + b212*x) / (exp(a1 + b212*x) + exp(a2 + b222*x) + exp(a3 + b232*x) + exp(0))\nt42 <- exp(a2 + b222*x) / (exp(a1 + b212*x) + exp(a2 + b222*x) + exp(a3 + b232*x) + exp(0))\nt43 <- exp(a3 + b232*x) / (exp(a1 + b212*x) + exp(a2 + b222*x) + exp(a3 + b232*x) + exp(0))\nt44 <- 1 - (t41 + t42 + t43)"},{"path":"lta-with-covariates.html","id":"create-transition-table-1","chapter":"14 LTA with Covariates","heading":"14.2.4 Create Transition Table","text":"","code":"\n\nt_matrix <- tibble(\n  \"Time1\" = c(\"C1=Anti-Science\",\"C1=Amb. w/ Elevated\",\"C1=Amb. w/ Minimal\",\"C1=Pro-Science\"),\n  \"C2=Anti-Science\" = c(t11,t21,t31,t41),\n  \"C2=Amb. w/ Elevated\" = c(t12,t22,t32,t42),\n  \"C2=Amb. w/ Minimal\" = c(t13,t23,t33,t43),\n  \"C2=Pro-Science\" = c(t14,t24,t34,t44))\n\nt_matrix %>% \n  gt(rowname_col = \"Time1\") %>%\n  tab_stubhead(label = \"7th grade\") %>% \n  tab_header(\n    title = md(\"**FEMALES: Student transitions from 7th grade (rows) to 10th grade (columns)**\")) %>% \n  fmt_number(2:5,decimals = 3) %>% \n  tab_spanner(label = \"10th grade\",columns = 2:5) %>% \n  tab_footnote(\n    footnote = md(\n    \"*Note.* Transition matrix values are the identical to Table 5, however Table 5 \n    has the values rearranged by class for interpretation purposes. Classes may be arranged\n    directly through Mplus syntax using start values.\"), \n    locations = cells_title())\n\n\n\nt_matrix <- tibble(\n  \"Time1\" = c(\"C1=Anti-Science\",\"C1=Amb. w/ Elevated\",\"C1=Amb. w/ Minimal\",\"C1=Pro-Science\"),\n  \"C2=Anti-Science\" = c(t11B,t21B,t31B,t41B),\n  \"C2=Amb. w/ Elevated\" = c(t12B,t22B,t32B,t42B),\n  \"C2=Amb. w/ Minimal\" = c(t13B,t23B,t33B,t43B),\n  \"C2=Pro-Science\" = c(t14B,t24B,t34B,t44B))\n\nt_matrix %>% \n  gt(rowname_col = \"Time1\") %>%\n  tab_stubhead(label = \"7th grade\") %>% \n  tab_header(\n    title = md(\"**MALES: Student transitions from 7th grade (rows) to 10th grade (columns)**\")) %>% \n  fmt_number(2:5,decimals = 3) %>% \n  tab_spanner(label = \"10th grade\",columns = 2:5) %>% \n  tab_footnote(\n    footnote = md(\n    \"*Note.* Transition matrix values are the identical to Table 5, however Table 5 \n    has the values rearranged by class for interpretation purposes. Classes may be arranged\n    directly through Mplus syntax using start values.\"), \n    locations = cells_title())"},{"path":"joint-occurrence.html","id":"joint-occurrence","chapter":"15 Joint Occurrence","heading":"15 Joint Occurrence","text":"Example: Longitudinal Study American YouthData source: : See documentation ","code":""},{"path":"joint-occurrence.html","id":"load-packages-5","chapter":"15 Joint Occurrence","heading":"15.1 Load Packages","text":"","code":"\nlibrary(MplusAutomation)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(glue)\nlibrary(gt)\nlibrary(cowplot)\nlibrary(kableExtra)\nlibrary(psych)\nlibrary(float)\nlibrary(janitor)\nlibrary(ggalluvial)\nlibrary(DiagrammeR)\nlibrary(modelsummary)\nlibrary(corrplot)\nlibrary(ggrepel)"},{"path":"joint-occurrence.html","id":"path-diagram","chapter":"15 Joint Occurrence","heading":"15.2 Path Diagram","text":"Read LSAY dataset","code":"\ngrViz(\"\ndigraph model {\n    graph [layout=dot, overlap=true]\n\n    node [shape=box]\n    math_enjoy    [label=\\\"Math: Enjoy\\\"]    \n    math_useful    [label=\\\"Math: Useful\\\"]\n    math_logical   [label=\\\"Math: Logical\\\"]\n    math_job       [label=\\\"Math: Job\\\"]\n    math_adult     [label=\\\"Math: Adult\\\"]\n    science_enjoy [label=\\\"Science: Enjoy\\\"]\n    science_useful [label=\\\"Science: Useful\\\"]\n    science_logical[label=\\\"Science: Logical\\\"]\n    science_job    [label=\\\"Science: Job\\\"]\n    science_adult  [label=\\\"Science: Adult\\\"]\n\n    node [shape=circle]\n    C_math [label=<C<SUB>Math<\/SUB>>];\n    C_sci [label=<C<SUB>Science<\/SUB>>];\n\n    edge []\n    C_math -> {math_enjoy math_useful math_logical math_job math_adult}\n    C_sci -> {science_enjoy science_useful science_logical science_job science_adult}\n    C_math -> C_sci\n\n    {rank = same; C_math; C_sci;}\n}\n\")\ndata <- read_csv(here(\"data\", \"lsay_joint_occurrence.csv\")) %>% \n  rename(\n    math_enjoy   = KA46A, # Renaming the variables\n    math_useful  = KA46H,\n    math_logical = KA46I,\n    math_job     = KA46K,\n    math_adult   = KA46L,\n    sci_enjoy   = KA47A,\n    sci_useful  = KA47H,\n    sci_logical = KA47I,\n    sci_job     = KA47K,\n    sci_adult   = KA47L\n  ) %>% \n  clean_names() %>% # Making variables lower-case\n  mutate(across(\n    .cols = math_enjoy:sci_adult, # Dichtomizing the variables\n    .fns = ~ case_when(\n      . %in% c(1, 2) ~ 1,\n      . %in% c(3, 4, 5) ~ 0,\n      TRUE ~ NA_real_\n    )\n  ))"},{"path":"joint-occurrence.html","id":"descriptive-statistics-12","chapter":"15 Joint Occurrence","heading":"15.3 Descriptive Statistics","text":"","code":""},{"path":"joint-occurrence.html","id":"descriptive-statistics-using-r-1","chapter":"15 Joint Occurrence","heading":"15.3.1 Descriptive Statistics using R:","text":"Quick view relevant variables:Proportion indicators using R:Data summary:Correlation table:Correlation plot:","code":"\ndata %>%\n  select(\n    math_enjoy, math_useful, math_logical, math_job, math_adult,\n    sci_enjoy, sci_useful, sci_logical, sci_job, sci_adult\n  ) %>%\n  psych::describe()\n# Set up data to find proportions of binary indicators\nds <- data %>% \n  pivot_longer(c(math_enjoy:sci_adult), names_to = \"Variable\") \n\n# Create table of variables and counts\ntab <- table(ds$Variable, ds$value)\n\n# Find proportions and round to 3 decimal places\nprop <- prop.table(tab, margin = 1) %>% \n  round(3)\n\n# Combine everything to one table \ndframe <- data.frame(Variables=rownames(tab), Proportion=prop[,2], Count=tab[,2])\n#remove row names\nrow.names(dframe) <- NULL\n\n# Create table\ngt(dframe) %>% \ntab_header(title = md(\"**LCA Indicator Proportions**\"), subtitle = md(\"&nbsp;\")) %>%\ntab_options(column_labels.font.weight = \"bold\", row_group.font.weight = \"bold\") %>% \ntab_row_group(group = \"Math\", rows = 1:5) %>% \ntab_row_group(group = \"Science\", rows = 6:10) %>%\nrow_group_order(groups = c(\"Math\",\"Science\")) %>% \ntab_options(column_labels.font.weight = \"bold\", row_group.font.weight = \"bold\") \nselect_data <- data %>% \n select(math_enjoy:sci_adult)\n\nf <- All(select_data) ~ Mean + SD + Min + Median + Max + Histogram\ndatasummary(f, data, output=\"markdown\")\nselect_data %>% \n  datasummary_correlation(output = \"markdown\")\nf_cor <- data %>% \n select(math_enjoy:sci_adult) %>% \n  cor(use = \"pairwise.complete.obs\")\n\ncorrplot(f_cor, \n         method = \"circle\",\n         type = \"upper\", \n         tl.col=\"black\", \n         tl.srt=45)"},{"path":"joint-occurrence.html","id":"descriptive-statistics-using-mplusautomation-1","chapter":"15 Joint Occurrence","heading":"15.3.2 Descriptive Statistics using MplusAutomation:","text":"View output (goes detail) see brief view descriptive statistics using get_sampstat():","code":"\nbasic_mplus  <- mplusObject(\n  TITLE = \"Descriptive Statistics;\",\n  \n  VARIABLE =\n    \"usevar = math_enjoy-sci_adult;\n    categorical = math_enjoy-sci_adult;\",\n\n  ANALYSIS = \"TYPE=basic;\",\n  \n  OUTPUT = \"sampstat;\",  \n  \n  usevariables = colnames(data),\n  rdata = data)\n\nbasic_mplus_fit <- mplusModeler(basic_mplus, \n                            dataout = here(\"joint_occurrence\", \"data.dat\"),\n                            modelout = here(\"joint_occurrence\",\"basic.inp\"),\n                            check = TRUE, run = TRUE, hashfilename = FALSE)\n# Using MplusAutomation\nMplusAutomation::get_sampstat(basic_mplus_fit)\n\n# Using base R\nsummary(data)"},{"path":"joint-occurrence.html","id":"enumeration-math-only","chapter":"15 Joint Occurrence","heading":"15.4 Enumeration (Math Only)","text":"code uses mplusObject function MplusAutomation package saves model runs mplus_enum folder.IMPORTANT: moving forward, make sure examine output document ensure models estimated normally. example, last model (6-class models) produce reliable output excluded.","code":"\n\nlca_enum_6  <- lapply(1:6, function(k) {\n  lca_enum  <- mplusObject(\n    \n    TITLE = glue(\"Math Attitudes: {k}-Class\"), \n    \n    VARIABLE = glue(\n      \"categorical = math_enjoy, math_useful, math_logical, math_job, math_adult; \n     usevar = math_enjoy, math_useful, math_logical, math_job, math_adult;\n     classes = c({k});\"),\n    \n    ANALYSIS = \n      \"estimator = mlr; \n    type = mixture;\n    processors = 12;\n    starts = 500 100;\",\n    \n    OUTPUT = \"sampstat residual tech11 tech14;\",\n\n    usevariables = colnames(data),\n    rdata = data)\n  \n  lca_enum_fit <- mplusModeler(lca_enum, \n                               dataout=glue(here(\"joint_occurrence\",\"enum_math\", \"data.dat\")),\n                               modelout=glue(here(\"joint_occurrence\",\"enum_math\", \"c{k}_math.inp\")) ,\n                               check=TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"joint-occurrence.html","id":"enumeration-science-only","chapter":"15 Joint Occurrence","heading":"15.5 Enumeration (Science Only)","text":"code uses mplusObject function MplusAutomation package saves model runs mplus_enum folder.IMPORTANT: moving forward, make sure examine output document ensure models estimated normally. example, last model (6-class models) produce reliable output excluded.","code":"\n\nlca_enum_6  <- lapply(1:6, function(k) {\n  lca_enum  <- mplusObject(\n    \n    TITLE = glue(\"Science Attitudes: {k}-Class\"), \n    \n    VARIABLE = glue(\n      \"categorical = sci_enjoy, sci_useful, sci_logical, sci_job, sci_adult; \n     usevar = sci_enjoy, sci_useful, sci_logical, sci_job, sci_adult;\n     classes = c({k});\"),\n    \n    ANALYSIS = \n      \"estimator = mlr; \n    type = mixture;\n    processors = 12;\n    starts = 500 100;\",\n    \n    OUTPUT = \"sampstat residual tech11 tech14;\",\n\n    usevariables = colnames(data),\n    rdata = data)\n  \n  lca_enum_fit <- mplusModeler(lca_enum, \n                               dataout=glue(here(\"joint_occurrence\",\"enum_sci\", \"data.dat\")),\n                               modelout=glue(here(\"joint_occurrence\",\"enum_sci\", \"c{k}_sci.inp\")) ,\n                               check=TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"joint-occurrence.html","id":"fit-table","chapter":"15 Joint Occurrence","heading":"15.5.0.1 Fit Table","text":"Save table:","code":"\nsource(here(\"functions\", \"enum_table_jo.R\"))\n\n# Read model outputs\noutput_enum_c1 <- readModels(here(\"joint_occurrence\", \"enum_math\"), quiet = TRUE)\noutput_enum_c2 <- readModels(here(\"joint_occurrence\", \"enum_sci\"), quiet = TRUE)\n\n# Define rows for row groups (assuming 6 models per time)\nrows_m1 <- 1:6\nrows_m2 <- 7:12\n\nfit_table_jo <- fit_table_jo(output_enum_c1, output_enum_c2, rows_m1, rows_m2)\nfit_table_jo\ngtsave(fit_table_jo, here(\"figures\", \"fit_table_jo.png\"))"},{"path":"joint-occurrence.html","id":"information-criteria-plot-4","chapter":"15 Joint Occurrence","heading":"15.5.1 Information Criteria Plot","text":"","code":"\nsource(here(\"functions\", \"ic_plot_lca.R\"))\nic_plot(output_enum_c1)\n#ggsave(here(\"figures\", \"info_criteria_jo1.png\"),  dpi = \"retina\", bg = \"white\", height=5, width=7, units=\"in\")\nic_plot(output_enum_c2)\n#ggsave(here(\"figures\", \"info_criteria_jo2.png\"),  dpi = \"retina\", bg = \"white\", height=5, width=7, units=\"in\")"},{"path":"joint-occurrence.html","id":"class-probability-plot-3","chapter":"15 Joint Occurrence","heading":"15.5.2 4-Class Probability Plot","text":"Use plot_lca function provided folder plot item probability plot. function requires one argument:\n- model_name: name Mplus readModels object (e.g., output_enum_c1$c4_math.)","code":"\nsource(here(\"functions\",\"plot_lca.R\"))\n\nplot_lca(model_name = output_enum_c1$c4_math.out)\n#ggsave(here(\"figures\", \"probability_plot_jo1.png\"), dpi = \"retina\", bg = \"white\", height=5, width=7, units=\"in\")\nplot_lca(model_name = output_enum_c2$c4_sci.out)\n#ggsave(here(\"figures\", \"probability_plot_jo2.png\"), dpi = \"retina\", bg = \"white\", height=5, width=7, units=\"in\")"},{"path":"joint-occurrence.html","id":"estimate-joint-occurrence-lca","chapter":"15 Joint Occurrence","heading":"15.6 Estimate Joint Occurrence LCA","text":"","code":""},{"path":"joint-occurrence.html","id":"step-1---estimate-unconditional-model","chapter":"15 Joint Occurrence","heading":"15.6.1 Step 1 - Estimate Unconditional Model","text":"Math AttitudesHere, included ID variable (casenum) can later join two datasets get step 2.Note: Since emerging classes similar math science, rearranged classes match using svaues option OUTPUT command. example, Class 1 Science LCA Class 4 Math LCA “High” class. changed Math class Class 4 Class 1.\nEvaluate output compare class counts proportions latent classes. Using OPTSEED function ensures replication best loglikelihood value run.Science AttitudesConfirm plots look expected (.e., identical enumeration model)","code":"\n\nstep1  <- mplusObject(\n  TITLE = \"Step 1 - Unconditional Model\", \n  VARIABLE = \"categorical = math_enjoy, math_useful, math_logical, math_job, math_adult;\n  usevar =  math_enjoy, math_useful, math_logical, math_job, math_adult;\n  idvariable = casenum; \n  classes = c(4);\",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 0;\n    OPTSEED = 830570;\",\n  \n  SAVEDATA = \n   \"File=savedata_math.dat;\n    Save=cprob;\",\n  \n  OUTPUT = \"sampstat residual tech11 tech14 svalues(4 1 2 3)\",  # I used `svalues` to rearrange the class labels\n  \n  usevariables = colnames(data),\n  rdata = data)\n\nstep1_fit <- mplusModeler(step1,\n                            dataout=here(\"joint_occurrence\", \"jo_model\", \"data.dat\"),\n                            modelout=here(\"joint_occurrence\", \"jo_model\",  \"one_math.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\n\nstep1  <- mplusObject(\n  TITLE = \"Step 1 - Unconditional Model\", \n  VARIABLE = \"categorical = sci_enjoy, sci_useful, sci_logical, sci_job, sci_adult;\n  usevar =  sci_enjoy, sci_useful, sci_logical, sci_job, sci_adult;\n  idvariable = casenum;\n  classes = c(4);\",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 0;\n    OPTSEED = 761633;\",\n  \n  SAVEDATA = \n   \"File=savedata_sci.dat;\n    Save=cprob;\",\n  \n  OUTPUT = \"sampstat residual tech11 tech1;\",\n  \n  usevariables = colnames(data),\n  rdata = data)\n\nstep1_fit <- mplusModeler(step1,\n                            dataout=here(\"joint_occurrence\", \"jo_model\", \"data.dat\"),\n                            modelout=here(\"joint_occurrence\", \"jo_model\",  \"one_sci.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\nsource(here(\"functions\",\"plot_lca.R\"))\n\noutput_math <- readModels(here(\"joint_occurrence\",\"jo_model\",\"one_math.out\"))\noutput_sci <- readModels(here(\"joint_occurrence\",\"jo_model\",\"one_sci.out\"))\n\nplot_lca(model_name = output_math)\nplot_lca(model_name = output_sci)"},{"path":"joint-occurrence.html","id":"step-2---determine-measurement-error-5","chapter":"15 Joint Occurrence","heading":"15.6.2 Step 2 - Determine Measurement Error","text":"Extract logits classification probabilities likely latent class:Extract saved dataset:Rename column savedata named “C” change “N”","code":"\n\nlogit_cprobs_math <- as.data.frame(output_math[[\"class_counts\"]]\n                                       [[\"logitProbs.mostLikely\"]])\n\nlogit_cprobs_sci <- as.data.frame(output_sci[[\"class_counts\"]]\n                                       [[\"logitProbs.mostLikely\"]])\n\nsavedata_math <- as.data.frame(output_math[[\"savedata\"]])\nsavedata_sci <- as.data.frame(output_sci[[\"savedata\"]])\n\ncolnames(savedata_math)[colnames(savedata_math)==\"C\"] <- \"N_math\"\ncolnames(savedata_sci)[colnames(savedata_sci)==\"C\"] <- \"N_sci\"\n\nsavedata <- savedata_math %>% \n  full_join(savedata_sci, by = \"CASENUM\")"},{"path":"joint-occurrence.html","id":"step-3---add-auxiliary-variables-2","chapter":"15 Joint Occurrence","heading":"15.6.3 Step 3 - Add Auxiliary Variables","text":"Build joint occurrence model:","code":"\nstep3_jo  <- mplusObject(\n  TITLE = \"Joint Occurrence LCA\", \n  \n  VARIABLE = \n \"nominal=N_math N_sci;\n  usevar = N_math N_sci;\n  classes = math(4) sci(4);\" ,\n  \n  ANALYSIS = \n \"estimator = mlr; \n  type = mixture; \n  starts = 0;\",\n \n  MODEL =\n  glue(\n \" %OVERALL%\n      \n   sci on math; \n\n  MODEL math:\n  \n  %math#1%\n  [N_math#1@{logit_cprobs_math[1,1]}];\n  [N_math#2@{logit_cprobs_math[1,2]}];\n  [N_math#3@{logit_cprobs_math[1,3]}];\n\n\n  %math#2%\n  [N_math#1@{logit_cprobs_math[2,1]}];\n  [N_math#2@{logit_cprobs_math[2,2]}];\n  [N_math#3@{logit_cprobs_math[2,3]}];\n  \n  %math#3%\n  [N_math#1@{logit_cprobs_math[3,1]}]; \n  [N_math#2@{logit_cprobs_math[3,2]}];\n  [N_math#3@{logit_cprobs_math[3,3]}];\n  \n  %math#4%\n  [N_math#1@{logit_cprobs_math[4,1]}]; \n  [N_math#2@{logit_cprobs_math[4,2]}];\n  [N_math#3@{logit_cprobs_math[4,3]}];  \n\n \n  MODEL sci:\n  \n  %sci#1%\n  [N_sci#1@{logit_cprobs_sci[1,1]}]; \n  [N_sci#2@{logit_cprobs_sci[1,2]}];\n  [N_sci#3@{logit_cprobs_sci[1,3]}];  \n\n  %sci#2%\n  [N_sci#1@{logit_cprobs_sci[2,1]}];\n  [N_sci#2@{logit_cprobs_sci[2,2]}];\n  [N_sci#3@{logit_cprobs_sci[2,3]}];\n  \n  %sci#3%\n  [N_sci#1@{logit_cprobs_sci[3,1]}];\n  [N_sci#2@{logit_cprobs_sci[3,2]}];\n  [N_sci#3@{logit_cprobs_sci[3,3]}];\n \n  %sci#4%\n  [N_sci#1@{logit_cprobs_sci[4,1]}];\n  [N_sci#2@{logit_cprobs_sci[4,2]}];\n  [N_sci#3@{logit_cprobs_sci[4,3]}];\"),\n \n  usevariables = colnames(savedata), \n  rdata = savedata)\n\nstep3_jo_fit <- mplusModeler(step3_jo,\n               dataout=here(\"joint_occurrence\",\"jo_model\",\"three.dat\"), \n               modelout=here(\"joint_occurrence\",\"jo_model\",\"three.inp\"), \n               check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"joint-occurrence.html","id":"joint-distribution","chapter":"15 Joint Occurrence","heading":"15.6.3.1 Joint Distribution","text":"Plot:Alternative plot:Table:Always check output make sure table correct!","code":"\njo_output <- readModels(here(\"joint_occurrence\",\"jo_model\",\"three.out\"))\n\nplot_lca(model_name = output_math)\nplot_lca(model_name = output_sci)\n\nsource(here(\"functions\", \"plot_patterns.R\"))\n\ntitle <- \"Joint Occurrence Model Patterns\"\nsubtitle <- \"Math & Science Attitudes\"\n\nplot_patterns(\n  model_name = jo_output,\n  facet_labels =c(              # These are the Math labels\n    `1` = \"Pro-Math with Elevated Utility Value\",\n    `2` = \"Math Ambivalent with Minimal Utility Value\",\n    `3` = \"Math Ambivalent with Elevated Utility Value\",\n    `4` = \"Anti-Math with Minimal Utility Value\"),\n  lca_labels = c('1' = \"Math Attitudes\", '2' = \"Science Attitudes\"), \n  class_labels = c(             # These are the Science labels\n    \"Pro-Science with Elevated Utility Value\",\n    \"Science Ambivalent with Minimal Utility Value\",\n    \"Science Ambivalent with Elevated Utility Value\",\n    \"Anti-Science with Minimal Utility Value\"\n    ), \n  title,\n  subtitle\n  )\n\n\n#ggsave(here(\"figures\",\"interdependencies_plot.png\"), dpi=500,bg = \"white\", height=7, width=12, units=\"in\")\n\njo_output <- readModels(here(\"joint_occurrence\",\"jo_model\",\"three.out\"))\njo_prob <- as.data.frame(jo_output$class_counts$transitionProbs$probability)\n\nc1_labels <- c(\"Pro-Math \\nwith Elevated Utility Value \\n(46%)\", \n               \"Math Ambivalent \\nwith Minimal Utility Value\\n(18%)\", \n               \"Math Ambivalent \\nwith Elevated Utility Value\\n(19%)\",\n               \"Anti-Math \\nwith Minimal Utility Value\\n(17%)\")\nc2_labels <- c(\"Pro-Science \\nwith Elevated Utility Value\\n(30%)\",\n               \"Science Ambivalent \\nwith Minimal Utility Value\\n(26%)\",\n               \"Science Ambivalent \\nwith Elevated Utility Value\\n(8%)\",\n               \"Anti-Science \\nwith Minimal Utility Value\\n(36%)\")\n\n\n# T1 → T2\nc1_c2 <- expand.grid(C1 = c1_labels, C2 = c2_labels) %>%\n  mutate(P12 = jo_prob[1:nrow(jo_prob), 1]) %>% \n  mutate(P12 = round(P12, 2))\n\n\n# Plot for T1 -> T2\n ggplot(c1_c2, aes(axis1 = C1, axis2 = C2, y = P12)) +\n  geom_alluvium(aes(fill = C1), width = 0.2, alpha = 0.7) +\n  # Make the stratum rectangles white instead of gray:\n  geom_stratum(width = 0.2, color = \"black\") +\n  geom_text(\n    stat = \"stratum\", \n    aes(label = after_stat(stratum)), \n    size = 3.5\n  ) +\n  # Label the flows themselves with the probability\n # geom_text(aes(label = P12),\n#            stat = \"flow\", nudge_x = .2, size = 5) +\n  scale_x_discrete(limits = c(\"Math Attitudes\", \"Science Attitudes\"), expand = c(.1, .1)) +\n  labs(subtitle = \"Math and Science Attitudes\", title = \"Joint Occurrence Model\", x = \"\") +\n  theme_minimal() +\n  theme(\n    text = element_text(family = \"serif\", size = 20),\n    legend.position = \"none\",\n    axis.text.x = element_text(color = \"black\"),\n    axis.title.y = element_blank(),  \n    axis.text.y  = element_blank(), \n    axis.ticks.y = element_blank(),\n    panel.grid.major = element_blank(), \n    panel.grid.minor = element_blank(),  \n    plot.subtitle = element_text(face = \"italic\", size = 20),\n    plot.title = element_text(size = 20)\n    )\n#ggsave(here(\"figures\", \"jo_sankey.jpg\"), width=8, height = 5.5, dpi=\"retina\", bg = \"white\",  units=\"in\")\n# Extract Probabilities\njo_prob_matrix <- as.matrix(jo_output$class_counts$transitionProbs$probability)\n\n\n# Label Classes\nc1_labels <- c(\"Pro-Math \\nwith Elevated Utility Value \\n(46%)\", \n               \"Math Ambivalent \\nwith Minimal Utility Value\\n(18%)\", \n               \"Math Ambivalent \\nwith Elevated Utility Value\\n(19%)\",\n               \"Anti-Math \\nwith Minimal Utility Value\\n(17%)\")\nc2_labels <- c(\"Pro-Science \\nwith Elevated Utility Value\\n(30%)\",\n               \"Science Ambivalent \\nwith Minimal Utility Value\\n(26%)\",\n               \"Science Ambivalent \\nwith Elevated Utility Value\\n(8%)\",\n               \"Anti-Science \\nwith Minimal Utility Value\\n(36%)\")\n\n# Number of Classes for each LCA\nC1 <- length(c1_labels)  \nC2 <- length(c2_labels)  \n\n# Format Probability Table\njo_df <- matrix(jo_prob_matrix, nrow = C1, ncol = C2, byrow = FALSE)\nrownames(jo_df) <- c1_labels\ncolnames(jo_df) <- c2_labels\nt_matrix <- as.data.frame(jo_df) %>%\n  rownames_to_column(var = \"Math Attitudes\")\n\n# Create Probability Table\nt_matrix %>% \n  gt(rowname_col = \"Math Attitudes\") %>%\n  tab_stubhead(label = \"Math Attitudes\") %>% \n  tab_header(\n    title = md(\"**Joint Distribution Matrix**\"),\n    subtitle = md(\"**Distribution Math Attitude Classes (Rows) conditioned on Science Attitude Classes (Columns)**\")) %>% \n  fmt_number(2:4,decimals = 3) %>% \n  tab_spanner(label = \"Science Attitudes\",columns = 2:(C2+1))#%>% \n  #gtsave(\"matrix.docx\")"},{"path":"growth-mixture-models.html","id":"growth-mixture-models","chapter":"16 Growth Mixture Models","heading":"16 Growth Mixture Models","text":"Example: Longitudinal Study American YouthData source: : example looks science IRT scores time (Grades 7-12). See documentation . Covariates include gender interest science issues 7th grade.","code":""},{"path":"growth-mixture-models.html","id":"load-packages-6","chapter":"16 Growth Mixture Models","heading":"16.1 Load Packages","text":"","code":"\nlibrary(tidyverse)\nlibrary(MplusAutomation)\nlibrary(here)\nlibrary(DiagrammeR)\nlibrary(glue)\nlibrary(cowplot)\nlibrary(gt)\nlibrary(Hmisc)"},{"path":"growth-mixture-models.html","id":"path-diagram-1","chapter":"16 Growth Mixture Models","heading":"16.2 Path Diagram","text":"Read LSAY dataset","code":"\nlsay_sci <- read_csv(here(\"data\",\"lsay_sci_gmm.csv\")) %>% \n  rename(\n    id = CASENUM,\n    female = GENDER,\n    interest7 = AB34D,\n    sci7 = ASCIIRT,\n    sci8 = CSCIIRT,\n    sci9 = ESCIIRT, \n    sci10 = GSCIIRT,\n    sci11 = ISCIIRT,\n    sci12 = KSCIIRT\n  ) %>% \n  mutate(female = ifelse(female == 1, 1, 0))"},{"path":"growth-mixture-models.html","id":"descriptive-statistics-13","chapter":"16 Growth Mixture Models","heading":"16.2.1 Descriptive Statistics","text":"","code":"\nlsay_sci %>% \n  select(-id) %>% \n  psych::describe()\n#>           vars    n  mean    sd median trimmed   mad   min\n#> female       1 5945  0.49  0.50   0.00    0.49  0.00  0.00\n#> interest7    2 2992  1.80  0.76   2.00    1.76  1.48  1.00\n#> sci7         3 3071 50.41 10.20  50.04   50.18 11.33 26.14\n#> sci8         4 2527 54.05 11.16  54.64   54.25 12.11 22.82\n#> sci9         5 2326 58.69 11.24  60.40   59.15 11.05 27.36\n#> sci10        6 4690 60.32 11.02  60.84   60.50 11.72 26.97\n#> sci11        7 3592 64.10 11.21  64.75   64.51 11.10 24.44\n#> sci12        8 2826 65.85 11.65  66.25   66.28 11.14 26.38\n#>             max range  skew kurtosis   se\n#> female     1.00  1.00  0.04    -2.00 0.01\n#> interest7  3.00  2.00  0.34    -1.20 0.01\n#> sci7      88.03 61.89  0.20    -0.46 0.18\n#> sci8      83.94 61.12 -0.17    -0.65 0.22\n#> sci9      91.21 63.85 -0.35    -0.40 0.23\n#> sci10     91.33 64.36 -0.14    -0.43 0.16\n#> sci11     93.13 68.69 -0.34    -0.17 0.19\n#> sci12     95.56 69.18 -0.35     0.02 0.22"},{"path":"growth-mixture-models.html","id":"correlation-table-1","chapter":"16 Growth Mixture Models","heading":"16.2.1.1 Correlation Table","text":"","code":"\ncor_data <- lsay_sci %>% \n  select(-id)\n\nrcorr(as.matrix(cor_data)) \n#>           female interest7  sci7  sci8  sci9 sci10 sci11\n#> female      1.00     -0.15 -0.07 -0.01 -0.05 -0.09 -0.11\n#> interest7  -0.15      1.00  0.24  0.24  0.25  0.26  0.22\n#> sci7       -0.07      0.24  1.00  0.81  0.76  0.75  0.74\n#> sci8       -0.01      0.24  0.81  1.00  0.85  0.81  0.81\n#> sci9       -0.05      0.25  0.76  0.85  1.00  0.88  0.86\n#> sci10      -0.09      0.26  0.75  0.81  0.88  1.00  0.88\n#> sci11      -0.11      0.22  0.74  0.81  0.86  0.88  1.00\n#> sci12      -0.14      0.26  0.74  0.76  0.82  0.83  0.90\n#>           sci12\n#> female    -0.14\n#> interest7  0.26\n#> sci7       0.74\n#> sci8       0.76\n#> sci9       0.82\n#> sci10      0.83\n#> sci11      0.90\n#> sci12      1.00\n#> \n#> n\n#>           female interest7 sci7 sci8 sci9 sci10 sci11 sci12\n#> female      5945      2992 3071 2527 2326  4690  3592  2826\n#> interest7   2992      2992 2951 2431 2236  1907  1531  1103\n#> sci7        3071      2951 3071 2494 2297  1958  1573  1134\n#> sci8        2527      2431 2494 2527 2067  1745  1436  1051\n#> sci9        2326      2236 2297 2067 2326  1792  1432  1043\n#> sci10       4690      1907 1958 1745 1792  4690  3323  2621\n#> sci11       3592      1531 1573 1436 1432  3323  3592  2433\n#> sci12       2826      1103 1134 1051 1043  2621  2433  2826\n#> \n#> P\n#>           female interest7 sci7   sci8   sci9   sci10 \n#> female           0.0000    0.0003 0.6275 0.0264 0.0000\n#> interest7 0.0000           0.0000 0.0000 0.0000 0.0000\n#> sci7      0.0003 0.0000           0.0000 0.0000 0.0000\n#> sci8      0.6275 0.0000    0.0000        0.0000 0.0000\n#> sci9      0.0264 0.0000    0.0000 0.0000        0.0000\n#> sci10     0.0000 0.0000    0.0000 0.0000 0.0000       \n#> sci11     0.0000 0.0000    0.0000 0.0000 0.0000 0.0000\n#> sci12     0.0000 0.0000    0.0000 0.0000 0.0000 0.0000\n#>           sci11  sci12 \n#> female    0.0000 0.0000\n#> interest7 0.0000 0.0000\n#> sci7      0.0000 0.0000\n#> sci8      0.0000 0.0000\n#> sci9      0.0000 0.0000\n#> sci10     0.0000 0.0000\n#> sci11            0.0000\n#> sci12     0.0000"},{"path":"growth-mixture-models.html","id":"spaghetti-plot","chapter":"16 Growth Mixture Models","heading":"16.2.1.2 Spaghetti Plot","text":"","code":"\nplot_data <- lsay_sci[1:500,] %>%\n  drop_na() %>% \n  pivot_longer(cols = starts_with(\"sci\"),  \n               names_to = \"grade\",          \n               values_to = \"value\") %>% \n  mutate(grade = factor(grade, \n                        levels = c(\"sci7\", \"sci8\", \"sci9\", \"sci10\", \"sci11\", \"sci12\"), \n                        labels = c(7,8,9,10,11,12)))\n\nmean_sci <- plot_data %>%\n  drop_na() %>% \n  group_by(grade) %>%\n  summarise(mean_response = mean(value),\n            se_response = sd(value) / sqrt(n()))\n\nggplot() +                                                                   \n  geom_point(data = plot_data, aes(x = grade, y = value, group = id), alpha = .3) +     \n  geom_line(data = plot_data, aes(x = grade, y = value, group = id), alpha = .3) +      \n  geom_point(data=mean_sci, aes(x=grade, y = mean_response), color = \"Blue\", size = 1.2) +               \n  geom_line(data=mean_sci, aes(x=grade, y = mean_response, group = 1), color = \"blue\", size = 1.2) + \n  geom_errorbar(data = mean_sci, aes(x = grade, ymin = mean_response - se_response, \n                                      ymax = mean_response + se_response),\n                width = 0.2, size = 1.2, color = \"blue\") +  \n  labs(title = \"Spaghetti Plot with Mean Line and Error Bars\",\n       x=\"Grade\", \n       y=\"Science Score\") +                                                           \n  theme_cowplot()                                                                              "},{"path":"growth-mixture-models.html","id":"unconditional-growth-mixture-model","chapter":"16 Growth Mixture Models","heading":"16.2.2 Unconditional Growth Mixture Model","text":"MplusAutomation code loops class-specific statements include freeing variances covariances.","code":"\ngmm_6 <- lapply(1:6, function(k){ \n  \n  # This MODEL section changes the model specification\n  MODEL <- paste(sapply(1:k, function(i) {\n    glue(\"\n    %c#{i}%\n    s WITH I;      ! covariances are freely estimated\n    sci7-sci12;    ! variances are freely estimated\n    \")\n  }), collapse = \"\\n\")\n  \n  gmm_enum  <- mplusObject(\n    TITLE = glue(\"GMM {k}-Class\"), \n    \n    VARIABLE = glue(\n    \"usevar = sci7-sci12; \n     classes = c({k}); \"),\n    \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 200 100; \n    processors = 12;\",\n    \n    MODEL = glue(\"%OVERALL%\n       i s | sci7@0 sci8@1 sci9@2 sci10@3 sci11@4 sci12@5;\n       \n       {MODEL}\"), # The `MODEL` object is placed here.\n    \n    OUTPUT = \"tech1 tech11 tech14 sampstat standardized svalues;\",\n    \n  SAVEDATA = \n    glue(\"FILE IS savedata_c{k}.dat;\n     SAVE = cprobabilities;\"),\n  \n  PLOT = \"type=plot3;\n          series = sci7-sci12(*)\",\n  \n  usevariables = colnames(lsay_sci),\n  rdata = lsay_sci)\n\ngmm_enum_fit <- mplusModeler(gmm_enum, \n                            dataout=glue(here(\"gmm\", \"gmm_enum\", \"gmm_lsay.dat\")),\n                            modelout=glue(here(\"gmm\", \"gmm_enum\", \"c{k}_gmm_lsay.inp\")) ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"growth-mixture-models.html","id":"table-of-fit-4","chapter":"16 Growth Mixture Models","heading":"16.2.2.1 Table of Fit","text":"First, extract data:, create table:","code":"\n\noutput_gmm <- readModels(here(\"gmm\",\"gmm_enum\"), filefilter = \"gmm\", quiet = TRUE)\n\n# Extract fit indices\nenum_extract <- LatexSummaryTable(\n  output_gmm,\n  keepCols = c(\n    \"Title\",\n    \"Parameters\",\n    \"LL\",\n    \"BIC\",\n    \"aBIC\",\n    \"BLRT_PValue\",\n    \"T11_VLMR_PValue\",\n    \"Observations\"\n  ),\n  sortBy = \"Title\"\n)\n\n# Extract lowest class size\nmin_sizes <- map_df(names(output_gmm), ~ {\n  model <- output_gmm[[.x]]\n  min_size <- min(model$class_counts$modelEstimated$proportion) * 100\n  tibble(Model = .x, min_cs = round(min_size, 2))\n})\n\n# Combine dataframe\ncombined <- cbind(enum_extract, min_sizes)\n\n# Calculate additional fit indices\nallFit <- combined %>%\n  mutate(CAIC = -2 * LL + Parameters * (log(Observations) + 1)) %>%\n  mutate(AWE = -2 * LL + 2 * Parameters * (log(Observations) + 1.5)) %>%\n  mutate(SIC = -.5 * BIC) %>%\n  mutate(expSIC = exp(SIC - max(SIC))) %>%\n  mutate(BF = exp(SIC - lead(SIC))) %>%\n  mutate(cmPk = expSIC / sum(expSIC)) %>%\n  dplyr::select(Title, Parameters, min_cs, LL, BIC, aBIC, CAIC, AWE, BLRT_PValue, T11_VLMR_PValue, BF, cmPk) %>%\n  arrange(Parameters)\nfit_table1 <- allFit %>%\n  gt() %>%\n  tab_header(title = md(\"**Model Fit Summary Table**\")) %>%\n  cols_label(\n    Title = \"Classes\",\n    Parameters = md(\"Par\"),\n    min_cs = md(\"Min. Class Size\"),\n    LL = md(\"*LL*\"),\n    T11_VLMR_PValue = \"VLMR\",\n    BLRT_PValue = \"BLRT\",\n    BF = md(\"BF\"),\n    cmPk = md(\"*cmPk*\")\n  ) %>%\n  tab_footnote(\n    footnote = md(\n      \"*Note.* Par = Parameters; *LL* = model log likelihood;\nBIC = Bayesian information criterion;\naBIC = sample size adjusted BIC; CAIC = consistent Akaike information criterion;\nAWE = approximate weight of evidence criterion;\nBLRT = bootstrapped likelihood ratio test p-value;\nVLMR = Vuong-Lo-Mendell-Rubin adjusted likelihood ratio test p-value;\n*cmPk* = approximate correct model probability.\"\n    ),\nlocations = cells_title()\n  ) %>%\n  tab_options(column_labels.font.weight = \"bold\") %>%\n  fmt_number(c(3:8),\n             decimals = 2) %>%\n  fmt_missing(1:12,\n              missing_text = \"--\") %>%\n  fmt(\n    c(9:10, 12),\n    fns = function(x)\n      ifelse(x < 0.001, \"<.001\",\n             scales::number(x, accuracy = .01))\n  ) %>%\n  fmt(\n    11,\n    fns = function (x)\n      ifelse(x > 100, \">100\",\n             scales::number(x, accuracy = .01))\n  ) %>%  \n  tab_style(\n    style = list(\n      cell_text(weight = \"bold\")\n      ),\n    locations = list(cells_body(\n     columns = BIC,\n     row = BIC == min(BIC[c(1:6)]) # Change this to the number of classes you estimated\n    ),\n    cells_body(\n     columns = aBIC,\n     row = aBIC == min(aBIC[1:6])\n    ),\n    cells_body(\n     columns = CAIC,\n     row = CAIC == min(CAIC[1:6])\n    ),\n    cells_body(\n     columns = AWE,\n     row = AWE == min(AWE[1:6])\n    ),\n    cells_body(\n     columns = cmPk,\n     row =  cmPk == max(cmPk[1:6])\n     ),    \n    cells_body(\n     columns = BF,\n     row =  BF > 10),\n    cells_body( \n     columns =  T11_VLMR_PValue,\n     row =  ifelse(T11_VLMR_PValue < .001 & lead(T11_VLMR_PValue) > .05, T11_VLMR_PValue < .001, NA)),\n    cells_body(\n     columns =  BLRT_PValue,\n     row =  ifelse(BLRT_PValue < .001 & lead(BLRT_PValue) > .05, BLRT_PValue < .001, NA))\n  )\n)\n\nfit_table1"},{"path":"growth-mixture-models.html","id":"information-criteria-plot-5","chapter":"16 Growth Mixture Models","heading":"16.2.2.2 Information Criteria Plot","text":"","code":"\nallFit %>%\n  dplyr::select(LL:AWE) %>%\n  rowid_to_column() %>%\n  pivot_longer(`BIC`:`AWE`,\n               names_to = \"Index\",\n               values_to = \"ic_value\") %>%\n  mutate(Index = factor(Index,\n                        levels = c (\"AWE\", \"CAIC\", \"BIC\", \"aBIC\"))) %>%\n  ggplot(aes(\n    x = rowid,\n    y = ic_value,\n    color = Index,\n    shape = Index,\n    group = Index,\n    lty = Index\n  )) +\n  geom_point(size = 2.0) + geom_line(size = .8) +\n  scale_x_continuous(breaks = 1:nrow(allFit)) +\n  scale_colour_grey(end = .5) +\n  theme_cowplot() +\n  labs(x = \"Number of Classes\", y = \"Information Criteria Value\", title = \"Information Criteria\") +\n  theme(\n    text = element_text(family = \"Times\", size = 12),\n    legend.text = element_text(family=\"Times\", size=12),\n    legend.key.width = unit(3, \"line\"),\n    legend.title = element_blank(),\n    legend.position = \"top\"  \n  )"},{"path":"growth-mixture-models.html","id":"plot-gmm","chapter":"16 Growth Mixture Models","heading":"16.2.2.3 Plot GMM","text":"","code":"\nplotGrowthMixtures(output_gmm, estimated = TRUE, rawdata = TRUE, \n                   time_scale = c(1, 2, 3, 4, 5, 6), alpha_range = c(0, 0.01))"},{"path":"growth-mixture-models.html","id":"covariates-growth-mixture-model","chapter":"16 Growth Mixture Models","heading":"16.2.3 Covariates Growth Mixture Model","text":"Two covariates used GMM analysis related latent class variable: gender interest science issues 7th grade.","code":"\n\nstep1  <- mplusObject(\n  TITLE = \"GMM with Covariates\", \n  VARIABLE = \n  \"usevar = sci7-sci12\n  female interest7;\n    \n   classes = c(4);\",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 200 100; \n    processors = 12;\",\n  \n  MODEL = \n    \"%OVERALL%\n    \n    i s on female interest7;\n    \n    i s | sci7@0 sci8@1 sci9@2 sci10@3 sci11@4 sci12@5;\n    \n    %c#1%\n    s WITH I;      ! covariances are freely estimated\n    sci7-sci12;    ! variances are freely estimated\n    i s on female interest7;\n    \n    %c#2%\n    s WITH I;     \n    sci7-sci12;   \n    i s on female interest7;\n    \n    %c#3%\n    s WITH I;     \n    sci7-sci12;    \n    i s on female interest7;\n    \n    %c#4%\n    s WITH I;     \n    sci7-sci12;   \n    i s on female interest7;\",\n  \n  OUTPUT = \"tech1 tech11 tech14 sampstat standardized svalues;\",\n    \n  SAVEDATA = \n    glue(\"FILE IS savedata_c4.dat;\n     SAVE = cprobabilities;\"),\n  \n  PLOT = \"type=plot3;\n          series = sci7-sci12(*)\",\n  \n  usevariables = colnames(lsay_sci),\n  rdata = lsay_sci)\n\nstep1_fit <- mplusModeler(step1,\n                            dataout=here(\"gmm\", \"gmm_cov\", \"gmm_cov.dat\"),\n                            modelout=here(\"gmm\", \"gmm_cov\", \"gmm_cov.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"growth-mixture-models.html","id":"plot-gmm-1","chapter":"16 Growth Mixture Models","heading":"16.2.3.1 Plot GMM","text":"","code":"\ngmm_cov <- readModels(here(\"gmm\", \"gmm_cov\", \"gmm_cov.out\"))\n\nplotGrowthMixtures(gmm_cov, estimated = TRUE, rawdata = TRUE, \n                   time_scale = c(1, 2, 3, 4, 5, 6), alpha_range = c(0, 0.01), bw = TRUE)"},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"distal-outcomes-nylund-gibson-grimm-masyn-2019","chapter":"17 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"17 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","text":"chapter replicates workflow Nylund-Gibson et al. (2019).Citation: Nylund-Gibson, K., Grimm, R. P., & Masyn, K. E. (2019). Prediction latent classes: demonstration different approaches include distal outcomes mixture models. Structural equation modeling: multidisciplinary Journal, 26(6), 967-985.","code":""},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"data-overview","chapter":"17 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"17.1 Data overview","text":"Math attitude indicator:Five math attitudinal variables used response indicators LCA model.enjoym - enjoy math.enjoym - enjoy math.goodm - good math.goodm - good math.udnerstandm - usually understand math.udnerstandm - usually understand math.nervousm - math often makes nervous upset.nervousm - math often makes nervous upset.scaredm - often get scared open math book see page problems.scaredm - often get scared open math book see page problems.Covariate:female - gender (female = 1; male = 0)Distal outcomes:mathjob - Math job (1 = student felt math useful later job; 0 = student feel math useful later job)mathjob - Math job (1 = student felt math useful later job; 0 = student feel math useful later job)mathirt - Math IRT score grade 9 (continuous, higher math scores represent higher math achievement)mathirt - Math IRT score grade 9 (continuous, higher math scores represent higher math achievement)","code":""},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"load-packages-7","chapter":"17 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"17.2 Load Packages","text":"","code":"\nlibrary(MplusAutomation)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(glue)\nlibrary(gt)\nlibrary(cowplot)\nlibrary(kableExtra)\nlibrary(psych)\nlibrary(float)\nlibrary(janitor)\nlibrary(naniar)"},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"data-preparation","chapter":"17 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"17.3 Data Preparation","text":"","code":"\nlsay_df <- read_csv(here(\"distals\", \"data\", \"lsay_df.csv\"))"},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"descriptive-statistics-14","chapter":"17 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"17.4 Descriptive Statistics","text":"","code":"\npsych::describe(lsay_df)\n#>          vars    n  mean    sd median trimmed   mad   min\n#> enjoym      1 2668  0.67  0.47    1.0    0.71  0.00  0.00\n#> goodm       2 2670  0.69  0.46    1.0    0.74  0.00  0.00\n#> undrstdm    3 2648  0.76  0.43    1.0    0.83  0.00  0.00\n#> nervousm    4 2622  0.59  0.49    1.0    0.61  0.00  0.00\n#> scaredm     5 2651  0.69  0.46    1.0    0.73  0.00  0.00\n#> mathjob     6 2321  0.69  0.46    1.0    0.74  0.00  0.00\n#> mathirt     7 2241 58.81 12.60   59.3   58.93 13.49 26.57\n#> female      8 3116  0.48  0.50    0.0    0.47  0.00  0.00\n#>            max range  skew kurtosis   se\n#> enjoym    1.00  1.00 -0.72    -1.49 0.01\n#> goodm     1.00  1.00 -0.84    -1.30 0.01\n#> undrstdm  1.00  1.00 -1.24    -0.47 0.01\n#> nervousm  1.00  1.00 -0.36    -1.87 0.01\n#> scaredm   1.00  1.00 -0.81    -1.35 0.01\n#> mathjob   1.00  1.00 -0.81    -1.34 0.01\n#> mathirt  94.19 67.62 -0.06    -0.55 0.27\n#> female    1.00  1.00  0.09    -1.99 0.01\nvis_miss(lsay_df)"},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"enumeration---maysn-2017","chapter":"17 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"17.5 Enumeration - Maysn (2017)","text":"","code":"\nlca_math  <- lapply(1:6, function(k) {\n  lca_math_enum  <- mplusObject(\n      \n    TITLE = glue(\"{k}-Class\"), \n  \n    VARIABLE = glue(\n    \"categorical = enjoym-scaredm; \n     usevar = enjoym-scaredm;\n     classes = c({k}); \"),\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 200 100; \n    processors = 10;\",\n  \n  OUTPUT = \"sampstat residual tech11 tech14 svalues;\",\n  \n  PLOT = \n    \"type = plot3; \n    series = enjoym-scaredm(*);\",\n  \n  usevariables = colnames(lsay_df),\n  rdata = lsay_df)\n\nlca_enum_fit <- mplusModeler(lca_math_enum, \n                            dataout=here(\"distals\",\"enum\",\"enum.dat\"),\n                            modelout=glue(here(\"distals\",\"enum\",\"c{k}_enum.inp\")),\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"distal-as-indicator-approach","chapter":"17 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"17.6 Distal-as-indicator approach","text":"distal--indicator approach, also called 1-step approach latent class variable distal outcome means measured one modeling step (Vermunt, 2010), distal outcome part measurement model latent variable; , distal outcome treated indicator latent class variable (Muthén & Shedden, 1999).model, five math attitude variables, gender covariate, two distal outcomes included class indicators.","code":"\ndasi <- lapply(1:6, function(k) {\n  lca_enum  <- mplusObject(\n      \n    TITLE = glue(\"{k}-Class\"), \n  \n    VARIABLE = glue(\n    \"categorical = enjoym-mathjob female; \n     usevar = enjoym-female;\n     classes = c({k});\"),\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 200 100; \n    processors = 10;\",\n  \n  OUTPUT = \"sampstat residual tech1 tech8 tech10 tech11 tech14 svalues;\",\n  \n  usevariables = colnames(lsay_df),\n  rdata = lsay_df)\n\ndasi_enum_fit <- mplusModeler(lca_enum, \n                            dataout=here(\"distals\",\"enum_dasi\",\"dasi.dat\"),\n                            modelout=glue(here(\"distals\",\"enum_dasi\",\"c{k}_dasi.inp\")),\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"table-of-fit-5","chapter":"17 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"17.6.1 Table of fit","text":"Create table:results distal--indicator approach suggested 4-class model well. Though differences emergent latent classes compared classes unconditional LCA model without distals, substantive interpretation largely remain respect math attitudinal variables.distal mean binary outcome variable (case, math job) can located “RESULTS PROBABILITY SCALE” section. excerpt presents distal mean math job Latent Class 1.distal mean continuous outcome variable (case, math IRT score) can located “MODEL RESULTS” section. excerpt shows distal mean math IRT score Latent Class 1.","code":"\nsource(here(\"functions\", \"extract_mplus_info.R\"))\n\n# Define the directory where all of the .out files are located.\noutput_dir <- here(\"distals\", \"enum_dasi\")\n\n# Get all .out files\noutput_files <- list.files(output_dir, pattern = \"\\\\.out$\", full.names = TRUE)\n\n# Process all .out files into one dataframe\nfinal_data <- map_dfr(output_files, extract_mplus_info_extended)\n\n# Extract Sample_Size from final_data\nsample_size <- unique(final_data$Sample_Size)\n\n# Read in .out files into `MplusAutomation`\noutput_dasi <- readModels(here(\"distals\", \"enum_dasi\"),\n                          filefilter = \"dasi\",\n                          quiet = TRUE)\n\n# Extract fit indices\nenum_extract <- LatexSummaryTable(\n  output_dasi,\n  keepCols = c(\n    \"Title\",\n    \"Parameters\",\n    \"LL\",\n    \"BIC\",\n    \"aBIC\",\n    \"BLRT_PValue\",\n    \"T11_VLMR_PValue\",\n    \"Observations\"\n  ),\n  sortBy = \"Title\"\n)\n\n# Calculate additional fit indices\nallFit <- enum_extract %>%\n  mutate(CAIC = -2 * LL + Parameters * (log(Observations) + 1)) %>%\n  mutate(AWE = -2 * LL + 2 * Parameters * (log(Observations) + 1.5)) %>%\n  mutate(SIC = -.5 * BIC) %>%\n  mutate(expSIC = exp(SIC - max(SIC))) %>%\n  mutate(BF = exp(SIC - lead(SIC))) %>%\n  mutate(cmPk = expSIC / sum(expSIC)) %>%\n  dplyr::select(1:5, 9:10, 6:7, 13, 14) %>%\n  arrange(Parameters)\n\n# Merge columns with LL replications and class size from `final_data`\nmerged_table <- allFit %>%\n  mutate(Title = str_trim(Title)) %>%\n  left_join(\n    final_data %>%\n      dplyr::select(\n        Class_Model,\n        Perc_Convergence,\n        Replicated_LL_Perc,\n        Smallest_Class,\n        Smallest_Class_Perc\n      ),\n    by = c(\"Title\" = \"Class_Model\")\n  ) %>%\n  mutate(Smallest_Class = coalesce(Smallest_Class, final_data$Smallest_Class[match(Title, final_data$Class_Model)])) %>%\n  relocate(Perc_Convergence, Replicated_LL_Perc, .after = LL) %>%\n  mutate(Smallest_Class_Combined = paste0(Smallest_Class, \"\\u00A0(\", Smallest_Class_Perc, \"%)\")) %>%\n  dplyr::select(-Smallest_Class, -Smallest_Class_Perc) %>%\n  dplyr::select(\n    Title,\n    Parameters,\n    LL,\n    Perc_Convergence,\n    Replicated_LL_Perc,\n    BIC,\n    aBIC,\n    CAIC,\n    AWE,\n    T11_VLMR_PValue,\n    BLRT_PValue,\n    Smallest_Class_Combined,\n    BF,\n    cmPk\n  )\nfit_table1 <- merged_table %>%\n  dplyr::select(\n    Title,\n    Parameters,\n    LL,\n    Perc_Convergence,\n    Replicated_LL_Perc,\n    BIC,\n    aBIC,\n    CAIC,\n    AWE,\n    T11_VLMR_PValue,\n    BLRT_PValue,\n    Smallest_Class_Combined\n  ) %>%\n  gt() %>%\n  tab_header(title = md(\"**Model Fit Summary Table**\")) %>%\n  tab_spanner(label = \"Model Fit Indices\", columns = c(BIC, aBIC, CAIC, AWE)) %>%\n  tab_spanner(label = \"LRTs\",\n              columns = c(T11_VLMR_PValue, BLRT_PValue)) %>%\n  tab_spanner(\n    label = md(\"Smallest\\u00A0Class\"),\n    columns = c(Smallest_Class_Combined)\n  ) %>%  # ✅ Non-Breaking Space\n  \n  cols_label(\n    Title = \"Classes\",\n    Parameters = md(\"npar\"),\n    LL = md(\"*LL*\"),\n    Perc_Convergence = \"% Converged\",\n    Replicated_LL_Perc = \"% Replicated\",\n    BIC = \"BIC\",\n    aBIC = \"aBIC\",\n    CAIC = \"CAIC\",\n    AWE = \"AWE\",\n    T11_VLMR_PValue = \"VLMR\",\n    BLRT_PValue = \"BLRT\",\n    Smallest_Class_Combined = \"n (%)\"\n  ) %>%\n  tab_footnote(\n    footnote = md(\n      \"*Note.* Par = Parameters; *LL* = model log likelihood;\n      BIC = Bayesian information criterion;\n      aBIC = sample size adjusted BIC; CAIC = consistent Akaike information criterion;\n      AWE = approximate weight of evidence criterion;\n      BLRT = bootstrapped likelihood ratio test p-value;\n      VLMR = Vuong-Lo-Mendell-Rubin adjusted likelihood ratio test p-value;\n      *cmPk* = approximate correct model probability;\n      Smallest K = Number of cases in the smallest class (n (%));\n      LL Replicated = Whether the best log-likelihood was replicated.\"\n    ),\n    locations = cells_title()\n  ) %>%\n  tab_options(column_labels.font.weight = \"bold\") %>%\n  fmt_number(columns = c(3, 6:9), decimals = 2) %>%\n  fmt(\n    columns = c(T11_VLMR_PValue, BLRT_PValue),\n    fns = function(x)\n      ifelse(is.na(x), \"—\", ifelse(\n        x < 0.001, \"<.001\", scales::number(x, accuracy = .01)\n      ))\n  ) %>%\n  fmt_percent(\n    columns = c(Perc_Convergence, Replicated_LL_Perc),\n    decimals = 0,\n    scale_values = FALSE\n  ) %>%\n  \n  cols_align(align = \"center\", columns = everything()) %>%\n  tab_style(\n    style = list(cell_text(weight = \"bold\")),\n    locations = list(\n      cells_body(columns = BIC, row = BIC == min(BIC)),\n      cells_body(columns = aBIC, row = aBIC == min(aBIC)),\n      cells_body(columns = CAIC, row = CAIC == min(CAIC)),\n      cells_body(columns = AWE, row = AWE == min(AWE)),\n      cells_body(\n        columns = T11_VLMR_PValue,\n        row = ifelse(\n          T11_VLMR_PValue < .05 &\n            lead(T11_VLMR_PValue) > .05,\n          T11_VLMR_PValue < .05,\n          NA\n        )\n      ),\n      cells_body(\n        columns = BLRT_PValue,\n        row = ifelse(BLRT_PValue < .05 &\n                       lead(BLRT_PValue) > .05, BLRT_PValue < .05, NA)\n      )\n    )\n  )\n\nfit_table1"},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"conditional-item-probability-plot","chapter":"17 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"17.6.2 Conditional Item Probability Plot","text":"","code":"\nsource(here(\"functions\", \"plot_lca.R\"))\n\nplot_lca(model_name = output_dasi$c4_dasi.out)"},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"adding-constraints-to-test-the-distal-means","chapter":"17 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"17.7 Adding constraints to test the distal means","text":"significance newly specified parameters (e.g., DJ1V2, DM1V2) indicates whether statistically significant differences latent classes. example, significant p-value DM1V2 suggests meaningful difference distal outcome math IRT score Latent Class 1 Latent Class 2.","code":"\ndasi_cons  <- mplusObject(\n  TITLE = \"D as I with constraints\", \n  VARIABLE = \n  \"categorical = enjoym-mathjob female;\n   usevar = enjoym-female;\n   classes = c(4);\",\n  \n  ANALYSIS = \n  \"estimator = mlr; \n    type = mixture;\n    starts = 200 100; \n    processors = 10;\",\n  \n  MODEL =\n    \" %overall%\n\n      %c#1%\n      [mathjob$1] (dj1);\n      [mathirt]   (dm1);\n      [female$1]  (df1);\n\n      %c#2%\n      [mathjob$1] (dj2);\n      [mathirt]   (dm2);\n      [female$1]  (df2);\n\n      %c#3%\n      [mathjob$1] (dj3);\n      [mathirt]   (dm3);\n      [female$1]  (df3);\n\n      %c#4%\n      [mathjob$1] (dj4);\n      [mathirt]   (dm4);\n      [female$1]  (df4);\",\n  \n  SAVEDATA = \n   \"File=3step_savedata.dat;\n    Save=cprob;\",\n  \n  OUTPUT = \"residual tech11 tech14\",\n  \n  MODELCONSTRAINT = \n  \"New (dj1v2 dj1v3 dj1v4 dj2v3 dj2v4 dj3v4\n       dm1v2 dm1v3 dm1v4 dm2v3 dm2v4 dm3v4\n       df1v2 df1v3 df1v4 df2v3 df2v4 df3v4);\n\n    dj1v2 = dj1-dj2;\n    dj1v3 = dj1-dj3;\n    dj1v4 = dj1-dj4;\n    dj2v3 = dj2-dj3;\n    dj2v4 = dj2-dj4;\n    dj3v4 = dj3-dj4;\n\n    dm1v2 = dm1-dm2;\n    dm1v3 = dm1-dm3;\n    dm1v4 = dm1-dm4;\n    dm2v3 = dm2-dm3;\n    dm2v4 = dm2-dm4;\n    dm3v4 = dm3-dm4;\n\n    df1v2 = df1-df2;\n    df1v3 = df1-df3;\n    df1v4 = df1-df4;\n    df2v3 = df2-df3;\n    df2v4 = df2-df4;\n    df3v4 = df3-df4;\",\n  \n  usevariables = colnames(lsay_df),\n  rdata = lsay_df)\n\ndasi_cons_fit <- mplusModeler(dasi_cons,\n                            dataout=here(\"distals\",\"constraints\",\"dasi_constraints.dat\"),\n                            modelout=here(\"distals\",\"constraints\",\"dasi_constraints.inp\"),\n                            check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"manual-3-step","chapter":"17 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"17.8 Manual 3-step","text":"","code":""},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"step-1-class-enumeration-w-auxiliary-specification","chapter":"17 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"17.8.1 Step 1 Class Enumeration w/ Auxiliary Specification","text":"first step involves identifying best-fitting unconditional model saving posterior probabilities modal class assignment model (.e., savedata: save=cprob;) specifying distal outcome variables auxiliary variables included new data file saved savedata command.step done class enumeration. example, four class model best. Therefore, re-estimating four-class model using optseed efficiency. optseed can found “RANDOM STARTS RESULTS RANKED BEST WORST LOGLIKELIHOOD VALUES” section. random start value yields best log-likelihood can used optseed, produce identical parameter estimates.SAVEDATA command, can save posterior probabilities modal class assignment steps two three.","code":"\nml_step1  <- mplusObject(\n  TITLE = \"Step 1 \", \n  VARIABLE = \n  \"categorical = enjoym-scaredm; \n   usevar = enjoym-scaredm;\n    \n   classes = c(4); \n    \n   auxiliary =   ! list all potential covariates and distals here\n   female        ! covariate\n   mathjob mathirt;      ! distal math test score in 9th grade \",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 0;\n    optseed = 484501;\",\n  \n  SAVEDATA = \n   \"File = 3step_savedata.dat;\n    Save = cprob;\",\n  \n  OUTPUT = \"residual tech11 tech14\",\n  \n  usevariables = colnames(lsay_df),\n  rdata = lsay_df)\n\nml_step1_fit <- mplusModeler(ml_step1,\n                            dataout=here(\"distals\",\"three_step\",\"ML_step1.dat\"),\n                            modelout=here(\"distals\",\"three_step\",\"ML_step1.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"step-2---determine-measurement-error-6","chapter":"17 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"17.8.2 Step 2 - Determine Measurement Error","text":"","code":"\noutput_lsay <- readModels(here(\"distals\",\"three_step\",\"ML_step1.out\"))\n\nlogit_cprobs <- as.data.frame(output_lsay[[\"class_counts\"]]\n                                       [[\"logitProbs.mostLikely\"]])\n\nsavedata_lsay <- as.data.frame(output_lsay[[\"savedata\"]])\n\ncolnames(savedata_lsay)[colnames(savedata_lsay)==\"C\"] <- \"N\""},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"step-3---add-auxiliary-variables-3","chapter":"17 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"17.8.3 Step 3 - Add Auxiliary Variables","text":"distal means outcome variables located ‘MODEL RESULTS’ section. excerpt presents distal means math job math IRT score Latent Class 1.","code":"\nML_step3  <- mplusObject(\n  TITLE = \"Step3 \", \n  \n  VARIABLE = \n \"nominal = N;\n  usevar = n;\n  \n  classes = c(4);\n  \n  usevar = female mathjob mathirt;\" ,\n  \n  ANALYSIS = \n \"estimator = mlr; \n  type = mixture; \n  starts = 100 50;\n  processors = 4;\",\n \n  DEFINE = \n   \"center female (grandmean);\",\n  \n  MODEL =\n  glue(\n \" %OVERALL%\n \n  mathirt on female; ! covariate as a predictor of the distal outcome\n  mathjob on female;\n  C on female;        ! covariate as predictor of C\n\n     %C#1%\n  [n#1@{logit_cprobs[1,1]}]; ! MUST EDIT if you do not have a 4-class model. \n  [n#2@{logit_cprobs[1,2]}];\n  [n#3@{logit_cprobs[1,3]}];\n  \n  [mathirt](m1);    ! conditional distal mean \n  mathirt;          ! conditional distal variance (freely estimated)\n  [mathjob](j1);\n  mathjob;\n\n  %C#2%\n  [n#1@{logit_cprobs[2,1]}];\n  [n#2@{logit_cprobs[2,2]}];\n  [n#3@{logit_cprobs[2,3]}];\n  \n  [mathirt](m2);\n  mathirt;\n  [mathjob](j2);\n  mathjob;\n  \n  %C#3%\n  [n#1@{logit_cprobs[3,1]}];\n  [n#2@{logit_cprobs[3,2]}];\n  [n#3@{logit_cprobs[3,3]}];\n  \n  [mathirt](m3);\n  mathirt;\n  [mathjob](j3);\n  mathjob;\n\n  %C#4%\n  [n#1@{logit_cprobs[4,1]}];\n  [n#2@{logit_cprobs[4,2]}];\n  [n#3@{logit_cprobs[4,3]}];\n  \n  [mathirt](m4);\n  mathirt; \n  [mathjob](j4);\n  mathjob;  \"),\n  \n  MODELCONSTRAINT = \n   \"New (dm1v2 dm1v3 dm2v3 dm1v4 dm2v4 dm3v4 \n         dj1v2 dj1v3 dj2v3 dj1v4 dj2v4 dj3v4 \n    );\n  \n    dm1v2 = m1-m2;  ! test pairwise distal mean differences of IRT score\n    dm1v3 = m1-m3;\n    dm2v3 = m2-m3;\n    dm1v4 = m1-m4;\n    dm2v4 = m2-m4;\n    dm3v4 = m3-m4;\n    dj1v2 = j1-j2;  ! test pairwise distal mean differences of math job\n    dj1v3 = j1-j3;\n    dj2v3 = j2-j3;\n    dj1v4 = j1-j4;\n    dj2v4 = j2-j4;\n    dj3v4 = j3-j4;\",\n  \n  MODELTEST = \"     ! omnibus test of distal means \n    m1=m2;\n    m2=m3;\n    m3=m4;\n    j1=j2;\n    j2=j3;\n    j3=j4;\",\n \n  usevariables = colnames(savedata_lsay), \n  rdata = savedata_lsay)\n\nstep3_fit <- mplusModeler(ML_step3,\n               dataout=here(\"distals\",\"three_step\",\"ML_step3.dat\"), \n               modelout=here(\"distals\",\"three_step\",\"ML_step3.inp\"), \n               check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"bch-approach","chapter":"17 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"17.9 BCH approach","text":"BCH method similar three-step approach except instead calculating average classification error second step, classification errors individual computed, inverse logits individual-level error rates used weights third step rather using modal class assignment imperfect latent class indicator. Advantages approach appears resistant shifts latent classes third step (problem manual 3-step Mplus) can often used irrespective variances equal unequal across latent classes (can problem LTB approach).","code":""},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"step-1---class-enumeration-w-auxiliary-specification-and-bch-weights","chapter":"17 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"17.9.1 Step 1 - Class Enumeration w/ Auxiliary Specification and BCH Weights","text":"","code":"\nstep1_bch  <- mplusObject(\n  TITLE = \"Step 1 - BCH Method\", \n  VARIABLE = \n  \"categorical = enjoym-scaredm; \n   usevar = enjoym-scaredm;\n    \n   classes = c(4); \n    \n   auxiliary =   ! list all potential covariates and distals here\n   female        ! covariate\n   mathjob mathirt;      ! distal math test score in 9th grade \",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 500 100;\",\n  \n  SAVEDATA = \n   \"File=3step_savedata.dat;\n    Save=bchweights; ! Here we save the BCH weights\n    format = free;\",\n\n  OUTPUT = \"sampstat residual tech11 tech14\",\n  \n  PLOT = \n    \"type = plot3; \n    series = enjoym-scaredm(*);\",\n  \n  usevariables = colnames(lsay_df),\n  rdata = lsay_df)\n\nstep1_fit_bch <- mplusModeler(step1_bch,\n                            dataout=here(\"distals\",\"three_step\",\"BCH_Step1.dat\"),\n                            modelout=here(\"distals\",\"three_step\",\"BCH_Step1.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"step-2---extract-bch-weights","chapter":"17 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"17.9.1.1 Step 2 - Extract BCH Weights","text":"Extract saved dataset part mplusObject “step1_fit_bch”Rename column savedata named “C” change “N”","code":"\noutput_bch <- readModels(here(\"distals\",\"three_step\",\"BCH_step1.out\"))\n\nsavedata_bch <- as.data.frame(output_bch[[\"savedata\"]])\ncolnames(savedata_bch)[colnames(savedata_bch)==\"C\"] <- \"N\""},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"step-3---add-auxiliary-variables-and-bch-weights","chapter":"17 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"17.9.1.2 Step 3 - Add Auxiliary Variables and BCH Weights","text":"distal means outcome variables located ‘MODEL RESULTS’ section. excerpt presents distal means math job math IRT score Latent Class 1.Note class numbering may consistent across different methods. Check class proportions make sure comparing class across methods. example, Latent Class 1 BCH results matches Latent Class 4 ML 3-step results..ML-3 step resultsBCH approach results","code":"\nstep3_bch  <- mplusObject(\n  TITLE = \"Step3 - BCH Method\", \n  \n  VARIABLE = \n \"classes = c(4);\n  \n  missing are all(9999);\n  \n  usevar = BCHW1-BCHW4 mathjob mathirt female;\n  \n  training = BCHW1-BCHW4(bch);\" ,\n  \n  ANALYSIS = \n \"estimator = mlr; \n  type = mixture; \n  starts = 500 200;\",\n  \n  MODEL =\n  glue(\n \" %OVERALL%\n  \n  mathirt on female; ! covariate as a predictor of the distal outcome\n  mathjob on female;\n  C on female;        ! covariate as predictor of C\n\n  %C#1%\n     \n  [mathirt](m1);    ! conditional distal mean \n  mathirt;          ! conditional distal variance (freely estimated)\n  [mathjob](j1);\n  mathjob;\n  \n  mathirt on female (rm1);\n  mathjob on female (rj1);\n\n  %C#2%\n  \n  [mathirt](m2);\n  mathirt;\n  [mathjob](j2);\n  mathjob;\n  \n  mathirt on female (rm2);\n  mathjob on female (rj2);\n  \n  %C#3%\n  \n  [mathirt](m3);\n  mathirt;\n  [mathjob](j3);\n  mathjob;\n  \n  mathirt on female (rm3);\n  mathjob on female (rj3);\n\n  %C#4%\n  \n  [mathirt](m4);\n  mathirt; \n  [mathjob](j4);\n  mathjob;\n \n   mathirt on female (rm4);\n   mathjob on female (rj4);\"),\n  \n  MODELCONSTRAINT = \n \n   \"New (dm1v2 dm1v3 dm2v3 dm1v4 dm2v4 dm3v4\n   dj1v2 dj1v3 dj2v3 dj1v4 dj2v4 dj3v4\n    );\n  \n    dm1v2 = m1-m2;  ! test pairwise distal mean differences of IRT score\n    dm1v3 = m1-m3;\n    dm2v3 = m2-m3;\n    dm1v4 = m1-m4;\n    dm2v4 = m2-m4;\n    dm3v4 = m3-m4;\n    dj1v2 = j1-j2;  ! test pairwise distal mean differences of math job\n    dj1v3 = j1-j3;\n    dj2v3 = j2-j3;\n    dj1v4 = j1-j4;\n    dj2v4 = j2-j4;\n    dj3v4 = j3-j4;\",\n  \n  MODELTEST = \"     ! omnibus test of distal means\n    m1=m2;\n    m2=m3;\n    m3=m4;\n    j1=j2;\n    j2=j3;\n    j3=j4;\n \",\n  \n  OUTPUT = \"Tech1 svalues sampstat\",\n \n  usevariables = colnames(savedata_bch), \n  rdata = savedata_bch)\n\nstep3_fit_bch <- mplusModeler(step3_bch,\n               dataout=here(\"distals\",\"three_step\",\"BCH_Step3.dat\"), \n               modelout=here(\"distals\",\"three_step\",\"BCH_Step3.inp\"), \n               check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"ten-frequently-asked-questions-nylund-gibson-7-choi-2018.html","id":"ten-frequently-asked-questions-nylund-gibson-7-choi-2018","chapter":"18 Ten Frequently Asked Questions (Nylund-Gibson 7 Choi, 2018)","heading":"18 Ten Frequently Asked Questions (Nylund-Gibson 7 Choi, 2018)","text":"","code":""},{"path":"ten-frequently-asked-questions-nylund-gibson-7-choi-2018.html","id":"example-positive-youth-development-inventory-analysis","chapter":"18 Ten Frequently Asked Questions (Nylund-Gibson 7 Choi, 2018)","heading":"18.1 Example: Positive Youth Development Inventory Analysis","text":"original paper illustrated modeling ideas described article using seven items (see Table 2) Positive Youth Development Inventory (PYDI) Contribution subscale (Arnold, Nott, & Meinhold, 2012) administered 1629 college university students.PYDI measures behavioral, psychological, social characteristics theorized indicate positive youth development, Contribution Subscale specifically measures degree youth express values behaviors associated channeling positive psychosocial strengths contribute meaningfully local community (Lerner et al., 2005).Citation: Nylund-Gibson, K., & Choi, . Y. (2018). Ten frequently asked questions latent class analysis. Translational Issues Psychological Science, 4(4), 440–461.","code":""},{"path":"ten-frequently-asked-questions-nylund-gibson-7-choi-2018.html","id":"load-packages-8","chapter":"18 Ten Frequently Asked Questions (Nylund-Gibson 7 Choi, 2018)","heading":"18.2 Load packages","text":"","code":"\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(glue)\nlibrary(MplusAutomation)\nlibrary(here)\nlibrary(janitor)\nlibrary(gt)\nlibrary(cowplot)\nlibrary(DiagrammeR) \nlibrary(webshot2)\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(readr)\nlibrary(flextable)\nlibrary(officer)\nlibrary(glue)\nlibrary(htmltools)"},{"path":"ten-frequently-asked-questions-nylund-gibson-7-choi-2018.html","id":"variable-description-4","chapter":"18 Ten Frequently Asked Questions (Nylund-Gibson 7 Choi, 2018)","heading":"18.3 Variable Description","text":"original research question examine whether construct positive contribution comprised qualitatively distinct subtypes among college university students, examine whether subtypes (indeed present) meaningfully associated demographic predictor wellbeing-related outcome.","code":""},{"path":"ten-frequently-asked-questions-nylund-gibson-7-choi-2018.html","id":"prepare-data-6","chapter":"18 Ten Frequently Asked Questions (Nylund-Gibson 7 Choi, 2018)","heading":"18.4 Prepare Data","text":"","code":"\ndf_qa <- read_csv(here(\"10faq\", \"data\", \"lca10faq.csv\"))"},{"path":"ten-frequently-asked-questions-nylund-gibson-7-choi-2018.html","id":"descriptive-statistics-15","chapter":"18 Ten Frequently Asked Questions (Nylund-Gibson 7 Choi, 2018)","heading":"18.5 Descriptive Statistics","text":"","code":"\n# Set up data to find proportions of binary indicators\nds <- df_qa %>% \n  pivot_longer(c(PYDI1Ab, PYDI2Ab, PYDI3Ab, PYDI4Ab, PYDI5Ab, PYDI6Ab, PYDI7Ab),\n               names_to = \"variable\") \n\n# Create table of variables and counts, then find proportions and round to 3 decimal places\nprop_df <- ds %>%\n  count(variable, value) %>%\n  group_by(variable) %>%\n  mutate(prop = n / sum(n)) %>%\n  ungroup() %>%\n  mutate(prop = round(prop, 3))\n\n\n# Make it a gt() table\nprop_table <- prop_df %>% \n  gt(groupname_col = \"variable\", rowname_col = \"value\") %>%\n  tab_stubhead(label = md(\"*Values*\")) %>%\n  tab_header(\n    md(\n      \"Variable Proportions\"\n    )\n  ) %>%\n  cols_label(\n    variable = md(\"*Variable*\"),\n    value = md(\"*Value*\"),\n    n = md(\"*N*\"),\n    prop = md(\"*Proportion*\")\n  ) \n  \nprop_table"},{"path":"ten-frequently-asked-questions-nylund-gibson-7-choi-2018.html","id":"enumeration-8","chapter":"18 Ten Frequently Asked Questions (Nylund-Gibson 7 Choi, 2018)","heading":"18.6 Enumeration","text":"code uses mplusObject function MplusAutomation package saves model runs enum folder.","code":"\nlca_6  <- lapply(1:6, function(k) {\n  lca_enum  <- mplusObject(\n      \n    TITLE = glue(\"{k}-Class\"), \n  \n    VARIABLE = glue(\n    \"categorical = PYDI1Ab PYDI2Ab PYDI3Ab PYDI4Ab PYDI5Ab PYDI6Ab PYDI7Ab; \n     MISSING ARE ALL (9999);\n     usevar = PYDI1Ab PYDI2Ab PYDI3Ab PYDI4Ab PYDI5Ab PYDI6Ab PYDI7Ab;\n     classes = c({k}); \"),\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 500 100; \n    processors = 10;\",\n  \n  OUTPUT = \"sampstat residual tech1 tech11 tech14;\",\n  \n  PLOT = \n    \"type = plot3; \n    series = PYDI1Ab PYDI2Ab PYDI3Ab PYDI4Ab PYDI5Ab PYDI6Ab PYDI7Ab(*);\",\n  \n  usevariables = colnames(df_qa),\n  rdata = df_qa)\n\nlca_enum_fit <- mplusModeler(lca_enum, \n                     dataout=glue(here(\"10faq\", \"enum\", \"lca10faq.dat\")),\n                     modelout=glue(here(\"10faq\", \"enum\", \"c{k}_lca10faq.inp\")),\n                     check=TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"ten-frequently-asked-questions-nylund-gibson-7-choi-2018.html","id":"examine-and-extract-mplus-files-1","chapter":"18 Ten Frequently Asked Questions (Nylund-Gibson 7 Choi, 2018)","heading":"18.6.1 Examine and Extract Mplus files","text":"Check models :WarningsErrorsConvergence Loglikelihood Replication Information","code":"\nsource(here(\"functions\", \"extract_mplus_info.R\"))\n\n# Define the directory where all of the .out files are located.\noutput_dir <- here(\"10faq\", \"enum\")\n\n# Get all .out files\noutput_files <- list.files(output_dir, pattern = \"\\\\.out$\", full.names = TRUE)\n\n# Process all .out files into one dataframe\nfinal_data <- map_dfr(output_files, extract_mplus_info_extended)\n\n# Extract Sample_Size from final_data\nsample_size <- unique(final_data$Sample_Size)"},{"path":"ten-frequently-asked-questions-nylund-gibson-7-choi-2018.html","id":"examine-mplus-warnings-2","chapter":"18 Ten Frequently Asked Questions (Nylund-Gibson 7 Choi, 2018)","heading":"18.6.1.1 Examine Mplus Warnings","text":"","code":"\nsource(here(\"functions\", \"extract_warnings.R\"))\n\nwarnings_table <- extract_warnings(final_data)\nwarnings_table"},{"path":"ten-frequently-asked-questions-nylund-gibson-7-choi-2018.html","id":"examine-mplus-errors-2","chapter":"18 Ten Frequently Asked Questions (Nylund-Gibson 7 Choi, 2018)","heading":"18.6.1.2 Examine Mplus Errors","text":"","code":"\nsource(here(\"functions\", \"error_visualization.R\"))\n\n# Process errors\nerror_table <- process_error_data(final_data)\nerror_table"},{"path":"ten-frequently-asked-questions-nylund-gibson-7-choi-2018.html","id":"examine-convergence-and-loglikelihood-replications-2","chapter":"18 Ten Frequently Asked Questions (Nylund-Gibson 7 Choi, 2018)","heading":"18.6.1.3 Examine Convergence and Loglikelihood Replications","text":"N = 1629Random StartsFinal starting value sets convergingLL ReplicationSmallest ClassModelBest LLnparInitialFinalf%f%f%1-Class-3,905.8927500100100100%100100.0%1,629100.0%2-Class-3,439.48315500100100100%100100.0%33120.3%3-Class-3,394.950235001005454%54100.0%905.5%4-Class-3,379.436315001005151%4180.4%885.4%5-Class-3,370.534395001004242%2764.3%352.1%6-Class-3,364.745475001003838%25.3%60.4%","code":"\nsource(here(\"functions\", \"summary_table.R\"))\n\n# **Print Table with Superheader & Heatmap**\nsummary_table <- create_flextable(final_data, sample_size)\nsummary_table"},{"path":"ten-frequently-asked-questions-nylund-gibson-7-choi-2018.html","id":"check-for-loglikelihood-replication-1","chapter":"18 Ten Frequently Asked Questions (Nylund-Gibson 7 Choi, 2018)","heading":"18.6.1.4 Check for Loglikelihood Replication","text":"Visualize examine loglikelihood replication values ouptut file individuallyVisualize examine loglikelihood replication output file together1-Class2-Class3-Class4-Class5-Class6-ClassLLN%LLN%LLN%LLN%LLN%LLN%-3905.892100100-3439.483100100-3394.9554100-3379.4364180.4-3370.5342764.3-3,364.74525.3—————————-3381.88812-3370.86624.8-3,365.674718.4—————————-3384.03917.6-3371.06812.4-3,365.675410.5————————————-3374.44712.4-3,366.00212.6————————————-3374.94112.4-3,366.02712.6————————————-3375.95712.4-3,366.13512.6————————————-3376.01512.4-3,366.31012.6————————————-3376.36512.4-3,366.31112.6————————————-3377.03912.4-3,366.51112.6————————————-3378.02512.4-3,366.94012.6————————————-3378.51624.8-3,367.16212.6————————————-3379.08812.4-3,367.24412.6————————————-3379.58612.4-3,367.67312.6————————————-3381.97412.4-3,367.81112.6———————————————-3,368.10312.6———————————————-3,368.16012.6———————————————-3,368.31612.6———————————————-3,368.32512.6———————————————-3,368.36925.3———————————————-3,368.448513.2———————————————-3,368.47712.6———————————————-3,369.88012.6———————————————-3,369.96112.6","code":"\n# Load the function for separate plots\nsource(here(\"functions\", \"ll_replication_plots.R\"))\n\n# Generate individual log-likelihood replication tables\nll_replication_tables<- generate_ll_replication_plots(final_data)\nll_replication_tables\nll_replication_table_all <- source(here(\"functions\", \"ll_replication_processing.R\"), local = TRUE)$value\nll_replication_table_all"},{"path":"ten-frequently-asked-questions-nylund-gibson-7-choi-2018.html","id":"table-of-fit-6","chapter":"18 Ten Frequently Asked Questions (Nylund-Gibson 7 Choi, 2018)","heading":"18.6.2 Table of Fit","text":"First, extract dataAdd Convergence percentage, LL Replication percentage, smallest class (%) ColumnsThen, create tablePrint Table","code":"\noutput_lca10faq <- readModels(here(\"10faq\", \"enum\"), filefilter = \"lca10faq\", quiet = TRUE)\n\nenum_extract <- LatexSummaryTable(output_lca10faq,\n    keepCols = c(\"Title\",\"Parameters\",\"LL\",\"BIC\",\"aBIC\",\n    \"BLRT_PValue\",\"T11_VLMR_PValue\",\"Observations\"),\n    sortBy = \"Title\") \n\nallFit <- enum_extract %>%\n  mutate(CAIC = -2 * LL + Parameters * (log(Observations) + 1)) %>%\n  mutate(AWE = -2 * LL + 2 * Parameters * (log(Observations) + 1.5)) %>%\n  mutate(SIC = -.5 * BIC) %>%\n  mutate(expSIC = exp(SIC - max(SIC))) %>%\n  mutate(BF = exp(SIC - lead(SIC))) %>%\n  mutate(cmPk = expSIC / sum(expSIC)) %>%\n  dplyr::select(1:5, 9:10, 6:7, 13, 14) %>%\n  arrange(Parameters)\nallFit <- allFit %>%\n  mutate(Title = str_trim(Title)) %>%\n  left_join(\n    final_data %>%\n      select(Class_Model, Perc_Convergence, Replicated_LL_Perc, \n             Smallest_Class, Smallest_Class_Perc),\n    by = c(\"Title\" = \"Class_Model\")\n  ) %>%\n  mutate(Smallest_Class = coalesce(Smallest_Class, \n                                   final_data$Smallest_Class[match(Title, final_data$Class_Model)])) %>%\n  relocate(Perc_Convergence, Replicated_LL_Perc, .after = LL) %>%\n  mutate(Smallest_Class_Combined = paste0(Smallest_Class, \"\\u00A0(\", Smallest_Class_Perc, \"%)\")) %>%  \n  select(-Smallest_Class, -Smallest_Class_Perc)  \n\nallFit <- allFit %>%\n  select(\n    Title, Parameters, LL,\n    Perc_Convergence, Replicated_LL_Perc,  \n    BIC, aBIC, CAIC, AWE, \n    T11_VLMR_PValue, BLRT_PValue, \n    Smallest_Class_Combined,  \n    BF, cmPk  \n  )\nfit_table1 <- allFit %>%\n  select(Title, Parameters, LL, Perc_Convergence, Replicated_LL_Perc, \n         BIC, aBIC, CAIC, AWE, \n         T11_VLMR_PValue, BLRT_PValue, \n         Smallest_Class_Combined  \n  ) %>%\n  gt() %>%\n  tab_header(title = md(\"**Model Fit Summary Table**\")) %>%\n  tab_spanner(label = \"Model Fit Indices\", columns = c(BIC, aBIC, CAIC, AWE)) %>%\n  tab_spanner(label = \"LRTs\", columns = c(T11_VLMR_PValue, BLRT_PValue)) %>%\n  tab_spanner(label = md(\"Smallest\\u00A0Class\"), columns = c(Smallest_Class_Combined)) %>%  \n  \n  cols_label(\n    Title = \"Classes\",\n    Parameters = md(\"npar\"),\n    LL = md(\"*LL*\"),\n    Perc_Convergence = \"% Converged\",\n    Replicated_LL_Perc = \"% Replicated\",\n    BIC = \"BIC\",\n    aBIC = \"aBIC\",\n    CAIC = \"CAIC\",\n    AWE = \"AWE\",\n    T11_VLMR_PValue = \"VLMR\",\n    BLRT_PValue = \"BLRT\",\n    Smallest_Class_Combined = \"n (%)\"\n  ) %>%\n  tab_footnote(\n    footnote = md(\n      \"*Note.* Par = Parameters; *LL* = model log likelihood;\n      BIC = Bayesian information criterion;\n      aBIC = sample size adjusted BIC; CAIC = consistent Akaike information criterion;\n      AWE = approximate weight of evidence criterion;\n      BLRT = bootstrapped likelihood ratio test p-value;\n      VLMR = Vuong-Lo-Mendell-Rubin adjusted likelihood ratio test p-value;\n      *cmPk* = approximate correct model probability;\n      Smallest K = Number of cases in the smallest class (n (%));\n      LL Replicated = Whether the best log-likelihood was replicated.\"\n    ),\n    locations = cells_title()\n  ) %>%\n  tab_options(column_labels.font.weight = \"bold\") %>%\n  fmt_number(\n    columns = c(3, 6:9),  \n    decimals = 2\n  ) %>%\n  fmt(\n    columns = c(T11_VLMR_PValue, BLRT_PValue),  \n    fns = function(x) ifelse(is.na(x), \"—\", ifelse(x < 0.001, \"<.001\", scales::number(x, accuracy = .01)))\n  ) %>%\n  fmt_percent(\n    columns = c(Perc_Convergence, Replicated_LL_Perc),\n    decimals = 0,\n    scale_values = FALSE  \n  ) %>%\n  \n  cols_align(align = \"center\", columns = everything()) %>% \n  tab_style(\n    style = list(cell_text(weight = \"bold\")),\n    locations = list(\n      cells_body(columns = BIC, row = BIC == min(BIC)),\n      cells_body(columns = aBIC, row = aBIC == min(aBIC)),\n      cells_body(columns = CAIC, row = CAIC == min(CAIC)),\n      cells_body(columns = AWE, row = AWE == min(AWE)),\n      cells_body(columns = T11_VLMR_PValue, \n                 row = ifelse(T11_VLMR_PValue < .05 & lead(T11_VLMR_PValue) > .05, T11_VLMR_PValue < .05, NA)),\n      cells_body(columns = BLRT_PValue, \n                 row = ifelse(BLRT_PValue < .05 & lead(BLRT_PValue) > .05, BLRT_PValue < .05, NA))\n    )\n  )\nif (knitr::is_latex_output()) {\n  cat(\"\\\\includegraphics[width=\\\\textwidth]{figures/fit_table1.png}\\n\")\n} else {\n  fit_table1  \n}"},{"path":"ten-frequently-asked-questions-nylund-gibson-7-choi-2018.html","id":"information-criteria-plot-6","chapter":"18 Ten Frequently Asked Questions (Nylund-Gibson 7 Choi, 2018)","heading":"18.6.3 Information Criteria Plot","text":"","code":"\n# Ensure CAIC exists and is numeric, if it exists\nallFit <- allFit %>%\n  mutate(CAIC = as.numeric(CAIC))  # Ensure CAIC is numeric\n\n# Ensure CAIC exists and is numeric, if it exists\nallFit <- allFit %>%\n  mutate(CAIC = as.numeric(CAIC))  # Ensure CAIC is numeric\n\n# Now, pivot the data\nallFit %>%\n  dplyr::select(Title, BIC, aBIC, CAIC, AWE) %>%\n  pivot_longer(\n    cols = c(BIC, aBIC, CAIC, AWE),  # Use these columns for pivoting\n    names_to = \"Index\", values_to = \"ic_value\"\n  ) %>%\n  mutate(\n    Index = factor(Index, levels = c(\"AWE\", \"CAIC\", \"BIC\", \"aBIC\"))  # Ensure proper ordering of Index\n  ) %>%\n  ggplot(aes(\n    x = Title,\n    y = ic_value,\n    color = Index,\n    shape = Index,\n    group = Index,\n    lty = Index\n  )) +\n  geom_point(size = 2.0) + \n  geom_line(size = .8) +\n  scale_x_discrete() +\n  scale_colour_grey(end = .5) +\n  theme_cowplot() +\n  labs(x = \"Number of Classes\", y = \"Information Criteria Value\", title = \"Information Criteria\") +\n  theme(\n    text = element_text(family = \"serif\", size = 12),  # Change font to Avenir Next\n    legend.text = element_text(family=\"serif\", size=12),\n    legend.key.width = unit(3, \"line\"),\n    legend.title = element_blank(),\n    legend.position = \"top\"  \n  )"},{"path":"ten-frequently-asked-questions-nylund-gibson-7-choi-2018.html","id":"compare-class-solutions-2","chapter":"18 Ten Frequently Asked Questions (Nylund-Gibson 7 Choi, 2018)","heading":"18.6.4 Compare Class Solutions","text":"Compare probability plots \\(K = 1:6\\) class solutionsModel Selection:Fit indices converge single solution, generally rule rather exception applied practice. ICs cmP suggested three-class solution, whereas likelihood tests supported four-class solution. Evaluating Figure 2, prominent “elbow” two-class model, whereas lowest point ICs three-class model. BF suggested three-, four-, five-class solutions plausible. Given BLRT specifically shown robust across diversity modeling conditions (Nylund, Asparouhov, & Muthén, 2007), tentatively selected four-class solution.Model Interpretation:figure illustrates conditional item probabilities selected 4-class solution, “conditional” refers likelihood endorsing item function class membership (e.g., probabilities “conditioned” class). model indicators labeled x axis whereas y axis presents metric item probabilities (0 1). four classes defined crisscrossing lines, preliminary labels listed bottom Figure 3 class prevalence (e.g., relative class sizes) parentheses. referred indicators Table 2 evaluating substantive meaning joint patterns item responses emerged within class. first largest class characterized high response probabilities indicators thus labeled Holistic–Collaborative class. Youth class likely value social contribution overall pursue pertinent activities cooperation others. second class labeled Altruistic–Low Selfefficacy likely comprised youth highly value social contribution yet believe effectiveness impactful engagement community. third class labeled Low Engagement given characteristically low ambivalent response probabilities model indicators. fourth class labeled Holistic–Independent given similarity Holistic–Collaborative class apart two indicators measuring interpersonal community engagement. Youth class likely value social contribution may prefer pursuing relevant activities independent introverted manner. Examining Figure 3, apparent two classes quite homogenous (thus well-separated) item responses except items 1 3, may diminish model classification statistics.","code":"\nmodel_results <- data.frame()\n\nfor (i in 1:length(output_lca10faq)) {\n  \n  temp <- output_lca10faq[[i]]$parameters$probability.scale %>%                                       \n    mutate(model = paste(i,\"-Class Model\"))                                                  \n  \n  model_results <- rbind(model_results, temp)\n}\n\nrm(temp)\n\ncompare_plot <-\n  model_results %>%\n  filter(category == 2) %>%\n  dplyr::select(est, model, LatentClass, param) %>%\n  mutate(param = as.factor(str_to_lower(param))) \n\ncompare_plot$param <- fct_inorder(compare_plot$param)\n\nggplot(\n  compare_plot,\n  aes(\n    x = param,\n    y = est,\n    color = LatentClass,\n    shape = LatentClass,\n    group = LatentClass,\n    lty = LatentClass\n  )\n) +\n  geom_point() + \n  geom_line() +\n  scale_colour_viridis_d() +\n  facet_wrap( ~ model, ncol = 2) +\n  labs(title = \"Bullying Items\",\n       x = \" \", y = \"Probability\") +\n  theme_minimal() +\n  theme(panel.grid.major.y = element_blank(),\n                          axis.text.x = element_text(angle = -45, hjust = -.1))        \nsource(here(\"functions\", \"plot_lca.R\"))\n\nplot_lca(model_name = output_lca10faq$c4_lca10faq.out)"},{"path":"ten-frequently-asked-questions-nylund-gibson-7-choi-2018.html","id":"including-auxilary-variables","chapter":"18 Ten Frequently Asked Questions (Nylund-Gibson 7 Choi, 2018)","heading":"18.7 Including Auxilary Variables","text":"","code":""},{"path":"ten-frequently-asked-questions-nylund-gibson-7-choi-2018.html","id":"step-1---class-enumeration-w-auxiliary-specification-3","chapter":"18 Ten Frequently Asked Questions (Nylund-Gibson 7 Choi, 2018)","heading":"18.7.1 Step 1 - Class Enumeration w/ Auxiliary Specification","text":"step done class enumeration (selected best latent class model). example, four class model best. Now, re-estimating four-class model using optseed efficiency. difference SAVEDATA command, can save posterior probabilities modal class assignment steps two three.Class 1: Holistic-Collaborative (65%)\nClass 2: Altruistic-Low Self-efficacy (8%)\nClass 3: Low Engagement (5%)\nClass 4: Holistic-Independent (22%)","code":"\nstep1  <- mplusObject(\n  TITLE = \"Step 1 - Three-Step\", \n  VARIABLE = \n  \"categorical = PYDI1Ab PYDI2Ab PYDI3Ab PYDI4Ab PYDI5Ab PYDI6Ab PYDI7Ab; \n   usevar = PYDI1Ab PYDI2Ab PYDI3Ab PYDI4Ab PYDI5Ab PYDI6Ab PYDI7Ab;\n  MISSING ARE ALL (9999);\n   classes = c(4); \n    \n   auxiliary =   ! list all potential covariates and distals here\n   hispanic        ! covariate\n   LifeSatA;      ! distal \",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 0;\n    optseed = 371246;\",\n  \n  SAVEDATA = \n   \"File=3step_savedata.dat;\n    Save=cprob;\",\n  \n  OUTPUT = \"residual tech11 tech14 svalues(2 4 3 1);\",\n  \n  PLOT = \n    \"type = plot3; \n    series = PYDI1Ab PYDI2Ab PYDI3Ab PYDI4Ab PYDI5Ab PYDI6Ab PYDI7Ab(*);\",\n  \n  usevariables = colnames(df_qa),\n  rdata = df_qa)\n\nstep1_fit <- mplusModeler(step1,\n                            dataout=here(\"10faq\", \"manual_3step\", \"Step1.dat\"),\n                            modelout=here(\"10faq\", \"manual_3step\", \"one.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\nsource(here(\"functions\", \"plot_lca.R\"))\noutput_lsay <- readModels(here(\"10faq\", \"manual_3step\",\"one.out\"))\n\nplot_lca(model_name = output_lsay)"},{"path":"ten-frequently-asked-questions-nylund-gibson-7-choi-2018.html","id":"step-2---determine-measurement-error-7","chapter":"18 Ten Frequently Asked Questions (Nylund-Gibson 7 Choi, 2018)","heading":"18.7.2 Step 2 - Determine Measurement Error","text":"Extract logits classification probabilities likely latent classExtract saved dataset part mplusObject “step1_fit”Rename column savedata named “C” change “N”","code":"\nlogit_cprobs <- as.data.frame(output_lsay[[\"class_counts\"]]\n                                       [[\"logitProbs.mostLikely\"]])\nsavedata <- as.data.frame(output_lsay[[\"savedata\"]])\ncolnames(savedata)[colnames(savedata)==\"C\"] <- \"N\""},{"path":"ten-frequently-asked-questions-nylund-gibson-7-choi-2018.html","id":"step-3---add-auxiliary-variables-4","chapter":"18 Ten Frequently Asked Questions (Nylund-Gibson 7 Choi, 2018)","heading":"18.7.3 Step 3 - Add Auxiliary Variables","text":"Model 1 covariate 1 distal outcome","code":"\nstep3  <- mplusObject(\n  TITLE = \"Step3 - 3step LSAY\", \n  \n  VARIABLE = \n \"nominal=N;\n  usevar = n;\n  \n  classes = c(4);\n  \n  usevar = hispanic LifeSatA;\" ,\n  \n  ANALYSIS = \n \"estimator = mlr; \n  type = mixture; \n  starts = 0;\",\n \n  DEFINE = \n   \"center hispanic (grandmean);\",\n  \n  MODEL =\n  glue(\n \" %OVERALL%\n \n  LifeSatA on hispanic; ! covariate as a predictor of the distal outcome\n  C on hispanic;        ! covariate as predictor of C\n\n     %C#1%\n  [n#1@{logit_cprobs[1,1]}]; ! MUST EDIT if you do not have a 4-class model. \n  [n#2@{logit_cprobs[1,2]}];\n  [n#3@{logit_cprobs[1,3]}];\n  \n  [LifeSatA](m1);    ! conditional distal mean \n  LifeSatA;          ! conditional distal variance (freely estimated)\n\n  %C#2%\n  [n#1@{logit_cprobs[2,1]}];\n  [n#2@{logit_cprobs[2,2]}];\n  [n#3@{logit_cprobs[2,3]}];\n  \n  [LifeSatA](m2);\n  LifeSatA;\n  \n  %C#3%\n  [n#1@{logit_cprobs[3,1]}];\n  [n#2@{logit_cprobs[3,2]}];\n  [n#3@{logit_cprobs[3,3]}];\n  \n  [LifeSatA](m3);\n  LifeSatA;\n\n  %C#4%\n  [n#1@{logit_cprobs[4,1]}];\n  [n#2@{logit_cprobs[4,2]}];\n  [n#3@{logit_cprobs[4,3]}];\n  \n  [LifeSatA](m4);\n  LifeSatA; \"),\n  \n  MODELCONSTRAINT = \n   \"New (diff12 diff13 diff23 \n    diff14 diff24 diff34);\n  \n    diff12 = m1-m2;  ! test pairwise distal mean differences\n    diff13 = m1-m3;\n    diff23 = m2-m3;\n    diff14 = m1-m4;\n    diff24 = m2-m4;\n    diff34 = m3-m4;\",\n  \n  MODELTEST = \"     ! omnibus test of distal means \n    m1=m2;\n    m2=m3;\n    m3=m4;\",\n \n  usevariables = colnames(savedata), \n  rdata = savedata)\n\nstep3_fit <- mplusModeler(step3,\n               dataout=here(\"10faq\", \"manual_3step\", \"Step3.dat\"), \n               modelout=here(\"10faq\", \"manual_3step\", \"three_starts.inp\"), \n               check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"ten-frequently-asked-questions-nylund-gibson-7-choi-2018.html","id":"wald-test-table-3","chapter":"18 Ten Frequently Asked Questions (Nylund-Gibson 7 Choi, 2018)","heading":"18.7.3.1 Wald Test Table","text":"","code":"\nmodelParams <- readModels(here(\"10faq\", \"manual_3step\", \"three_starts.out\"))\n\n# Extract information as data frame\nwald <- as.data.frame(modelParams[[\"summaries\"]]) %>%\n  dplyr::select(WaldChiSq_Value:WaldChiSq_PValue) %>% \n  mutate(WaldChiSq_DF = paste0(\"(\", WaldChiSq_DF, \")\")) %>% \n  unite(wald_test, WaldChiSq_Value, WaldChiSq_DF, sep = \" \") %>% \n  rename(pval = WaldChiSq_PValue) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001))))\n\n# Create table\n\nwald_table <- wald %>% \n  gt() %>%\n    tab_header(\n    title = \"Wald Test of Paramter Constraints (Math)\") %>%\n    cols_label(\n      wald_test = md(\"Wald Test (*df*)\"),\n      pval = md(\"*p*-value\")) %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\")\n\nwald_table"},{"path":"ten-frequently-asked-questions-nylund-gibson-7-choi-2018.html","id":"table-of-distal-outcome-differences-1","chapter":"18 Ten Frequently Asked Questions (Nylund-Gibson 7 Choi, 2018)","heading":"18.7.3.2 Table of Distal Outcome Differences","text":"","code":"\n# Extract information as data frame\ndiff <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n  filter(grepl(\"DIFF\", param)) %>% \n  dplyr::select(param:pval) %>% \n  mutate(se = paste0(\"(\", format(round(se,2), nsmall =2), \")\")) %>% \n  unite(estimate, est, se, sep = \" \") %>% \n  mutate(param = str_remove(param, \"DIFF\"),\n         param = as.numeric(param)) %>% \n  separate(param, into = paste0(\"Group\", 1:2), sep = 1) %>% \n  mutate(class = paste0(\"Class \", Group1, \" vs \", Group2)) %>% \n  select(class, estimate, pval) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001))))\n\n# Create table\n\ndiff %>% \n  gt() %>%\n    tab_header(\n    title = \"Distal Outcome Differences\") %>%\n    cols_label(\n      class = \"Class\",\n      estimate = md(\"Mean (*se*)\"),\n      pval = md(\"*p*-value\")) %>% \n    sub_missing(1:3,\n              missing_text = \"\") %>%\n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\")"},{"path":"ten-frequently-asked-questions-nylund-gibson-7-choi-2018.html","id":"covariate-relations","chapter":"18 Ten Frequently Asked Questions (Nylund-Gibson 7 Choi, 2018)","heading":"18.7.3.3 Covariate Relations","text":"","code":"\n# Extract information as data frame\ncov <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n   filter(str_detect(paramHeader, \"^C#\\\\d+\\\\.ON$\")) %>% \n  mutate(param = str_replace(param, \"HISPANIC\", \"Hispanic\")) %>%\n  mutate(est = format(round(est, 3), nsmall = 3),\n         se = round(se, 2),\n         pval = round(pval, 3)) %>% \n  mutate(latent_class = str_replace(paramHeader, \"^C#(\\\\d+)\\\\.ON$\", \"Class \\\\1\")) %>% \n  dplyr::select(param, est, se, pval, latent_class) %>% \n  mutate(se = paste0(\"(\", format(round(se,2), nsmall =2), \")\")) %>% \n  unite(logit, est, se, sep = \" \") %>% \n  dplyr::select(param, logit, pval, latent_class) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001)))) \n\nor <- as.data.frame(modelParams[[\"parameters\"]][[\"odds\"]]) %>%\n  filter(str_detect(paramHeader, \"^C#\\\\d+\\\\.ON$\")) %>% \n  mutate(param = str_replace(param, \"HISPANIC\", \"Hispanic\")) %>%\n  mutate(est = format(round(est, 3), nsmall = 3)) %>% \n  mutate(latent_class = str_replace(paramHeader, \"^C#(\\\\d+)\\\\.ON$\", \"Class \\\\1\")) %>% \n  mutate(CI = paste0(\"[\", format(round(lower_2.5ci, 3), nsmall = 3), \", \", format(round(upper_2.5ci, 3), nsmall = 3), \"]\")) %>% \n  dplyr::select(param, est, CI, latent_class) %>% \n  rename(or = est)\n  \ncombined <- or %>% \n  full_join(cov) %>% \n  dplyr::select(param, latent_class, logit, pval, or, CI)\n\n\n# Create table\n\ncombined %>% \n  gt(groupname_col = \"latent_class\", rowname_col = \"param\") %>%\n  tab_header(\n    title = \"Predictors of Class Membership\") %>%\n  cols_label(\n    logit = md(\"Logit (*se*)\"),\n    or = md(\"Odds Ratio\"),\n    CI = md(\"95% CI\"),\n    pval = md(\"*p*-value\")) %>% \n  sub_missing(1:3,\n              missing_text = \"\") %>%\n  sub_values(values = c(\"999.000\"), replacement = \"-\") %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\") %>%   \n  tab_footnote(\n    footnote = \"Reference Class: 4\",\n    locations = cells_title(groups = \"title\")\n  )"},{"path":"ten-frequently-asked-questions-nylund-gibson-7-choi-2018.html","id":"distal-outcome-regressed-on-covariate","chapter":"18 Ten Frequently Asked Questions (Nylund-Gibson 7 Choi, 2018)","heading":"18.7.3.4 Distal Outcome Regressed on Covariate","text":"Following three-step procedure, logit values classification probabilities four-class solution first step copied used third step. used values second set analyses fix measurement parameters latent classes, auxiliary variables relations estimated thereafter (see Appendix B). used multinomial logistic regression evaluate whether relative proportions Hispanic non-Hispanic youth equal across four classes, results reported Table 5. Notably, Hispanic youth (versus nonHispanic) likely Holistic–Collaborative class compared Altruistic–Low Self-efficacy class (2.71, p .001) Holistic Independent class (1.62, p .009). covariate-class relations statistically significant. Simultaneously, estimated classspecific means life satisfaction four classes. interpretation distal outcomes, centered covariate distal outcome mean differences across latent classes accounted relative proportion Hispanic versus non-Hispanic youth entire sample. Pairwise Wald tests revealed life satisfaction Holistic-Collaborative class significantly higher compared three classes. distal mean comparisons statistically significant.","code":"\ndonx <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n  filter(param %in% c(\"HISPANIC\")) %>% \n  mutate(param = str_replace(param, \"HISPANIC\", \"Hispanic\")) %>% \n  mutate(LatentClass = sub(\"^\",\"Class \", LatentClass)) %>%  \n  dplyr::select(!paramHeader) %>% \n  mutate(se = paste0(\"(\", format(round(se,2), nsmall =2), \")\")) %>% \n    unite(estimate, est, se, sep = \" \") %>% \n  dplyr::select(param, estimate, pval) %>% \n  distinct(param, .keep_all=TRUE) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001))))\n\n\n# Create table\n\ndonx %>% \n  gt(groupname_col = \"LatentClass\", rowname_col = \"param\") %>%\n  tab_header(\n    title = \"Race (Hispanic) Predicting Life Satisfaction\") %>%\n  cols_label(\n    estimate = md(\"Estimate (*se*)\"),\n    pval = md(\"*p*-value\")) %>% \n  sub_missing(1:3,\n              missing_text = \"\") %>%\n  sub_values(values = c(\"999.000\"), replacement = \"-\") %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\")"},{"path":"ten-frequently-asked-questions-nylund-gibson-7-choi-2018.html","id":"references","chapter":"18 Ten Frequently Asked Questions (Nylund-Gibson 7 Choi, 2018)","heading":"18.8 References","text":"Arnold, M. E., Nott, B. D., & Meinhold, J. L. (2012). Positive Youth Development Inventory full version. Corvallis: Oregon State University.Lerner, R. M., Lerner, J. V., Almerigi, J. B., Theokas, C., Phelps, E., Gestsdottir, S., . . . von Eye, . (2005). Positive youth development, participation community youth development programs, community contributions fifth-grade adolescents: Findings first wave 4-H Study Positive Youth Development. Journal Early Adolescence, 25, 17–71. http://dx.doi .org/10.1177/0272431604272461Nylund, K. L., Asparouhov, T., & Muthén, B. O. (2007). Deciding number classes latent class analysis growth mixture modeling: Monte Carlo simulation study. Structural Equation Modeling, 14, 535–569. http://dx.doi.org/10.1080/ 10705510701575396","code":""},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","text":"document presents replication latent transition analysis (LTA) conducted Ing Nylund-Gibson, (2017), investigated students’ attitudes toward mathematics science evolve time attitudinal trajectories relate later academic outcomes. Using nationally representative longitudinal data LSAY study, follow students Grade 7 Grade 12, classifying latent attitudinal profiles wave modeling transitions profiles across time. replication reproduces authors’ manual 3-step LTA approach, including: (1) estimating invariant unconditional model identify latent profiles across grades; (2) using fixed logits classify students profiles wave; (3) examining class membership transitions relate distal outcomes demographic covariates. applicable, extend original analysis visualizing transition patterns, computing subgroup differences using z-tests, formatting outputs clear interpretability.Citation: Ing,M., & Nylund-Gibson, K. (2017). importance early attitudes toward mathematics science. Teachers College Record: Voice Scholarship Education, 119(5), 1-32.","code":""},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"load-packages-9","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.1 Load Packages","text":"","code":"\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(glue)\nlibrary(MplusAutomation)\nlibrary(here)\nlibrary(janitor)\nlibrary(gt)\nlibrary(cowplot)\nlibrary(DiagrammeR) \nlibrary(webshot2)\nlibrary(stringr)\nlibrary(flextable)\nlibrary(officer)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(haven)\nlibrary(psych)\nlibrary(ggrepel)\nlibrary(PNWColors)\nlibrary(multcompView)"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"prepare-data-7","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.2 Prepare Data","text":"","code":"\nlsay_data <- read_sav(here(\"tc_lta\", \"data\", \"Dataset_Jul9_Mplus.sav\"))\n\n# Filter to follow-up sample (target ~1824 rows)\nlsay_data <- lsay_data %>% filter(RSTEMM %in% c(0, 1))\n\n# Define survey questions\nall_questions <- c(\n  \"AB39A\", \"AB39H\", \"AB39I\", \"AB39K\", \"AB39L\", \"AB39M\", \"AB39T\", \"AB39U\", \"AB39W\", \"AB39X\", # 7th grade\n  \"GA32A\", \"GA32H\", \"GA32I\", \"GA32K\", \"GA32L\", \"GA33A\", \"GA33H\", \"GA33I\", \"GA33K\", \"GA33L\", # 10th grade\n  \"KA46A\", \"KA46H\", \"KA46I\", \"KA46K\", \"KA46L\", \"KA47A\", \"KA47H\", \"KA47I\", \"KA47K\", \"KA47L\"  # 12th grade\n)\n\n# Rename variables >8 characters\nlsay_data <- lsay_data %>%\n  rename(\n    SCIG8 = ScienceG8,\n    SCIG11 = ScienceG11\n  )\n\n# Recode 9999 to NA (missing)\nlsay_data <- lsay_data %>%\n  mutate(across(all_of(all_questions), ~if_else(. == 9999, NA_real_, .)))\n\nnames(lsay_data) <- toupper(names(lsay_data))"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"descriptive-statistics-16","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.3 Descriptive Statistics","text":"","code":""},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"create-table-1","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.3.1 Create Table 1","text":"table provides overall mean standard deviation attitudinal item Grades 7, 10, 12. values summarize general trends full sample serve foundation latent class models follow.Save Table 1","code":"\n# Function to compute stats (count, mean, SD)\ncompute_stats <- function(data, question, grade, question_name) {\n  data %>%\n    summarise(\n      Grade = grade,\n      Count = sum(!is.na(.data[[question]])),\n      Mean = mean(.data[[question]], na.rm = TRUE),\n      SD = sd(.data[[question]], na.rm = TRUE)\n    ) %>%\n    mutate(Question = question_name)\n}\n\n# Define question names and mappings\ntable_setup <- tibble(\n  question_code = all_questions,\n  grade = rep(c(7, 10, 12), each = 10),\n  question_name = rep(\n    c(\n      \"I enjoy math\",\n      \"Math is useful in everyday problems\",\n      \"Math helps a person think logically\",\n      \"It is important to know math to get a good job\",\n      \"I will use math in many ways as an adult\",\n      \"I enjoy science\",\n      \"Science is useful in everyday problems\",\n      \"Science helps a person think logically\",\n      \"It is important to know science to get a good job\",\n      \"I will use science in many ways as an adult\"\n    ),\n    times = 3\n  )\n)\n\n# Compute stats for all questions\ntable1_data <- pmap_dfr(\n  list(table_setup$question_code, table_setup$grade, table_setup$question_name),\n  ~compute_stats(lsay_data, ..1, ..2, ..3)\n) %>%\n  mutate(\n    Mean = round(Mean, 2),\n    SD = round(SD, 2)\n  ) %>%\n  arrange(match(Question, table_setup$question_name), Grade) %>%\n  select(Question, Grade, Count, Mean, SD)\n\n# Build table\ntable1_gt <- table1_data %>%\n  gt(groupname_col = \"Question\") %>%\n  tab_header(\n    title = \"Table 1. Descriptive Statistics for Mathematics and Science Attitudinal Survey Items Included In Analyses\"\n  ) %>%\n  cols_label(\n    Grade = \"Grade\",\n    Count = \"N\",\n    Mean = \"M\",\n    SD = \"SD\"\n  ) %>%\n  fmt_number(\n    columns = c(Mean, SD),\n    decimals = 2\n  )\n\n# Show table\ntable1_gt\n# Save table as PNG\ngtsave(table1_gt, here(\"figures\", \"table1.png\"))"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"create-table-2","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.3.2 Create Table 2","text":"table presents model fit statistics latent profile enumeration Grades 7, 10, 12. grade, models 1 6 profiles estimated evaluated using commonly used information criteria (AIC, BIC, SSA-BIC), entropy, likelihood ratio tests (LMR, BLRT). values help determine appropriate number latent profiles retain grade.Save Table 2","code":"\n# Coerce to numeric just in case\nlsay_data <- lsay_data %>%\n  mutate(\n    MATHG12 = as.numeric(MATHG12),\n    SCIG11  = as.numeric(SCIG11),\n    STEMSUP = as.numeric(STEMSUP)\n  )\n\n# Build the summary table\ntable2_data <- tibble::tibble(\n  `Outcome Variable` = c(\n    \"12th Grade Mathematics Achievement\",\n    \"11th Grade Science Achievement\",\n    \"STEM Career Attainmentᵃ\"\n  ),\n  N = c(\n    sum(lsay_data$MATHG12 != 9999 & !is.na(lsay_data$MATHG12)),\n    sum(lsay_data$SCIG11  != 9999 & !is.na(lsay_data$SCIG11)),\n    sum(lsay_data$STEMSUP != 9999 & !is.na(lsay_data$STEMSUP))\n  ),\n  M = c(\n    round(mean(lsay_data$MATHG12[lsay_data$MATHG12 != 9999 & !is.na(lsay_data$MATHG12)]), 2),\n    round(mean(lsay_data$SCIG11[lsay_data$SCIG11 != 9999 & !is.na(lsay_data$SCIG11)]), 2),\n    round(mean(lsay_data$STEMSUP[lsay_data$STEMSUP != 9999 & !is.na(lsay_data$STEMSUP)]), 2)\n  ),\n  SD = c(\n    round(sd(lsay_data$MATHG12[lsay_data$MATHG12 != 9999 & !is.na(lsay_data$MATHG12)]), 2),\n    round(sd(lsay_data$SCIG11[lsay_data$SCIG11 != 9999 & !is.na(lsay_data$SCIG11)]), 2),\n    NA\n  )\n)\n\n# Render the gt table\ntable2_gt <- table2_data %>%\n  gt() %>%\n  tab_header(\n    title = md(\"**Table 2. Descriptive Statistics for Distal Outcome Variables**\")\n  ) %>%\n  cols_label(\n    `Outcome Variable` = \"Outcome Variable\",\n    N = \"N\",\n    M = \"M\",\n    SD = \"SD\"\n  ) %>%\n  sub_missing(columns = everything(), missing_text = \"\") %>%\n  tab_footnote(\n    footnote = \"Binary indicator coded 1 = STEM occupation in mid-30s follow-up.\",\n    locations = cells_body(rows = 3, columns = \"Outcome Variable\")\n  )\n\ntable2_gt\ngtsave(\n  data = table2_gt,\n  filename = here(\"tc_lta\",\"figures\", \"table2_distal_outcomes.png\")\n)"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"run-independent-lcas-for-each-timepoint","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.4 Run Independent LCAs for Each Timepoint","text":"","code":""},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"prepare-data-for-mplusautomation","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.4.1 Prepare Data for MPlusAutomation","text":"determine appropriate number structure attitudinal profiles timepoint, first estimate separate latent class models Grades 7, 10, 12. step involves preparing data wide format running independent LCAs using MplusAutomation. goal identify optimal number latent profiles per grade based model fit moving longitudinal transition model.","code":""},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"run-lca-on-7th-grade-items","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.4.2 Run LCA on 7th Grade Items","text":"","code":"\n\nlca_belonging <- lapply(1:8, function(k) {\n  lca_enum <- mplusObject(\n    TITLE = glue(\"{k}-Class LCA for LSAY 7th Grade\"),\n\n    VARIABLE = glue(\n      \"categorical = AB39A AB39H AB39I AB39K AB39L AB39M AB39T AB39U AB39W AB39X;\n       usevar = AB39A AB39H AB39I AB39K AB39L AB39M AB39T AB39U AB39W AB39X;\n       missing = all(9999);\n       classes = c({k});\"\n    ),\n\n    ANALYSIS = \"\n      estimator = mlr;\n      type = mixture;\n      starts = 500 10;\n      processors = 10;\",\n\n    OUTPUT = \"sampstat; residual; tech11; tech14;\",\n\n    PLOT = \"\n      type = plot3;\n      series = AB39A AB39H AB39I AB39K AB39L AB39M \n               AB39T AB39U AB39W AB39X(*);\",\n\n    rdata = lsay_data\n  )\n\n  lca_enum_fit <- mplusModeler(\n    lca_enum,\n    dataout = glue(here(\"tc_lta\",\"g7_enum\", \"lsay_g7.dat\")),\n    modelout = glue(here(\"tc_lta\",\"g7_enum\", \"c{k}_g7.inp\")),\n    check = TRUE,\n    run = TRUE,\n    hashfilename = FALSE\n  )\n})"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"run-lca-on-10th-grade","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.4.3 Run LCA on 10th Grade","text":"","code":"\n\nlca_belonging <- lapply(1:8, function(k) {\n  lca_enum <- mplusObject(\n    TITLE = glue(\"{k}-Class LCA for LCA 10th Grade\"),\n\n    VARIABLE = glue(\n      \"categorical = GA32A GA32H GA32I GA32K GA32L GA33A GA33H GA33I GA33K GA33L;\n       usevar = GA32A GA32H GA32I GA32K GA32L GA33A GA33H GA33I GA33K GA33L;\n       missing = all(9999);\n       classes = c({k});\"\n    ),\n\n    ANALYSIS = \"\n      estimator = mlr;\n      type = mixture;\n      starts = 500 10;\n      processors = 10;\",\n\n    OUTPUT = \"sampstat; residual; tech11; tech14;\",\n\n    PLOT = \"\n      type = plot3;\n      series = GA32A GA32H GA32I GA32K GA32L GA33A \n               GA33H GA33I GA33K GA33L (*);\",\n\n    rdata = lsay_data\n  )\n\n  lca_enum_fit <- mplusModeler(\n    lca_enum,\n    dataout = glue(here(\"tc_lta\",\"g10_enum\", \"lsay_g10.dat\")),\n    modelout = glue(here(\"tc_lta\",\"g10_enum\", \"c{k}_g10.inp\")),\n    check = TRUE,\n    run = TRUE,\n    hashfilename = FALSE\n  )\n})"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"run-lca-for-12th-grade","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.4.4 Run LCA for 12th Grade","text":"Extract Mplus Information","code":"\n\nlca_belonging <- lapply(1:8, function(k) {\n  lca_enum <- mplusObject(\n    TITLE = glue(\"{k}-Class LCA for LCA 12th Grade\"),\n\n    VARIABLE = glue(\n      \"categorical = KA46A KA46H KA46I KA46K KA46L KA47A KA47H KA47I KA47K KA47L;\n       usevar = KA46A KA46H KA46I KA46K KA46L KA47A KA47H KA47I KA47K KA47L;\n       missing = all(9999);\n       classes = c({k});\"\n    ),\n\n    ANALYSIS = \"\n      estimator = mlr;\n      type = mixture;\n      starts = 500 10;\n      processors = 10;\",\n\n    OUTPUT = \"sampstat; residual; tech11; tech14;\",\n\n    PLOT = \"\n      type = plot3;\n      series = KA46A KA46H KA46I KA46K KA46L KA47A \n               KA47H KA47I KA47K KA47L (*);\",\n\n    rdata = lsay_data\n  )\n\n  lca_enum_fit <- mplusModeler(\n    lca_enum,\n    dataout = glue(here(\"tc_lta\",\"g12_enum\", \"lsay_g12.dat\")),\n    modelout = glue(here(\"tc_lta\",\"g12_enum\", \"c{k}_g12.inp\")),\n    check = TRUE,\n    run = TRUE,\n    hashfilename = FALSE\n  )\n})\n# LCA Extraction\nsource(here(\"tc_lta\",\"functions\", \"extract_mplus_info.R\"))\noutput_dir_lca <- here(\"tc_lta\",\"enum\")\noutput_files_lca <- list.files(output_dir_lca, pattern = \"\\\\.out$\", full.names = TRUE)\nfinal_data_lca <- map_dfr(output_files_lca, extract_mplus_info_extended) %>% \n  mutate(Model_Type = \"LCA\")"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"screen-output-for-warnings-errors-and-loglikelihood-replication","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.5 Screen Output for Warnings, Errors, and Loglikelihood Replication","text":"estimating LCA model, examine Mplus output files warnings, estimation errors, loglikelihood replication issues. quality check helps ensure solutions trustworthy selected models based local maxima convergence failures. step, flag estimation concerns verify best loglikelihood value replicated consistently across random starts.Extract Warnings Output Files","code":"\nsource(here(\"tc_lta\",\"functions\", \"extract_mplus_info.R\"))\n\n# Extract 7th grade LCAs\noutput_dir_g7 <- here(\"tc_lta\",\"g7_enum\")\noutput_files_g7 <- list.files(output_dir_g7, pattern = \"\\\\.out$\", full.names = TRUE)\n\nfinal_data_g7 <- map_dfr(output_files_g7, extract_mplus_info_extended) %>%\n  mutate(Model_Type = \"LCA\", Grade = \"7th\")\n\n\n# Extract 10th grade LCAs\noutput_dir_g10 <- here(\"tc_lta\",\"g10_enum\")\noutput_files_g10 <- list.files(output_dir_g10, pattern = \"\\\\.out$\", full.names = TRUE)\n\nfinal_data_g10 <- map_dfr(output_files_g10, extract_mplus_info_extended) %>%\n  mutate(Model_Type = \"LCA\", Grade = \"10th\")\n\n\n# Extract 12th grade LCAs\noutput_dir_g12 <- here(\"tc_lta\",\"g12_enum\")\noutput_files_g12 <- list.files(output_dir_g12, pattern = \"\\\\.out$\", full.names = TRUE)\n\nfinal_data_g12 <- map_dfr(output_files_g12, extract_mplus_info_extended) %>%\n  mutate(Model_Type = \"LCA\", Grade = \"12th\")"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"examine-output-warnings","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.5.1 Examine Output Warnings","text":"Save Warning TablesExtract Errors Output Files","code":"\nsource(here(\"tc_lta\",\"functions\", \"extract_warnings.R\"))\n\n# ---- 7th Grade ----\nwarnings_g7 <- extract_warnings(final_data_g7) %>%\n  left_join(select(final_data_g7, File_Name), by = \"File_Name\")\n\nwarnings_table_g7 <- warnings_g7 %>%\n  gt() %>%\n  tab_header(title = md(\"**Model Warnings — 7th Grade LCA**\")) %>%\n  cols_label(\n    File_Name = \"Output File\",\n    Warning_Summary = \"# of Warnings\",\n    Warnings = \"Warning Message(s)\"\n  ) %>%\n  cols_align(align = \"left\", columns = everything()) %>%\n  cols_width(\n    File_Name ~ px(150),\n    Warning_Summary ~ px(150),\n    Warnings ~ px(400)\n  ) %>%\n  tab_options(table.width = pct(100))\n\n# ---- 10th Grade ----\nwarnings_g10 <- extract_warnings(final_data_g10) %>%\n  left_join(select(final_data_g10, File_Name), by = \"File_Name\")\n\nwarnings_table_g10 <- warnings_g10 %>%\n  gt() %>%\n  tab_header(title = md(\"**Model Warnings — 10th Grade LCA**\")) %>%\n  cols_label(\n    File_Name = \"Output File\",\n    Warning_Summary = \"# of Warnings\",\n    Warnings = \"Warning Message(s)\"\n  ) %>%\n  cols_align(align = \"left\", columns = everything()) %>%\n  cols_width(\n    File_Name ~ px(150),\n    Warning_Summary ~ px(150),\n    Warnings ~ px(400)\n  ) %>%\n  tab_options(table.width = pct(100))\n\n# ---- 12th Grade ----\nwarnings_g12 <- extract_warnings(final_data_g12) %>%\n  left_join(select(final_data_g12, File_Name), by = \"File_Name\")\n\nwarnings_table_g12 <- warnings_g12 %>%\n  gt() %>%\n  tab_header(title = md(\"**Model Warnings — 12th Grade LCA**\")) %>%\n  cols_label(\n    File_Name = \"Output File\",\n    Warning_Summary = \"# of Warnings\",\n    Warnings = \"Warning Message(s)\"\n  ) %>%\n  cols_align(align = \"left\", columns = everything()) %>%\n  cols_width(\n    File_Name ~ px(150),\n    Warning_Summary ~ px(150),\n    Warnings ~ px(400)\n  ) %>%\n  tab_options(table.width = pct(100))\n\n# Print all three\nwarnings_table_g7\nwarnings_table_g10\nwarnings_table_g12\ngtsave(warnings_table_g7, filename = here(\"tc_lta\",\"figures\", \"warnings_g7_lca.png\"))\ngtsave(warnings_table_g10, filename = here(\"tc_lta\",\"figures\", \"warnings_g10_lca.png\"))\ngtsave(warnings_table_g12, filename = here(\"tc_lta\",\"figures\", \"warnings_g12_lca.png\"))\nsource(here(\"tc_lta\",\"functions\", \"error_visualization.R\"))\n\n# ---- 7th Grade Errors ----\nerror_table_g7 <- process_error_data(final_data_g7)\n\n# ---- 10th Grade Errors ----\nerror_table_g10 <- process_error_data(final_data_g10)\n\n# ---- 12th Grade Errors ----\nerror_table_g12 <- process_error_data(final_data_g12)"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"examine-errors","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.5.2 Examine Errors","text":"Save Error Tables","code":"\n\n# Helper to conditionally render or notify\nrender_error_table <- function(error_df, grade_label) {\n  if (nrow(error_df) > 0) {\n    error_df %>%\n      gt() %>%\n      tab_header(title = md(glue(\"**Model Estimation Errors — {grade_label} Grade**\"))) %>%\n      cols_label(\n        File_Name     = \"Output File\",\n        Class_Model   = \"Model Type\",\n        Error_Message = \"Error Message\"\n      ) %>%\n      cols_align(align = \"left\", columns = everything()) %>%\n      cols_width(\n        File_Name     ~ px(150),\n        Class_Model   ~ px(100),\n        Error_Message ~ px(400)\n      ) %>%\n      tab_options(table.width = px(600)) %>%\n      fmt(\n        columns = \"Error_Message\",\n        fns = function(x) gsub(\"\\n\", \"<br>\", x)\n      )\n  } else {\n    cat(glue(\"✅ No errors detected for {grade_label} Grade.\\n\"))\n  }\n}\n\n# Print or notify for each grade\nrender_error_table(error_table_g7, \"7th\")\nrender_error_table(error_table_g10, \"10th\")\nrender_error_table(error_table_g12, \"12th\")\nif (exists(\"error_table_g7\") && nrow(error_table_g7) > 0) {\n  gtsave(render_error_table(error_table_g7, \"7th\"), here(\"tc_lta\",\"figures\", \"errors_g7_lca.png\"))\n}\n\nif (exists(\"error_table_g10\") && nrow(error_table_g10) > 0) {\n  gtsave(render_error_table(error_table_g10, \"10th\"), here(\"tc_lta\",\"figures\", \"errors_g10_lca.png\"))\n}\n\nif (exists(\"error_table_g12\") && nrow(error_table_g12) > 0) {\n  gtsave(render_error_table(error_table_g12, \"12th\"), here(\"tc_lta\",\"figures\", \"errors_g12_lca.png\"))\n}"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"examine-convergence-information","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.5.3 Examine Convergence Information","text":"LCA Convergence Table — 7th Grade (N = 1886)N = 1886Random StartsFinal starts convergingLL ReplicationSmallest ClassModelBest LLnparInitialFinal𝒇%𝒇%𝒇%1-Class-11,803.429105001010100%10100%1,886100.0%2-Class-10,418.761215001010100%10100%78241.5%3-Class-10,165.874325001010100%10100%38420.4%4-Class-10,042.970435001010100%10100%39020.7%5-Class-9,969.239545001010100%660%1759.3%6-Class-9,915.148655001010100%880%1809.5%7-Class-9,891.204765001010100%550%472.5%8-Class-9,871.411875001010100%110%552.9%LCA Convergence Table — 10th Grade (N = 1534)N = 1534Random StartsFinal starts convergingLL ReplicationSmallest ClassModelBest LLnparInitialFinal𝒇%𝒇%𝒇%1-Class-10,072.926105001010100%10100%1,534100.0%2-Class-8,428.384215001010100%10100%65842.9%3-Class-8,067.612325001010100%10100%29719.4%4-Class-7,905.535435001010100%10100%29018.9%5-Class-7,845.441545001010100%10100%22014.3%6-Class-7,806.987655001010100%330%1308.5%7-Class-7,779.821765001010100%110%624.1%8-Class-7,754.190875001010100%220%845.5%LCA Convergence Table — 12th Grade (N = 1122)N = 1122Random StartsFinal starts convergingLL ReplicationSmallest ClassModelBest LLnparInitialFinal𝒇%𝒇%𝒇%1-Class-7,349.129105001010100%10100%1,122100.0%2-Class-5,976.598215001010100%10100%53447.5%3-Class-5,670.595325001010100%10100%20318.1%4-Class-5,543.616435001010100%10100%21919.5%5-Class-5,483.669545001010100%990%13111.7%6-Class-5,444.055655001010100%550%817.2%7-Class-5,420.551765001010100%110%443.9%8-Class-5,395.532875001010100%110%615.4%Save Convergence TablesScrape Replication Data","code":"\n# Load function\nsource(here(\"tc_lta\",\"functions\", \"summary_table.R\"))\n\n# Helper: clean + prep for each dataset\nprepare_convergence_table <- function(data_object, grade_label) {\n  sample_size <- data_object$Sample_Size[1]\n\n  data_flat <- data_object %>%\n    select(-LogLikelihoods, -Errors, -Warnings) %>%\n    mutate(across(\n      c(\n        Best_LogLikelihood,\n        Perc_Convergence,\n        Replicated_LL_Perc,\n        Smallest_Class_Perc,\n        Condition_Number\n      ),\n      ~ as.numeric(gsub(\",\", \"\", .))\n    ))\n\n  tbl <- create_flextable(data_flat, sample_size)\n\n  # Get actual number of columns in the flextable object\n  n_cols <- length(tbl$body$col_keys)\n\n  # Title string\n  title_text <- glue(\"LCA Convergence Table — {grade_label} Grade (N = {sample_size})\")\n\n  # This is the correct call: title as one string, colwidth = full span\n  tbl <- add_header_row(\n    tbl,\n    values = title_text,\n    colwidths = n_cols\n  ) %>%\n    align(i = 1, align = \"center\", part = \"header\") %>%\n    fontsize(i = 1, size = 12, part = \"header\") %>%\n    bg(i = 1, bg = \"#ffffff\", part = \"header\")\n\n  return(tbl)\n}\n\n# Create tables\nsummary_table_g7  <- prepare_convergence_table(final_data_g7, \"7th\")\nsummary_table_g10 <- prepare_convergence_table(final_data_g10, \"10th\")\nsummary_table_g12 <- prepare_convergence_table(final_data_g12, \"12th\")\n\nsummary_table_g7 \nsummary_table_g10\nsummary_table_g12\n# Save convergence tables as PNGs\ninvisible(save_as_image(summary_table_g7,  path = here(\"tc_lta\",\"figures\", \"convergence_g7_lca.png\")))\ninvisible(save_as_image(summary_table_g10, path = here(\"tc_lta\",\"figures\", \"convergence_g10_lca.png\")))\ninvisible(save_as_image(summary_table_g12, path = here(\"tc_lta\",\"figures\", \"convergence_g12_lca.png\")))\n# Load the function file containing generate_ll_replication_plots and create_ll_replication_table_all\nsource(here(\"tc_lta\",\"functions\", \"ll_replication_plots.R\"))\n# Load the second function from ll_replication_processing.R\nsource(here(\"tc_lta\",\"functions\", \"ll_replication_processing.R\"))"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"examine-loglikelihood-replication-information","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.5.4 Examine Loglikelihood Replication Information","text":"Log-Likelihood Replication Table — 7th Grade LCA1-Class2-Class3-Class4-Class5-Class6-Class7-Class8-ClassLL_c1n_c1perc_c1LL_c2n_c2perc_c2LL_c3n_c3perc_c3LL_c4n_c4perc_c4LL_c5n_c5perc_c5LL_c6n_c6perc_c6LL_c7n_c7perc_c7LL_c8n_c8perc_c8-11803.42910100-10418.76110100-10165.87410100-10042.9710100-9969.239660-9915.148880-9891.204550-9,871.411110————————————-9969.697440-9915.778110-9893.864110-9,872.124110———————————————-9916.262110-9894.265110-9,874.200110——————————————————-9898.309110-9,876.173110——————————————————-9904.214110-9,876.914110——————————————————-9904.532110-9,877.583110—————————————————————-9,877.741110—————————————————————-9,877.806110—————————————————————-9,879.960110—————————————————————-9,888.932110Log-Likelihood Replication Table — 10th Grade LCA1-Class2-Class3-Class4-Class5-Class6-Class7-Class8-ClassLL_c1n_c1perc_c1LL_c2n_c2perc_c2LL_c3n_c3perc_c3LL_c4n_c4perc_c4LL_c5n_c5perc_c5LL_c6n_c6perc_c6LL_c7n_c7perc_c7LL_c8n_c8perc_c8-10072.92610100-8428.38410100-8067.61210100-7905.53510100-7845.44110100-7806.987330-7779.821110-7,754.190220———————————————-7807.464440-7780.664550-7,754.206110———————————————-7807.485110-7781.893110-7,757.915110———————————————-7808.751110-7783.866110-7,758.288110———————————————-7820.453110-7784.195110-7,759.634110——————————————————-7784.933110-7,759.730110—————————————————————-7,760.773110—————————————————————-7,762.451110—————————————————————-7,762.777110Log-Likelihood Replication Table — 12th Grade LCA1-Class2-Class3-Class4-Class5-Class6-Class7-Class8-ClassLL_c1n_c1perc_c1LL_c2n_c2perc_c2LL_c3n_c3perc_c3LL_c4n_c4perc_c4LL_c5n_c5perc_c5LL_c6n_c6perc_c6LL_c7n_c7perc_c7LL_c8n_c8perc_c8-7349.12910100-5976.59810100-5670.59510100-5543.61610100-5483.669990-5444.055550-5420.551110-5,395.532110————————————-5484.101110-5446.399440-5421.469110-5,397.310110———————————————-5446.424110-5421.554220-5,397.311110——————————————————-5421.845110-5,398.708110——————————————————-5422.176110-5,399.930110——————————————————-5423.003110-5,400.458110——————————————————-5424.777110-5,400.463110——————————————————-5426.79110-5,401.156110——————————————————-5427.065110-5,403.971110—————————————————————-5,405.328110Save LL Replication Tables","code":"\n# Generate replication plots (invisible, for diagnostic use)\nll_replication_tables_g7  <- generate_ll_replication_plots(final_data_g7)\nll_replication_tables_g10 <- generate_ll_replication_plots(final_data_g10)\nll_replication_tables_g12 <- generate_ll_replication_plots(final_data_g12)\n\n# Create replication tables\nll_replication_table_g7 <- create_ll_replication_table_all(final_data_g7)\nll_replication_table_g10 <- create_ll_replication_table_all(final_data_g10)\nll_replication_table_g12 <- create_ll_replication_table_all(final_data_g12)\n\n# Add visible title row to each table\nll_replication_table_g7 <- add_header_row(\n  ll_replication_table_g7,\n  values = \"Log-Likelihood Replication Table — 7th Grade LCA\",\n  colwidths = ncol(ll_replication_table_g7$body$dataset)\n) %>%\n  align(i = 1, align = \"center\", part = \"header\") %>%\n  fontsize(i = 1, size = 12, part = \"header\") %>%\n  bg(i = 1, bg = \"#ffffff\", part = \"header\")\n\nll_replication_table_g10 <- add_header_row(\n  ll_replication_table_g10,\n  values = \"Log-Likelihood Replication Table — 10th Grade LCA\",\n  colwidths = ncol(ll_replication_table_g10$body$dataset)\n) %>%\n  align(i = 1, align = \"center\", part = \"header\") %>%\n  fontsize(i = 1, size = 12, part = \"header\") %>%\n  bg(i = 1, bg = \"#ffffff\", part = \"header\")\n\nll_replication_table_g12 <- add_header_row(\n  ll_replication_table_g12,\n  values = \"Log-Likelihood Replication Table — 12th Grade LCA\",\n  colwidths = ncol(ll_replication_table_g12$body$dataset)\n) %>%\n  align(i = 1, align = \"center\", part = \"header\") %>%\n  fontsize(i = 1, size = 12, part = \"header\") %>%\n  bg(i = 1, bg = \"#ffffff\", part = \"header\")\n\n# Display the three titled tables\nll_replication_table_g7\nll_replication_table_g10\nll_replication_table_g12\n\ninvisible({\n  flextable::save_as_image(ll_replication_table_g7, path = here::here(\"tc_lta\",\"figures\", \"ll_replication_table_g7.png\"))\n  flextable::save_as_image(ll_replication_table_g10, path = here::here(\"tc_lta\",\"figures\", \"ll_replication_table_g10.png\"))\n  flextable::save_as_image(ll_replication_table_g12, path = here::here(\"tc_lta\",\"figures\", \"ll_replication_table_g12.png\"))\n})"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"examine-model-fit-for-optimal-solution","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.6 Examine Model Fit for Optimal Solution","text":"evaluate model fit grade-level LCA identify optimal number latent profiles. Fit statistics BIC, entropy, likelihood ratio tests (LMR BLRT) compared across models. goal select parsimonious solution provides clear separation classes replicates reliably. selections form basis longitudinal model next stage.Extract fit statistics","code":"\n\n# Define grade-specific folders and data\ngrades <- c(\"g7\", \"g10\", \"g12\")\nfolder_paths <- c(\n  g7 = here(\"tc_lta\",\"g7_enum\"),\n  g10 = here(\"tc_lta\",\"g10_enum\"),\n  g12 = here(\"tc_lta\",\"g12_enum\")\n)\nfinal_data_list <- list(\n  g7 = final_data_g7,\n  g10 = final_data_g10,\n  g12 = final_data_g12\n)\n\n# Initialize storage\noutput_models_all <- list()\nallFit_list <- list()\n\n# Process each grade\nfor (grade in grades) {\n  # Get all .out files (1-8 classes)\n  out_files <- list.files(folder_paths[grade], pattern = \"^c[1-8]_g\\\\d+\\\\.out$\", full.names = TRUE)\n  \n  # Read Mplus output files\n  output_models <- list()\n  for (file in out_files) {\n    model <- readModels(file, quiet = TRUE)\n    if (!is.null(model) && length(model) > 0) {\n      output_models[[basename(file)]] <- model\n    }\n  }\n  \n  # Store models\n  output_models_all[[grade]] <- output_models\n  \n  # Extract summary table\n  model_extract <- LatexSummaryTable(\n    output_models,\n    keepCols = c(\"Title\", \"Parameters\", \"LL\", \"BIC\", \"aBIC\", \"T11_VLMR_PValue\", \"BLRT_PValue\", \"Observations\", \"Entropy\"),\n    sortBy = \"Title\"\n  )\n  \n  # Compute additional fit indices\n  allFit <- model_extract %>%\n    mutate(\n      Title = str_trim(Title),\n      Grade = toupper(grade),\n      Classes = as.integer(str_extract(Title, \"\\\\d+\")), # Extract class number\n      File_Name = names(output_models),\n      CAIC = -2 * LL + Parameters * (log(Observations) + 1),\n      AWE = -2 * LL + 2 * Parameters * (log(Observations) + 1.5),\n      SIC = -0.5 * BIC,\n      expSIC = exp(SIC - max(SIC, na.rm = TRUE)),\n      BF = if_else(is.na(lead(SIC)), NA_real_, exp(SIC - lead(SIC))),\n      cmPk = expSIC / sum(expSIC, na.rm = TRUE)\n    ) %>%\n    arrange(Classes)\n  \n  # Clean Class_Model for joining\n  final_data_clean <- final_data_list[[grade]] %>%\n    mutate(\n      Classes = as.integer(str_extract(Class_Model, \"\\\\d+\"))\n    ) %>%\n    select(Class_Model, Classes, Perc_Convergence, Replicated_LL_Perc, Smallest_Class, Smallest_Class_Perc)\n  \n  # Merge with final_data\n  allFit <- allFit %>%\n    left_join(\n      final_data_clean,\n      by = \"Classes\"\n    ) %>%\n    mutate(\n      Smallest_Class_Combined = paste0(Smallest_Class, \"\\u00A0(\", Smallest_Class_Perc, \"%)\")\n    ) %>%\n    relocate(Grade, Classes, Parameters, LL, Perc_Convergence, Replicated_LL_Perc, .before = BIC) %>%\n    select(\n      Grade, Classes, Parameters, LL, Perc_Convergence, Replicated_LL_Perc,\n      BIC, aBIC, CAIC, AWE, T11_VLMR_PValue, BLRT_PValue, Entropy, Smallest_Class_Combined, BF, cmPk\n    )\n  \n  # Store fit data\n  allFit_list[[grade]] <- allFit\n}\n\n# Combine fit data\nallFit_combined <- bind_rows(allFit_list) %>%\n  arrange(Grade, Classes)"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"examine-fit-statistics","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.6.1 Examine Fit Statistics","text":"Save Fit Tables","code":"\n\n# Initialize list to store tables\nfit_tables <- list()\n\n# Create and render table for each grade\nfor (grade in grades) {\n  # Filter data for current grade\n  allFit_grade <- allFit_combined %>%\n    filter(Grade == toupper(grade)) %>%\n    select(-Perc_Convergence, -Replicated_LL_Perc) # Exclude convergence columns\n  \n  # Create table\n  fit_table <- allFit_grade %>%\n    gt() %>%\n    tab_header(title = md(sprintf(\"**Model Fit Summary Table for %s Grade**\", toupper(grade)))) %>%\n    tab_spanner(label = \"Model Fit Indices\", columns = c(BIC, aBIC, CAIC, AWE)) %>%\n    tab_spanner(label = \"LRTs\", columns = c(T11_VLMR_PValue, BLRT_PValue)) %>%\n    tab_spanner(label = md(\"Smallest\\u00A0Class\"), columns = c(Smallest_Class_Combined)) %>%\n    cols_label(\n      Grade = \"Grade\",\n      Classes = \"Classes\",\n      Parameters = md(\"npar\"),\n      LL = md(\"*LL*\"),\n      # Perc_Convergence = \"% Converged\", # Commented out\n      # Replicated_LL_Perc = \"% Replicated\", # Commented out\n      BIC = \"BIC\",\n      aBIC = \"aBIC\",\n      CAIC = \"CAIC\",\n      AWE = \"AWE\",\n      T11_VLMR_PValue = \"VLMR\",\n      BLRT_PValue = \"BLRT\",\n      Entropy = \"Entropy\",\n      Smallest_Class_Combined = \"n (%)\",\n      BF = \"BF\",\n      cmPk = \"cmPk\"\n    ) %>%\n    tab_footnote(\n      footnote = md(\n        \"*Note.* npar = Parameters; *LL* = model log likelihood;\n        BIC = Bayesian information criterion;\n        aBIC = sample size adjusted BIC; CAIC = consistent Akaike information criterion;\n        AWE = approximate weight of evidence criterion;\n        BLRT = bootstrapped likelihood ratio test p-value;\n        VLMR = Vuong-Lo-Mendell-Rubin adjusted likelihood ratio test p-value;\n        Smallest n (%) = Number of cases in the smallest class.\"\n      ),\n      locations = cells_title()\n    ) %>%\n    tab_options(column_labels.font.weight = \"bold\") %>%\n    fmt_number(\n      columns = c(LL, BIC, aBIC, CAIC, AWE, Entropy, BF, cmPk),\n      decimals = 2\n    ) %>%\n    fmt(\n      columns = c(T11_VLMR_PValue, BLRT_PValue),\n      fns = function(x) ifelse(is.na(x), \"—\", ifelse(x < 0.001, \"<.001\", scales::number(x, accuracy = .01)))\n    ) %>%\n    cols_align(align = \"center\", columns = everything()) %>%\n    tab_style(\n      style = list(cell_text(weight = \"bold\")),\n      locations = list(\n        cells_body(columns = BIC, rows = BIC == min(BIC)),\n        cells_body(columns = aBIC, rows = aBIC == min(aBIC)),\n        cells_body(columns = CAIC, rows = CAIC == min(CAIC)),\n        cells_body(columns = AWE, rows = AWE == min(AWE)),\n        cells_body(\n          columns = T11_VLMR_PValue,\n          rows = T11_VLMR_PValue < .05 & lead(T11_VLMR_PValue, default = 1) > .05\n        ),\n        cells_body(\n          columns = BLRT_PValue,\n          rows = BLRT_PValue < .05 & lead(BLRT_PValue, default = 1) > .05\n        )\n      )\n    )\n  \n  # Store table\n  fit_tables[[grade]] <- fit_table\n  \n  # Render table without markdown header\n  # cat(sprintf(\"\\n### LCA Fit Table for %s Grade\\n\", toupper(grade))) # Commented out to avoid markdown headers\n  print(fit_table)\n}\n# Save tables as PNG files\ngtsave(fit_tables[[\"g7\"]], filename = here(\"tc_lta\",\"figures\", \"fit_table_lca_g7.png\"))\ngtsave(fit_tables[[\"g10\"]], filename = here(\"tc_lta\",\"figures\", \"fit_table_lca_g10.png\"))\ngtsave(fit_tables[[\"g12\"]], filename = here(\"tc_lta\",\"figures\", \"fit_table_lca_g12.png\"))"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"create-and-examine-probability-plots-for-four-class-solution","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.7 Create and Examine Probability Plots for Four Class Solution","text":"aid interpretation, visualize conditional response probabilities selected four-class solution grade. plots show probability endorsing response category within latent profile help clarify classes","code":"\n# Source plot_lca function\nsource(here(\"tc_lta\",\"functions\", \"plot_lca.txt\"))\n\n# Define grades and folders\ngrades <- c(\"g7\", \"g10\", \"g12\")\nfolder_paths <- c(\n  g7 = here(\"tc_lta\",\"g7_enum\"),\n  g10 = here(\"tc_lta\",\"g10_enum\"),\n  g12 = here(\"tc_lta\",\"g12_enum\")\n)\n\n# Generate plot for each grade\nfor (grade in grades) {\n  # Read 4-class model\n  model_file <- file.path(folder_paths[grade], paste0(\"c4_\", grade, \".out\"))\n  model <- readModels(model_file, quiet = TRUE)\n  \n  # Base plot\n  base_plot <- plot_lca(model_name = model)\n  \n  # Get class sizes and round to 1 decimal\n  c_size <- round(model$class_counts$modelEstimated$proportion * 100, 1)\n  \n  # Customize plot with generic class labels\n  final_plot <- base_plot +\n    scale_colour_discrete(labels = c(\n      glue(\"Class 1 ({c_size[1]}%)\"),\n      glue(\"Class 2 ({c_size[2]}%)\"),\n      glue(\"Class 3 ({c_size[3]}%)\"),\n      glue(\"Class 4 ({c_size[4]}%)\")\n    )) +\n    scale_shape_discrete(labels = c(\n      glue(\"Class 1 ({c_size[1]}%)\"),\n      glue(\"Class 2 ({c_size[2]}%)\"),\n      glue(\"Class 3 ({c_size[3]}%)\"),\n      glue(\"Class 4 ({c_size[4]}%)\")\n    )) +\n    scale_linetype_discrete(labels = c(\n      glue(\"Class 1 ({c_size[1]}%)\"),\n      glue(\"Class 2 ({c_size[2]}%)\"),\n      glue(\"Class 3 ({c_size[3]}%)\"),\n      glue(\"Class 4 ({c_size[4]}%)\")\n    )) +\n    labs(title = glue(\"4-Class LCA Item Probability Plot for Grade {toupper(grade)}\"))\n  \n  # Display plot\n  print(final_plot)\n}"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"conduct-step-1-invariant-latent-transition-analysis","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.8 Conduct step 1: Invariant Latent Transition Analysis","text":"Step 1 latent transition analysis (LTA), estimate unconditional longitudinal model measurement invariance across timepoints. means item-response thresholds constrained equal across Grades 7, 10, 12, allowing us interpret latent class transitions time consistent measurement scale. step provides foundation 3-step approach establishing stable class structure across waves.","code":"\n\n# Define LTA model\nlta_model <- mplusObject(\n  TITLE = \"4-Class LTA for G7, G10, G12\",\n  \n  VARIABLE = glue(\n    \"categorical = AB39A AB39H AB39I AB39K AB39L AB39M AB39T AB39U AB39W AB39X\n                   GA32A GA32H GA32I GA32K GA32L GA33A GA33H GA33I GA33K GA33L\n                   KA46A KA46H KA46I KA46K KA46L KA47A KA47H KA47I KA47K KA47L;\n     usevar = AB39A AB39H AB39I AB39K AB39L AB39M AB39T AB39U AB39W AB39X\n              GA32A GA32H GA32I GA32K GA32L GA33A GA33H GA33I GA33K GA33L\n              KA46A KA46H KA46I KA46K KA46L KA47A KA47H KA47I KA47K KA47L\n               FEMALE MINORITY STEM STEMSUP ENGINEER MATHG8 SCIG8 MATHG11 SCIG11\n               MATHG7 MATHG10 MATHG12;\n     auxiliary = FEMALE MINORITY STEM STEMSUP ENGINEER MATHG8 SCIG8 MATHG11 SCIG11\n     MATHG7 MATHG10 MATHG12;\n     missing = all(9999);\n     classes = c1(4) c2(4) c3(4);\n    auxiliary = stem;\"\n  ),\n  \n  ANALYSIS = \"\n    estimator = mlr;\n    type = mixture;\n    starts = 500 10;\n    processors = 4;\",\n  \n  MODEL = glue(\n    \"%overall%\n     c2 on c1;\n     c3 on c2;\n     \n     MODEL c1:\n     %c1#1%\n     [AB39A$1-AB39X$1] (1-10);\n     %c1#2%\n     [AB39A$1-AB39X$1] (11-20);\n     %c1#3%\n     [AB39A$1-AB39X$1] (21-30);\n     %c1#4%\n     [AB39A$1-AB39X$1] (31-40);\n     \n     MODEL c2:\n     %c2#1%\n     [GA32A$1-GA33L$1] (1-10);\n     %c2#2%\n     [GA32A$1-GA33L$1] (11-20);\n     %c2#3%\n     [GA32A$1-GA33L$1] (21-30);\n     %c2#4%\n     [GA32A$1-GA33L$1] (31-40);\n     \n     MODEL c3:\n     %c3#1%\n     [KA46A$1-KA47L$1] (1-10);\n     %c3#2%\n     [KA46A$1-KA47L$1] (11-20);\n     %c3#3%\n     [KA46A$1-KA47L$1] (21-30);\n     %c3#4%\n     [KA46A$1-KA47L$1] (31-40);\"\n  ),\n  \n  OUTPUT = \"svalues; sampstat; tech11; tech14;\",\n  \n    SAVEDATA = \"\n    file = lta_4class_cprobs.dat;\n    save = cprobabilities;\n    missflag = 9999;\",\n  \n  PLOT = \"\n    type = plot3;\n    series = AB39A AB39H AB39I AB39K AB39L AB39M AB39T AB39U AB39W AB39X\n             GA32A GA32H GA32I GA32K GA32L GA33A GA33H GA33I GA33K GA33L\n             KA46A KA46H KA46I KA46K KA46L KA47A KA47H KA47I KA47K KA47L (*);\",\n  \n  rdata = lsay_data\n)\n\n# Run LTA model\nlta_fit <- mplusModeler(\n  lta_model,\n  dataout = here(\"tc_lta\",\"lta_enum\", \"lsay_lta.dat\"),\n  modelout = here(\"tc_lta\",\"lta_enum\", \"lta_4class.inp\"),\n  check = TRUE,\n  run = TRUE,\n  hashfilename = FALSE\n)"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"plot-invariant-probability-plot","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.8.1 Plot Invariant Probability Plot","text":"fitting invariant LTA model, examine conditional response probabilities latent class. plots illustrate students profile tend respond attitudinal items, averaged across timepoints. model assumes measurement invariance, differences classes can interpreted consistently across Grades 7, 10, 12.Save Invariant Probability Plot","code":"\n# Load LTA model\nlta_model <- readModels(here(\"tc_lta\",\"lta_enum\", \"lta_4class.out\"), quiet = TRUE)\n\n# Define G7 items\ng7_items <- c(\"AB39A\", \"AB39H\", \"AB39I\", \"AB39K\", \"AB39L\", \"AB39M\", \"AB39T\", \"AB39U\", \"AB39W\", \"AB39X\")\n\n# Define descriptive item labels (from Table 1 in the paper)\nitem_labels <- c(\n  \"I enjoy math\",\n  \"Math is useful in everyday problems\",\n  \"Math helps a person think logically\",\n  \"It is important to know math to get a good job\",\n  \"I will use math in many ways as an adult\",\n  \"I enjoy science\",\n  \"Science is useful in everyday problems\",\n  \"Science helps a person think logically\",\n  \"It is important to know science to get a good job\",\n  \"I will use science in many ways as an adult\"\n)\n\n# Define class names from the paper\nclass_names <- c(\n  \"Very Positive\",\n  \"Qualified Positive\",\n  \"Neutral\",\n  \"Less Positive\"\n)\n\n# Inline plot_lta function (with updated title to match Figure 1)\nplot_lta <- function(model_name, time_point = \"c1\", items = NULL) {\n  probs <- data.frame(model_name$parameters$probability.scale)\n  \n  pp_plots <- probs %>%\n    mutate(LatentClass = sub(\"^\", \"Class \", LatentClass)) %>%\n    filter(grepl(paste0(\"^Class \", toupper(time_point), \"#\"), LatentClass)) %>%\n    filter(category == 2) %>%\n    dplyr::select(est, LatentClass, param)\n  \n  if (!is.null(items)) {\n    pp_plots <- pp_plots %>%\n      filter(param %in% items)\n  } else {\n    pp_plots <- pp_plots %>%\n      filter(param %in% unique(param)[1:10])\n  }\n  \n  # Apply class names from the paper\n  pp_plots <- pp_plots %>%\n    mutate(LatentClass = case_when(\n      LatentClass == \"Class C1#1\" ~ class_names[1],\n      LatentClass == \"Class C1#2\" ~ class_names[2],\n      LatentClass == \"Class C1#3\" ~ class_names[3],\n      LatentClass == \"Class C1#4\" ~ class_names[4],\n      TRUE ~ LatentClass\n    ))\n  \n  pp_plots <- pp_plots %>%\n    pivot_wider(names_from = LatentClass, values_from = est) %>%\n    relocate(param, .after = last_col())\n  \n  # Extract class proportions (using Grade 7 proportions from Table 4)\n  c_size <- data.frame(cs = c(34, 33, 18, 14))\n  \n  colnames(pp_plots)[1:4] <- paste0(colnames(pp_plots)[1:4], glue(\" ({c_size[1:4,]}%)\"))\n  \n  plot_data <- pp_plots %>%\n    rename(\"param\" = ncol(pp_plots)) %>%\n    reshape2::melt(id.vars = \"param\") %>%\n    mutate(param = fct_inorder(param))\n  \n  # Apply descriptive item labels\n  levels(plot_data$param) <- item_labels\n  \n  # Create the plot with the exact title from Figure 1\n  p <- plot_data %>%\n    ggplot(\n      aes(\n        x = param,\n        y = value,\n        shape = variable,\n        linetype = variable,\n        group = variable\n      )\n    ) +\n    geom_point(size = 3) +\n    geom_line() +\n    scale_shape_manual(values = c(16, 17, 15, 18)) +\n    scale_linetype_manual(values = c(\"solid\", \"dashed\", \"dotted\", \"dotdash\")) +\n    scale_color_grey(start = 0.1, end = 0.6) +\n    scale_x_discrete(labels = function(x) str_wrap(x, width = 15)) +\n    ylim(0, 1) +\n    labs(\n      title = \"Proportion of seventh graders endorsing each item by attitudinal profile\",  # Exact title from Figure 1\n      x = \"\",\n      y = \"Probability of Endorsement\",\n      caption = \"Note: The four-class structure is invariant across grades 7, 10, and 12, but probabilities and proportions shown are for Grade 7.\"\n    ) +\n    theme_classic() +\n    theme(\n      text = element_text(family = \"serif\", size = 12),\n      legend.text = element_text(family = \"serif\", size = 12),\n      legend.key.width = unit(1, \"cm\"),\n      legend.title = element_blank(),\n      legend.position = \"top\",\n      axis.text.x = element_text(hjust = 0.5),\n      plot.margin = margin(b = 40),\n      plot.caption = element_text(hjust = 0, size = 10, family = \"serif\"),\n      plot.title = element_text(size = 12, family = \"serif\")  # Ensure title font matches\n    )\n  \n  return(p)\n}\n\n# Generate plot using plot_lta\nfinal_plot <- plot_lta(\n  model_name = lta_model,\n  time_point = \"c1\",\n  items = g7_items\n)\n\n# Display plot\nprint(final_plot)\n\n# Save the final_plot from Chunk 1\nggsave(\n  filename = here(\"tc_lta\",\"figures\", \"LTA_4class_invariant_plot.png\"),\n  plot = final_plot,\n  dpi = 300,\n  height = 5,\n  width = 7,\n  units = \"in\"\n)"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"conduct-step-2-lta","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.9 Conduct Step 2 LTA","text":"Step 2 3-step LTA procedure, re-estimate latent class models separately Grades 7, 10, 12, time fixing item-response thresholds logits obtained invariant Step 1 model. ensures class measurement remains consistent across time. also include auxiliary variables save modal class assignments student grade level. class assignments used Step 3 estimate covariate distal outcome effects accounting classification uncertainty.","code":""},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"scrape-logits-for-each-grade-for-the-invariant-lta","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.9.1 Scrape logits for each grade for the invariant LTA","text":"","code":"\nlta_fit <- readModels(here(\"tc_lta\",\"lta_enum\", \"lta_4class.out\"))\n\n\n# Extract item thresholds (logits) for all items across all latent classes\nitem_logits_full <- lta_fit$parameters$unstandardized %>%\n  filter(paramHeader == \"Thresholds\") %>%\n  filter(str_detect(param, \"\\\\$1\")) %>%\n  select(class = LatentClass, item = param, logit = est)"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"rerun-lca-for-grade-7-10-and-12-with-invariant-item-probabilities","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.9.2 Rerun LCA for Grade 7, 10, and 12 with invariant item probabilities","text":"Run LCA G7 fixed item probabilitiesRun LCA G10 fixed item probabilitiesRun LCA G12 fixed item probabilities","code":"\n\nlca_belonging_g7 <- {\n  g7_model <- mplusObject(\n    TITLE = \"4-Class G7 LCA with Fixed Thresholds from Invariant LTA\",\n\n  VARIABLE = \"\n  categorical = AB39A AB39H AB39I AB39K AB39L AB39M AB39T AB39U AB39W AB39X;\n  usevar = AB39A AB39H AB39I AB39K AB39L AB39M AB39T AB39U AB39W AB39X;\n  missing = all(9999);\n  classes = c(4);\n  auxiliary = GA32A GA32H GA32I GA32K GA32L GA33A GA33H GA33I GA33K GA33L\n            KA46A KA46H KA46I KA46K KA46L KA47A KA47H KA47I KA47K KA47L\n            STEM STEMSup ENGINEER MathG8 SciG8 MathG11 SciG11 Female\n            Minority MathG7 MathG10 MathG12 CASENUM;\",\n\n    ANALYSIS = \"\n      estimator = mlr;\n      type = mixture;\n      starts = 0;\n      processors = 4;\",\n\n    MODEL = glue(\"\n      %OVERALL%\n        [c#1@0];\n\n      %c#1%\n        [AB39A$1@{item_logits_full$logit[item_logits_full$class == 'C1#1' & item_logits_full$item == 'AB39A$1']}]\n        [AB39H$1@{item_logits_full$logit[item_logits_full$class == 'C1#1' & item_logits_full$item == 'AB39H$1']}]\n        [AB39I$1@{item_logits_full$logit[item_logits_full$class == 'C1#1' & item_logits_full$item == 'AB39I$1']}]\n        [AB39K$1@{item_logits_full$logit[item_logits_full$class == 'C1#1' & item_logits_full$item == 'AB39K$1']}]\n        [AB39L$1@{item_logits_full$logit[item_logits_full$class == 'C1#1' & item_logits_full$item == 'AB39L$1']}]\n        [AB39M$1@{item_logits_full$logit[item_logits_full$class == 'C1#1' & item_logits_full$item == 'AB39M$1']}]\n        [AB39T$1@{item_logits_full$logit[item_logits_full$class == 'C1#1' & item_logits_full$item == 'AB39T$1']}]\n        [AB39U$1@{item_logits_full$logit[item_logits_full$class == 'C1#1' & item_logits_full$item == 'AB39U$1']}]\n        [AB39W$1@{item_logits_full$logit[item_logits_full$class == 'C1#1' & item_logits_full$item == 'AB39W$1']}]\n        [AB39X$1@{item_logits_full$logit[item_logits_full$class == 'C1#1' & item_logits_full$item == 'AB39X$1']}]\n\n      %c#2%\n        [AB39A$1@{item_logits_full$logit[item_logits_full$class == 'C1#2' & item_logits_full$item == 'AB39A$1']}]\n        [AB39H$1@{item_logits_full$logit[item_logits_full$class == 'C1#2' & item_logits_full$item == 'AB39H$1']}]\n        [AB39I$1@{item_logits_full$logit[item_logits_full$class == 'C1#2' & item_logits_full$item == 'AB39I$1']}]\n        [AB39K$1@{item_logits_full$logit[item_logits_full$class == 'C1#2' & item_logits_full$item == 'AB39K$1']}]\n        [AB39L$1@{item_logits_full$logit[item_logits_full$class == 'C1#2' & item_logits_full$item == 'AB39L$1']}]\n        [AB39M$1@{item_logits_full$logit[item_logits_full$class == 'C1#2' & item_logits_full$item == 'AB39M$1']}]\n        [AB39T$1@{item_logits_full$logit[item_logits_full$class == 'C1#2' & item_logits_full$item == 'AB39T$1']}]\n        [AB39U$1@{item_logits_full$logit[item_logits_full$class == 'C1#2' & item_logits_full$item == 'AB39U$1']}]\n        [AB39W$1@{item_logits_full$logit[item_logits_full$class == 'C1#2' & item_logits_full$item == 'AB39W$1']}]\n        [AB39X$1@{item_logits_full$logit[item_logits_full$class == 'C1#2' & item_logits_full$item == 'AB39X$1']}]\n\n      %c#3%\n        [AB39A$1@{item_logits_full$logit[item_logits_full$class == 'C1#3' & item_logits_full$item == 'AB39A$1']}]\n        [AB39H$1@{item_logits_full$logit[item_logits_full$class == 'C1#3' & item_logits_full$item == 'AB39H$1']}]\n        [AB39I$1@{item_logits_full$logit[item_logits_full$class == 'C1#3' & item_logits_full$item == 'AB39I$1']}]\n        [AB39K$1@{item_logits_full$logit[item_logits_full$class == 'C1#3' & item_logits_full$item == 'AB39K$1']}]\n        [AB39L$1@{item_logits_full$logit[item_logits_full$class == 'C1#3' & item_logits_full$item == 'AB39L$1']}]\n        [AB39M$1@{item_logits_full$logit[item_logits_full$class == 'C1#3' & item_logits_full$item == 'AB39M$1']}]\n        [AB39T$1@{item_logits_full$logit[item_logits_full$class == 'C1#3' & item_logits_full$item == 'AB39T$1']}]\n        [AB39U$1@{item_logits_full$logit[item_logits_full$class == 'C1#3' & item_logits_full$item == 'AB39U$1']}]\n        [AB39W$1@{item_logits_full$logit[item_logits_full$class == 'C1#3' & item_logits_full$item == 'AB39W$1']}]\n        [AB39X$1@{item_logits_full$logit[item_logits_full$class == 'C1#3' & item_logits_full$item == 'AB39X$1']}]\n\n      %c#4%\n        [AB39A$1@{item_logits_full$logit[item_logits_full$class == 'C1#4' & item_logits_full$item == 'AB39A$1']}]\n        [AB39H$1@{item_logits_full$logit[item_logits_full$class == 'C1#4' & item_logits_full$item == 'AB39H$1']}]\n        [AB39I$1@{item_logits_full$logit[item_logits_full$class == 'C1#4' & item_logits_full$item == 'AB39I$1']}]\n        [AB39K$1@{item_logits_full$logit[item_logits_full$class == 'C1#4' & item_logits_full$item == 'AB39K$1']}]\n        [AB39L$1@{item_logits_full$logit[item_logits_full$class == 'C1#4' & item_logits_full$item == 'AB39L$1']}]\n        [AB39M$1@{item_logits_full$logit[item_logits_full$class == 'C1#4' & item_logits_full$item == 'AB39M$1']}]\n        [AB39T$1@{item_logits_full$logit[item_logits_full$class == 'C1#4' & item_logits_full$item == 'AB39T$1']}]\n        [AB39U$1@{item_logits_full$logit[item_logits_full$class == 'C1#4' & item_logits_full$item == 'AB39U$1']}]\n        [AB39W$1@{item_logits_full$logit[item_logits_full$class == 'C1#4' & item_logits_full$item == 'AB39W$1']}]\n        [AB39X$1@{item_logits_full$logit[item_logits_full$class == 'C1#4' & item_logits_full$item == 'AB39X$1']}]\n    \"),\n\n    OUTPUT = \"\n      sampstat;\n      tech1;\n      tech11;\n      tech14;\n      svalues;\",\n\n    SAVEDATA = \"\n      file = g7.dat;\n      save = cprobabilities;\n      missflag = 9999;\",\n  \n    rdata = lsay_data\n  )\n\n  mplusModeler(\n    g7_model,\n    dataout = here(\"tc_lta\",\"lca_enum2\", \"lca_g7.dat\"),\n    modelout = here(\"tc_lta\",\"lca_enum2\", \"lca_g7.inp\"),\n    check = TRUE,\n    run = TRUE,\n    hashfilename = FALSE\n  )\n}\n\nlca_belonging_g10 <- {\n  g10_model <- mplusObject(\n    TITLE = \"4-Class G10 LCA with Fixed Thresholds from Invariant LTA\",\n\n    VARIABLE = \"\n  categorical = GA32A GA32H GA32I GA32K GA32L GA33A GA33H GA33I GA33K GA33L;\n  usevar = GA32A GA32H GA32I GA32K GA32L GA33A GA33H GA33I GA33K GA33L;\n  auxiliary = AB39A AB39H AB39I AB39K AB39L AB39M AB39T AB39U AB39W AB39X\n              KA46A KA46H KA46I KA46K KA46L KA47A KA47H KA47I KA47K KA47L\n              STEM STEMSup ENGINEER MathG8 SciG8 MathG11 SciG11 Female \n              Minority MathG7 MathG10 MathG12 CASENUM;\n  missing = all(9999);\n  classes = c(4);\",\n\n    ANALYSIS = \"\n      estimator = mlr;\n      type = mixture;\n      starts = 0;\n      processors = 4;\",\n\n    MODEL = glue(\"\n      %OVERALL%\n        [c#1@0];\n\n      %c#1%\n        [GA32A$1@{item_logits_full$logit[item_logits_full$class == 'C2#1' & item_logits_full$item == 'GA32A$1']}]\n        [GA32H$1@{item_logits_full$logit[item_logits_full$class == 'C2#1' & item_logits_full$item == 'GA32H$1']}]\n        [GA32I$1@{item_logits_full$logit[item_logits_full$class == 'C2#1' & item_logits_full$item == 'GA32I$1']}]\n        [GA32K$1@{item_logits_full$logit[item_logits_full$class == 'C2#1' & item_logits_full$item == 'GA32K$1']}]\n        [GA32L$1@{item_logits_full$logit[item_logits_full$class == 'C2#1' & item_logits_full$item == 'GA32L$1']}]\n        [GA33A$1@{item_logits_full$logit[item_logits_full$class == 'C2#1' & item_logits_full$item == 'GA33A$1']}]\n        [GA33H$1@{item_logits_full$logit[item_logits_full$class == 'C2#1' & item_logits_full$item == 'GA33H$1']}]\n        [GA33I$1@{item_logits_full$logit[item_logits_full$class == 'C2#1' & item_logits_full$item == 'GA33I$1']}]\n        [GA33K$1@{item_logits_full$logit[item_logits_full$class == 'C2#1' & item_logits_full$item == 'GA33K$1']}]\n        [GA33L$1@{item_logits_full$logit[item_logits_full$class == 'C2#1' & item_logits_full$item == 'GA33L$1']}]\n\n      %c#2%\n        [GA32A$1@{item_logits_full$logit[item_logits_full$class == 'C2#2' & item_logits_full$item == 'GA32A$1']}]\n        [GA32H$1@{item_logits_full$logit[item_logits_full$class == 'C2#2' & item_logits_full$item == 'GA32H$1']}]\n        [GA32I$1@{item_logits_full$logit[item_logits_full$class == 'C2#2' & item_logits_full$item == 'GA32I$1']}]\n        [GA32K$1@{item_logits_full$logit[item_logits_full$class == 'C2#2' & item_logits_full$item == 'GA32K$1']}]\n        [GA32L$1@{item_logits_full$logit[item_logits_full$class == 'C2#2' & item_logits_full$item == 'GA32L$1']}]\n        [GA33A$1@{item_logits_full$logit[item_logits_full$class == 'C2#2' & item_logits_full$item == 'GA33A$1']}]\n        [GA33H$1@{item_logits_full$logit[item_logits_full$class == 'C2#2' & item_logits_full$item == 'GA33H$1']}]\n        [GA33I$1@{item_logits_full$logit[item_logits_full$class == 'C2#2' & item_logits_full$item == 'GA33I$1']}]\n        [GA33K$1@{item_logits_full$logit[item_logits_full$class == 'C2#2' & item_logits_full$item == 'GA33K$1']}]\n        [GA33L$1@{item_logits_full$logit[item_logits_full$class == 'C2#2' & item_logits_full$item == 'GA33L$1']}]\n\n      %c#3%\n        [GA32A$1@{item_logits_full$logit[item_logits_full$class == 'C2#3' & item_logits_full$item == 'GA32A$1']}]\n        [GA32H$1@{item_logits_full$logit[item_logits_full$class == 'C2#3' & item_logits_full$item == 'GA32H$1']}]\n        [GA32I$1@{item_logits_full$logit[item_logits_full$class == 'C2#3' & item_logits_full$item == 'GA32I$1']}]\n        [GA32K$1@{item_logits_full$logit[item_logits_full$class == 'C2#3' & item_logits_full$item == 'GA32K$1']}]\n        [GA32L$1@{item_logits_full$logit[item_logits_full$class == 'C2#3' & item_logits_full$item == 'GA32L$1']}]\n        [GA33A$1@{item_logits_full$logit[item_logits_full$class == 'C2#3' & item_logits_full$item == 'GA33A$1']}]\n        [GA33H$1@{item_logits_full$logit[item_logits_full$class == 'C2#3' & item_logits_full$item == 'GA33H$1']}]\n        [GA33I$1@{item_logits_full$logit[item_logits_full$class == 'C2#3' & item_logits_full$item == 'GA33I$1']}]\n        [GA33K$1@{item_logits_full$logit[item_logits_full$class == 'C2#3' & item_logits_full$item == 'GA33K$1']}]\n        [GA33L$1@{item_logits_full$logit[item_logits_full$class == 'C2#3' & item_logits_full$item == 'GA33L$1']}]\n\n      %c#4%\n        [GA32A$1@{item_logits_full$logit[item_logits_full$class == 'C2#4' & item_logits_full$item == 'GA32A$1']}]\n        [GA32H$1@{item_logits_full$logit[item_logits_full$class == 'C2#4' & item_logits_full$item == 'GA32H$1']}]\n        [GA32I$1@{item_logits_full$logit[item_logits_full$class == 'C2#4' & item_logits_full$item == 'GA32I$1']}]\n        [GA32K$1@{item_logits_full$logit[item_logits_full$class == 'C2#4' & item_logits_full$item == 'GA32K$1']}]\n        [GA32L$1@{item_logits_full$logit[item_logits_full$class == 'C2#4' & item_logits_full$item == 'GA32L$1']}]\n        [GA33A$1@{item_logits_full$logit[item_logits_full$class == 'C2#4' & item_logits_full$item == 'GA33A$1']}]\n        [GA33H$1@{item_logits_full$logit[item_logits_full$class == 'C2#4' & item_logits_full$item == 'GA33H$1']}]\n        [GA33I$1@{item_logits_full$logit[item_logits_full$class == 'C2#4' & item_logits_full$item == 'GA33I$1']}]\n        [GA33K$1@{item_logits_full$logit[item_logits_full$class == 'C2#4' & item_logits_full$item == 'GA33K$1']}]\n        [GA33L$1@{item_logits_full$logit[item_logits_full$class == 'C2#4' & item_logits_full$item == 'GA33L$1']}]\n    \"),\n\n    OUTPUT = \"\n      sampstat;\n      tech1;\n      tech11;\n      tech14;\n      svalues;\",\n\n    SAVEDATA = \"\n      file = g10.dat;\n      save = cprob;\n      missflag = 9999;\",\n\n    rdata = lsay_data\n  )\n\n  mplusModeler(\n    g10_model,\n    dataout = here(\"tc_lta\",\"lca_enum2\", \"lca_g10.dat\"),\n    modelout = here(\"tc_lta\",\"lca_enum2\", \"lca_g10.inp\"),\n    check = TRUE,\n    run = TRUE,\n    hashfilename = FALSE\n  )\n}\nlca_belonging_g12 <- {\n  g12_model <- mplusObject(\n    TITLE = \"4-Class G12 LCA with Fixed Thresholds from Invariant LTA\",\n\n    VARIABLE = \"\n  categorical = KA46A KA46H KA46I KA46K KA46L KA47A KA47H KA47I KA47K KA47L;\n  usevar = KA46A KA46H KA46I KA46K KA46L KA47A KA47H KA47I KA47K KA47L;\n  auxiliary = AB39A AB39H AB39I AB39K AB39L AB39M AB39T AB39U AB39W AB39X\n              GA32A GA32H GA32I GA32K GA32L GA33A GA33H GA33I GA33K GA33L\n              STEM STEMSup ENGINEER MathG8 SciG8 MathG11 SciG11 Female \n              Minority MathG7 MathG10 MathG12 CASENUM;\n  missing = all(9999);\n  classes = c(4);\",\n\n    ANALYSIS = \"\n      estimator = mlr;\n      type = mixture;\n      starts = 0;\n      processors = 4;\",\n\n    MODEL = glue(\"\n      %OVERALL%\n        [c#1@0];\n\n      %c#1%\n        [KA46A$1@{item_logits_full$logit[item_logits_full$class == 'C3#1' & item_logits_full$item == 'KA46A$1']}]\n        [KA46H$1@{item_logits_full$logit[item_logits_full$class == 'C3#1' & item_logits_full$item == 'KA46H$1']}]\n        [KA46I$1@{item_logits_full$logit[item_logits_full$class == 'C3#1' & item_logits_full$item == 'KA46I$1']}]\n        [KA46K$1@{item_logits_full$logit[item_logits_full$class == 'C3#1' & item_logits_full$item == 'KA46K$1']}]\n        [KA46L$1@{item_logits_full$logit[item_logits_full$class == 'C3#1' & item_logits_full$item == 'KA46L$1']}]\n        [KA47A$1@{item_logits_full$logit[item_logits_full$class == 'C3#1' & item_logits_full$item == 'KA47A$1']}]\n        [KA47H$1@{item_logits_full$logit[item_logits_full$class == 'C3#1' & item_logits_full$item == 'KA47H$1']}]\n        [KA47I$1@{item_logits_full$logit[item_logits_full$class == 'C3#1' & item_logits_full$item == 'KA47I$1']}]\n        [KA47K$1@{item_logits_full$logit[item_logits_full$class == 'C3#1' & item_logits_full$item == 'KA47K$1']}]\n        [KA47L$1@{item_logits_full$logit[item_logits_full$class == 'C3#1' & item_logits_full$item == 'KA47L$1']}]\n\n      %c#2%\n        [KA46A$1@{item_logits_full$logit[item_logits_full$class == 'C3#2' & item_logits_full$item == 'KA46A$1']}]\n        [KA46H$1@{item_logits_full$logit[item_logits_full$class == 'C3#2' & item_logits_full$item == 'KA46H$1']}]\n        [KA46I$1@{item_logits_full$logit[item_logits_full$class == 'C3#2' & item_logits_full$item == 'KA46I$1']}]\n        [KA46K$1@{item_logits_full$logit[item_logits_full$class == 'C3#2' & item_logits_full$item == 'KA46K$1']}]\n        [KA46L$1@{item_logits_full$logit[item_logits_full$class == 'C3#2' & item_logits_full$item == 'KA46L$1']}]\n        [KA47A$1@{item_logits_full$logit[item_logits_full$class == 'C3#2' & item_logits_full$item == 'KA47A$1']}]\n        [KA47H$1@{item_logits_full$logit[item_logits_full$class == 'C3#2' & item_logits_full$item == 'KA47H$1']}]\n        [KA47I$1@{item_logits_full$logit[item_logits_full$class == 'C3#2' & item_logits_full$item == 'KA47I$1']}]\n        [KA47K$1@{item_logits_full$logit[item_logits_full$class == 'C3#2' & item_logits_full$item == 'KA47K$1']}]\n        [KA47L$1@{item_logits_full$logit[item_logits_full$class == 'C3#2' & item_logits_full$item == 'KA47L$1']}]\n\n      %c#3%\n        [KA46A$1@{item_logits_full$logit[item_logits_full$class == 'C3#3' & item_logits_full$item == 'KA46A$1']}]\n        [KA46H$1@{item_logits_full$logit[item_logits_full$class == 'C3#3' & item_logits_full$item == 'KA46H$1']}]\n        [KA46I$1@{item_logits_full$logit[item_logits_full$class == 'C3#3' & item_logits_full$item == 'KA46I$1']}]\n        [KA46K$1@{item_logits_full$logit[item_logits_full$class == 'C3#3' & item_logits_full$item == 'KA46K$1']}]\n        [KA46L$1@{item_logits_full$logit[item_logits_full$class == 'C3#3' & item_logits_full$item == 'KA46L$1']}]\n        [KA47A$1@{item_logits_full$logit[item_logits_full$class == 'C3#3' & item_logits_full$item == 'KA47A$1']}]\n        [KA47H$1@{item_logits_full$logit[item_logits_full$class == 'C3#3' & item_logits_full$item == 'KA47H$1']}]\n        [KA47I$1@{item_logits_full$logit[item_logits_full$class == 'C3#3' & item_logits_full$item == 'KA47I$1']}]\n        [KA47K$1@{item_logits_full$logit[item_logits_full$class == 'C3#3' & item_logits_full$item == 'KA47K$1']}]\n        [KA47L$1@{item_logits_full$logit[item_logits_full$class == 'C3#3' & item_logits_full$item == 'KA47L$1']}]\n\n      %c#4%\n        [KA46A$1@{item_logits_full$logit[item_logits_full$class == 'C3#4' & item_logits_full$item == 'KA46A$1']}]\n        [KA46H$1@{item_logits_full$logit[item_logits_full$class == 'C3#4' & item_logits_full$item == 'KA46H$1']}]\n        [KA46I$1@{item_logits_full$logit[item_logits_full$class == 'C3#4' & item_logits_full$item == 'KA46I$1']}]\n        [KA46K$1@{item_logits_full$logit[item_logits_full$class == 'C3#4' & item_logits_full$item == 'KA46K$1']}]\n        [KA46L$1@{item_logits_full$logit[item_logits_full$class == 'C3#4' & item_logits_full$item == 'KA46L$1']}]\n        [KA47A$1@{item_logits_full$logit[item_logits_full$class == 'C3#4' & item_logits_full$item == 'KA47A$1']}]\n        [KA47H$1@{item_logits_full$logit[item_logits_full$class == 'C3#4' & item_logits_full$item == 'KA47H$1']}]\n        [KA47I$1@{item_logits_full$logit[item_logits_full$class == 'C3#4' & item_logits_full$item == 'KA47I$1']}]\n        [KA47K$1@{item_logits_full$logit[item_logits_full$class == 'C3#4' & item_logits_full$item == 'KA47K$1']}]\n        [KA47L$1@{item_logits_full$logit[item_logits_full$class == 'C3#4' & item_logits_full$item == 'KA47L$1']}]\n    \"),\n\n    OUTPUT = \"\n      sampstat;\n      tech1;\n      tech11;\n      tech14;\n      svalues;\",\n\n    SAVEDATA = \"\n      file = g12.dat;\n      save = cprob;\n      missflag = 9999;\",\n\n    rdata = lsay_data\n  )\n\n  mplusModeler(\n    g12_model,\n    dataout = here(\"tc_lta\",\"lca_enum2\", \"lca_g12.dat\"),\n    modelout = here(\"tc_lta\",\"lca_enum2\", \"lca_g12.inp\"),\n    check = TRUE,\n    run = TRUE,\n    hashfilename = FALSE\n  )\n}"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"prepare-data-for-step-2-lta","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.9.3 Prepare data for Step 2 LTA","text":"","code":"\n# Helper: extract variable names from a .out file\nscrape_saved_vars <- function(out_path) {\n  lines <- readLines(out_path)\n  start <- grep(\"^SAVEDATA INFORMATION$\", lines)\n  section <- lines[start:length(lines)]\n  var_start <- grep(\"^\\\\s*Order and format of variables\\\\s*$\", section)\n  var_lines <- section[(var_start + 1):length(section)]\n  end_idx <- which(grepl(\"^\\\\s*Save file format\\\\s*$\", var_lines))[1]\n  if (!is.na(end_idx)) {\n    var_lines <- var_lines[1:(end_idx - 1)]\n  }\n  var_lines <- var_lines[grepl(\"^\\\\s*[A-Za-z0-9_]+\\\\s+F10\\\\.3\\\\s*$\", var_lines)]\n  trimws(gsub(\"\\\\s+F10\\\\.3\\\\s*$\", \"\", var_lines))\n}\n\n# Paths\ng7_path <- here(\"tc_lta\",\"lca_enum2\", \"g7.dat\")\ng10_path <- here(\"tc_lta\",\"lca_enum2\", \"g10.dat\")\ng12_path <- here(\"tc_lta\",\"lca_enum2\", \"g12.dat\")\n\n# Output files\ng7_out <- here(\"tc_lta\",\"lca_enum2\", \"lca_g7.out\")\ng10_out <- here(\"tc_lta\",\"lca_enum2\", \"lca_g10.out\")\ng12_out <- here(\"tc_lta\",\"lca_enum2\", \"lca_g12.out\")\n\n# Scrape column names\ng7_vars <- scrape_saved_vars(g7_out)\ng10_vars <- scrape_saved_vars(g10_out)\ng12_vars <- scrape_saved_vars(g12_out)\n\n# Read and rename C column\ng7 <- read.table(g7_path, col.names = g7_vars)\ng10 <- read.table(g10_path, col.names = g10_vars)\ng12 <- read.table(g12_path, col.names = g12_vars)\n\n# Rename C columns\ng7 <- g7 %>% rename(N1 = C)\ng10 <- g10 %>% rename(N2 = C)\ng12 <- g12 %>% rename(N3 = C)\n\n# Write to CSV for inspection\nwrite_csv(g7, here(\"tc_lta\",\"lca_enum2\", \"g7_named.csv\"))\nwrite_csv(g10, here(\"tc_lta\",\"lca_enum2\", \"g10_named.csv\"))\nwrite_csv(g12, here(\"tc_lta\",\"lca_enum2\", \"g12_named.csv\"))"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"merge-files","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.10 Merge Files","text":"","code":"\n\n# Load base file\ng7 <- read_csv(here(\"tc_lta\",\"lca_enum2\", \"g7_named.csv\"), col_types = cols())\n\n# Only grab CASENUM + N2 / N3 from others\ng10 <- read_csv(here(\"tc_lta\",\"lca_enum2\", \"g10_named.csv\"), col_types = cols()) %>% select(CASENUM, N2)\ng12 <- read_csv(here(\"tc_lta\",\"lca_enum2\", \"g12_named.csv\"), col_types = cols()) %>% select(CASENUM, N3)\n# After loading all three CSVs:\ng7 <- g7 %>% mutate(across(everything(), ~replace_na(.x, 9999)))\ng10 <- g10 %>% mutate(across(everything(), ~replace_na(.x, 9999)))\ng12 <- g12 %>% mutate(across(everything(), ~replace_na(.x, 9999)))\n\n# Merge cleanly by CASENUM\nmerged <- g7 %>%\n  left_join(g10, by = \"CASENUM\") %>%\n  left_join(g12, by = \"CASENUM\") %>%\n  mutate(\n    N1 = replace_na(N1, 9999),\n    N2 = replace_na(N2, 9999),\n    N3 = replace_na(N3, 9999)\n  )\n\n# Write final output\nwrite_csv(merged, here(\"tc_lta\",\"lca_enum2\", \"merged.csv\"))"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"scrape-logits-for-lta","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.10.1 Scrape Logits for LTA","text":"","code":"\ng7_model  <- readModels(here(\"tc_lta\",\"lca_enum2\", \"lca_g7.out\"))\ng10_model <- readModels(here(\"tc_lta\",\"lca_enum2\", \"lca_g10.out\"))\ng12_model <- readModels(here(\"tc_lta\",\"lca_enum2\", \"lca_g12.out\"))"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"extract-logits","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.10.2 Extract Logits","text":"","code":"\n# Model list using correct source: class_counts$logitProbs.mostLikely\nmodel_list <- list(\n  G7  = g7_model$class_counts$logitProbs.mostLikely,\n  G10 = g10_model$class_counts$logitProbs.mostLikely,\n  G12 = g12_model$class_counts$logitProbs.mostLikely\n)\n\n# Reshape each 4x4 matrix into tidy long format\nstep2_logits <- map2_dfr(\n  model_list,\n  names(model_list),\n  function(mat, grade) {\n    # Assign column names explicitly\n    colnames(mat) <- paste0(\"V\", seq_len(ncol(mat)))\n\n    as.data.frame(mat) %>%\n      mutate(source_class = row_number()) %>%\n      pivot_longer(\n        cols = starts_with(\"V\"),\n        names_to = \"assigned_class\",\n        names_prefix = \"V\",\n        values_to = \"logit\"\n      ) %>%\n      mutate(\n        assigned_class = as.integer(assigned_class),\n        grade = grade\n      ) %>%\n      select(grade, source_class, assigned_class, logit)\n  }\n)"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"conduct-step-2-lta-with-fixed-classes","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.10.3 Conduct Step 2 LTA with fixed classes","text":"rerunning grade-specific LCAs fixed logits, merge saved modal class assignment files single dataset. merged file includes class membership grade along relevant auxiliary variables. Using file, run Step 2 LTA model fixing class assignment logits (classification probabilities) grade-specific models. step allows us model transitions across latent profiles time incorporating uncertainty class membership derived classification error rates.quality check, cross-reference estimated class sizes Step 2 invariant Step 1 model. Matching class sizes confirms Step 2 specification correctly replicates measurement structure preserves consistency profile estimation.","code":"\n\n# Extract logits for each timepoint into matrices\nlogits_n1 <- matrix(step2_logits$logit[step2_logits$grade == \"G7\"],  nrow = 4, byrow = TRUE)\nlogits_n2 <- matrix(step2_logits$logit[step2_logits$grade == \"G10\"], nrow = 4, byrow = TRUE)\nlogits_n3 <- matrix(step2_logits$logit[step2_logits$grade == \"G12\"], nrow = 4, byrow = TRUE)\n\n# Build Mplus object\nstep2_lta_model <- mplusObject(\n  TITLE = \"Step 2 LTA with Fixed Logits — No Covariates\",\n\n  VARIABLE = \"\n  USEVAR = n1 n2 n3;\n  NOMINAL = n1 n2 n3;\n  MISSING = all(9999);\n  CLASSES = c1(4) c2(4) c3(4);\",\n\n  ANALYSIS = \"\n  TYPE = mixture;\n  STARTS = 0;\",\n\n  MODEL = glue(\"\n  MODEL c1:\n    %c1#1%\n      [n1#1@{logits_n1[1,1]}];\n      [n1#2@{logits_n1[1,2]}];\n      [n1#3@{logits_n1[1,3]}];\n    %c1#2%\n      [n1#1@{logits_n1[2,1]}];\n      [n1#2@{logits_n1[2,2]}];\n      [n1#3@{logits_n1[2,3]}];\n    %c1#3%\n      [n1#1@{logits_n1[3,1]}];\n      [n1#2@{logits_n1[3,2]}];\n      [n1#3@{logits_n1[3,3]}];\n    %c1#4%\n      [n1#1@{logits_n1[4,1]}];\n      [n1#2@{logits_n1[4,2]}];\n      [n1#3@{logits_n1[4,3]}];\n\n  MODEL c2:\n    %c2#1%\n      [n2#1@{logits_n2[1,1]}];\n      [n2#2@{logits_n2[1,2]}];\n      [n2#3@{logits_n2[1,3]}];\n    %c2#2%\n      [n2#1@{logits_n2[2,1]}];\n      [n2#2@{logits_n2[2,2]}];\n      [n2#3@{logits_n2[2,3]}];\n    %c2#3%\n      [n2#1@{logits_n2[3,1]}];\n      [n2#2@{logits_n2[3,2]}];\n      [n2#3@{logits_n2[3,3]}];\n    %c2#4%\n      [n2#1@{logits_n2[4,1]}];\n      [n2#2@{logits_n2[4,2]}];\n      [n2#3@{logits_n2[4,3]}];\n\n  MODEL c3:\n    %c3#1%\n      [n3#1@{logits_n3[1,1]}];\n      [n3#2@{logits_n3[1,2]}];\n      [n3#3@{logits_n3[1,3]}];\n    %c3#2%\n      [n3#1@{logits_n3[2,1]}];\n      [n3#2@{logits_n3[2,2]}];\n      [n3#3@{logits_n3[2,3]}];\n    %c3#3%\n      [n3#1@{logits_n3[3,1]}];\n      [n3#2@{logits_n3[3,2]}];\n      [n3#3@{logits_n3[3,3]}];\n    %c3#4%\n      [n3#1@{logits_n3[4,1]}];\n      [n3#2@{logits_n3[4,2]}];\n      [n3#3@{logits_n3[4,3]}];\n  \"),\n\n  OUTPUT = \"svalues; tech11; tech14;\",\n\n  rdata = merged\n)\n\n# Run model\nstep2_lta_fit <- mplusModeler(\n  step2_lta_model,\n  modelout = here(\"tc_lta\",\"lta_enum\", \"step2_lta.inp\"),\n  dataout  = here(\"tc_lta\",\"lta_enum\", \"step2_lta.dat\"),\n  check = TRUE,\n  run = TRUE,\n  hashfilename = FALSE\n)"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"run-step-3-lta-with-covariates","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.11 Run Step 3 LTA with Covariates","text":"Step 3, estimate full LTA model covariates using merged dataset Step 2. step regresses latent class membership class transitions external variables (e.g., gender, race/ethnicity, prior achievement) examine background characteristics predict profile membership change time. classification error accounted Step 2, resulting covariate estimates unbiased reflect true relationships predictors latent trajectories.","code":"\n\n# Build Mplus object\nstep3_lta_model <- mplusObject(\n  TITLE = \"Step 3 LTA with Fixed Logits and Covariates\",\n\n  VARIABLE = \"\n  USEVAR = n1 n2 n3 Female Minority MathG7 MathG10 MathG12;\n  NOMINAL = n1 n2 n3;\n  MISSING = all(9999);\n  CLASSES = c1(4) c2(4) c3(4);\",\n\n  ANALYSIS = \"\n  TYPE = mixture;\n  STARTS = 0;\",\n\n  MODEL = glue(\"\n  %OVERALL%\n  c1 ON Minority Female (c11f c12f c13f) MathG7;\n  c2 ON c1 Minority Female MathG10;\n  c3 ON c2 Minority Female MathG12;\n  \n  MODEL c1:\n    %c1#1%\n      [n1#1@{logits_n1[1,1]}];\n      [n1#2@{logits_n1[1,2]}];\n      [n1#3@{logits_n1[1,3]}];\n    %c1#2%\n      [n1#1@{logits_n1[2,1]}];\n      [n1#2@{logits_n1[2,2]}];\n      [n1#3@{logits_n1[2,3]}];\n    %c1#3%\n      [n1#1@{logits_n1[3,1]}];\n      [n1#2@{logits_n1[3,2]}];\n      [n1#3@{logits_n1[3,3]}];\n    %c1#4%\n      [n1#1@{logits_n1[4,1]}];\n      [n1#2@{logits_n1[4,2]}];\n      [n1#3@{logits_n1[4,3]}];\n\n  MODEL c2:\n    %c2#1%\n      [n2#1@{logits_n2[1,1]}];\n      [n2#2@{logits_n2[1,2]}];\n      [n2#3@{logits_n2[1,3]}];\n    %c2#2%\n      [n2#1@{logits_n2[2,1]}];\n      [n2#2@{logits_n2[2,2]}];\n      [n2#3@{logits_n2[2,3]}];\n    %c2#3%\n      [n2#1@{logits_n2[3,1]}];\n      [n2#2@{logits_n2[3,2]}];\n      [n2#3@{logits_n2[3,3]}];\n    %c2#4%\n      [n2#1@{logits_n2[4,1]}];\n      [n2#2@{logits_n2[4,2]}];\n      [n2#3@{logits_n2[4,3]}];\n\n  MODEL c3:\n    %c3#1%\n      [n3#1@{logits_n3[1,1]}];\n      [n3#2@{logits_n3[1,2]}];\n      [n3#3@{logits_n3[1,3]}];\n    %c3#2%\n      [n3#1@{logits_n3[2,1]}];\n      [n3#2@{logits_n3[2,2]}];\n      [n3#3@{logits_n3[2,3]}];\n    %c3#3%\n      [n3#1@{logits_n3[3,1]}];\n      [n3#2@{logits_n3[3,2]}];\n      [n3#3@{logits_n3[3,3]}];\n    %c3#4%\n      [n3#1@{logits_n3[4,1]}];\n      [n3#2@{logits_n3[4,2]}];\n      [n3#3@{logits_n3[4,3]}];\n  \"),\n\n  OUTPUT = \"svalues; tech11; tech14;\",\n\n  rdata = merged\n)\n\n# Run model\nstep3_lta_fit <- mplusModeler(\n  step3_lta_model,\n  modelout = here(\"tc_lta\",\"lta_enum\", \"step3_lta.inp\"),\n  dataout  = here(\"tc_lta\",\"lta_enum\", \"step3_lta.dat\"),\n  check = TRUE,\n  run = TRUE,\n  hashfilename = FALSE\n)"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"create-transition-probability-plots","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.12 Create Transition Probability Plots","text":"explore students’ attitudinal profiles evolve time, generated transition probability plots using Sankey diagrams. began full sample, created subgroup-specific plots highlight potential differences gender racial/ethnic identity. Specifically, examined transitions female students, male students, female students minoritized backgrounds, female students part racial/ethnic minority. visualizations help illustrate profiles stable notable shifts occur across grades.Re order classes","code":"\n\nmerged <- merged %>%\n  mutate(\n    # Recode N1 (Grade 7)\n    N1_label = case_when(\n      N1 == 1 ~ \"Very Positive\",\n      N1 == 4 ~ \"Qualified Positive\",\n      N1 == 3 ~ \"Neutral\",\n      N1 == 2 ~ \"Less Positive\"\n    ),\n    # Recode N2 (Grade 10)\n    N2_label = case_when(\n      N2 == 1 ~ \"Very Positive\",\n      N2 == 4 ~ \"Qualified Positive\",\n      N2 == 3 ~ \"Neutral\",\n      N2 == 2 ~ \"Less Positive\"\n    ),\n    # Recode N3 (Grade 12)\n    N3_label = case_when(\n      N3 == 1 ~ \"Very Positive\",\n      N3 == 4 ~ \"Qualified Positive\",\n      N3 == 3 ~ \"Neutral\",\n      N3 == 2 ~ \"Less Positive\"\n    ),\n    \n    # Factorize with correct top-down plotting order\n    N1_label = factor(N1_label, levels = c(\n      \"Very Positive\", \"Qualified Positive\", \"Neutral\", \"Less Positive\"\n    )),\n    N2_label = factor(N2_label, levels = c(\n      \"Very Positive\", \"Qualified Positive\", \"Neutral\", \"Less Positive\"\n    )),\n    N3_label = factor(N3_label, levels = c(\n      \"Very Positive\", \"Qualified Positive\", \"Neutral\", \"Less Positive\"\n    ))\n  )"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"examine-transition-probability-plot-for-the-full-sample","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.12.1 Examine Transition Probability Plot for the Full Sample","text":"","code":"\n#### 1. Reload transition function and read model ####\nsource(here(\"tc_lta\", \"functions\", \"plot_transitions_function.R\"))\n\nstep3_full <- readModels(here::here(\"tc_lta\",\"lta_enum\", \"step3_lta.out\"))\n\n#### 2. Define color palette (new correct labels) ####\nclass_colors <- c(\n  \"Very Positive\"       = \"#BA68C8\",\n  \"Qualified Positive\"  = \"#00BCD4\",\n  \"Neutral\"             = \"#8BC34A\",\n  \"Less Positive\"       = \"#E74C3C\"\n)\n\n#### 3. Set correct class names per quadrant (TL → TR → BL → BR) ####\nplot_full <- plot_transitions_function(\n  model_name       = step3_full,\n  color_palette    = class_colors,\n  facet_labels     = c(\n    `1` = \"Very Positive\",\n    `2` = \"Qualified Positive\",\n    `3` = \"Neutral\",\n    `4` = \"Less Positive\"\n  ),\n  timepoint_labels = c(\n    `1` = \"Grade 7\",\n    `2` = \"Grade 10\",\n    `3` = \"Grade 12\"\n  ),\n  class_labels     = names(class_colors)\n)\n\n#### 4. Add single plot title and theme settings ####\nplot_full <- plot_full +\n  ggtitle(\"Attitudinal Profile Transitions: Grade 7 to 10 and Grade 10 to 12\") +\n  theme(\n    strip.text = element_text(\n      family = \"Avenir Next\",\n      size = 11,\n      face = \"plain\"\n    ),\n    plot.title = element_text(\n      family = \"Avenir Next\",\n      hjust = 0.5,\n      size = 14,\n      margin = margin(b = 10)\n    ),\n    text = element_text(family = \"Avenir Next\")\n  )\n\n#### 5. Render plot ####\nplot_full\nggsave(\n  filename = here(\"tc_lta\",\"figures\", \"transitions_full_sample.png\"),\n  plot     = plot_full,\n  width    = 8,\n  height   = 8,\n  dpi      = 300\n)"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"examine-transition-probability-plot-for-females","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.12.2 Examine Transition Probability Plot for Females","text":"Save Female plots","code":"\nsource(here(\"tc_lta\", \"functions\", \"make_transition_object.R\"))\n\nmerged_female <- merged %>%\n  filter(FEMALE == 1)\n\n#### 2. Build Mplus-style transition object from modal assignments ####\nfemale_obj <- make_transition_object(merged_female, \"N1\", \"N2\", \"N3\")\n\n#### 3. Create plot with updated facet labels and font ####\nplot_female <- plot_transitions_function(\n  model_name       = female_obj,\n  color_palette    = class_colors,\n  facet_labels     = c(\n    `1` = \"Very Positive\",\n    `2` = \"Qualified Positive\",\n    `3` = \"Neutral\",\n    `4` = \"Less Positive\"\n  ),\n  timepoint_labels = c(\n    `1` = \"Grade 7\",\n    `2` = \"Grade 10\",\n    `3` = \"Grade 12\"\n  ),\n  class_labels     = names(class_colors)\n) +\n  ggtitle(\"Attitudinal Profile Transitions\\nFemale Students: Grade 7 to 10 and Grade 10 to 12\") +\n  theme(\n    strip.text = element_text(\n      family = \"Avenir Next\",\n      size = 11,\n      face = \"plain\"\n    ),\n    plot.title = element_text(\n      family = \"Avenir Next\",\n      hjust = 0.5,\n      size = 14,\n      margin = margin(b = 10)\n    ),\n    text = element_text(family = \"Avenir Next\")\n  )\n\n#### 4. Display plot ####\nplot_female\nggsave(\n  filename = here(\"tc_lta\",\"figures\", \"transitions_female.png\"),\n  plot     = plot_female,\n  width    = 8,\n  height   = 8,\n  dpi      = 300\n)"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"examine-transition-probability-plot-for-males","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.12.3 Examine Transition Probability Plot for Males","text":"Save plots males","code":"\nmerged_male <- merged %>%\n  filter(FEMALE == 0)\n\n#### 2. Build Mplus-style transition object from modal assignments ####\nmale_obj <- make_transition_object(merged_male, \"N1\", \"N2\", \"N3\")\n\n#### 3. Create plot with updated facet labels and font ####\nplot_male <- plot_transitions_function(\n  model_name       = male_obj,\n  color_palette    = class_colors,\n  facet_labels     = c(\n    `1` = \"Very Positive\",\n    `2` = \"Qualified Positive\",\n    `3` = \"Neutral\",\n    `4` = \"Less Positive\"\n  ),\n  timepoint_labels = c(\n    `1` = \"Grade 7\",\n    `2` = \"Grade 10\",\n    `3` = \"Grade 12\"\n  ),\n  class_labels     = names(class_colors)\n) +\n  ggtitle(\"Attitudinal Profile Transitions\\nMale Students: Grade 7 to 10 and Grade 10 to 12\") +\n  theme(\n    strip.text = element_text(\n      family = \"Avenir Next\",\n      size = 11,\n      face = \"plain\"\n    ),\n    plot.title = element_text(\n      family = \"Avenir Next\",\n      hjust = 0.5,\n      size = 14,\n      margin = margin(b = 10)\n    ),\n    text = element_text(family = \"Avenir Next\")\n  )\n\n#### 4. Display plot ####\nplot_male\n\nggsave(\n  filename = here(\"tc_lta\",\"figures\", \"transitions_male.png\"),\n  plot     = plot_male,\n  width    = 8,\n  height   = 8,\n  dpi      = 300\n)"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"examine-transition-probability-plot-for-underrepresented-females","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.12.4 Examine Transition Probability Plot for Underrepresented Females","text":"Save plot minority females","code":"\nmerged_female_minority <- merged %>%\n  filter(FEMALE == 1, MINORITY == 1)\n\n#### 2. Build Mplus-style transition object from modal assignments ####\nfemale_minority_obj <- make_transition_object(merged_female_minority, \"N1\", \"N2\", \"N3\")\n\n#### 3. Assign plot to object with correct facet labels and custom font ####\nplot_female_minority <- plot_transitions_function(\n  model_name       = female_minority_obj,\n  color_palette    = class_colors,\n  facet_labels     = c(\n    `1` = \"Very Positive\",\n    `2` = \"Qualified Positive\",\n    `3` = \"Neutral\",\n    `4` = \"Less Positive\"\n  ),\n  timepoint_labels = c(\n    `1` = \"Grade 7\",\n    `2` = \"Grade 10\",\n    `3` = \"Grade 12\"\n  ),\n  class_labels     = names(class_colors)\n) +\n  ggtitle(\"Attitudinal Profile Transitions\\nFemale Minority Students: Grade 7 to 10 and Grade 10 to 12\") +\n  theme(\n    strip.text = element_text(\n      family = \"Avenir Next\",\n      size = 11,\n      face = \"plain\"\n    ),\n    plot.title = element_text(\n      family = \"Avenir Next\",\n      hjust = 0.5,\n      size = 14,\n      margin = margin(b = 10)\n    ),\n    text = element_text(family = \"Avenir Next\")\n  )\n\n#### 4. Display plot ####\nplot_female_minority\n\nggsave(\n  filename = here(\"tc_lta\",\"figures\", \"transitions_female_minority.png\"),\n  plot     = plot_female_minority,\n  width    = 8,\n  height   = 8,\n  dpi      = 300\n)"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"examine-transition-probability-plot-for-non-underrepresented-females","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.12.5 Examine Transition Probability Plot for Non-Underrepresented Females","text":"Save plots non underrepresented females","code":"\nmerged_female_nonminority <- merged %>%\n  filter(FEMALE == 1, MINORITY == 0)\n\n#### 2. Build Mplus-style transition object from modal assignments ####\nfemale_nonminority_obj <- make_transition_object(merged_female_nonminority, \"N1\", \"N2\", \"N3\")\n\n#### 3. Assign plot to object with labeled facets and Avenir Next font ####\nplot_female_nonminority <- plot_transitions_function(\n  model_name       = female_nonminority_obj,\n  color_palette    = class_colors,\n  facet_labels     = c(\n    `1` = \"Very Positive\",\n    `2` = \"Qualified Positive\",\n    `3` = \"Neutral\",\n    `4` = \"Less Positive\"\n  ),\n  timepoint_labels = c(\n    `1` = \"Grade 7\",\n    `2` = \"Grade 10\",\n    `3` = \"Grade 12\"\n  ),\n  class_labels     = names(class_colors)\n) +\n  ggtitle(\"Attitudinal Profile Transitions\\nNon-Minority Female Students: Grade 7 to 10 and Grade 10 to 12\") +\n  theme(\n    strip.text = element_text(\n      family = \"Avenir Next\",\n      size = 11,\n      face = \"plain\"\n    ),\n    plot.title = element_text(\n      family = \"Avenir Next\",\n      hjust = 0.5,\n      size = 14,\n      margin = margin(b = 10)\n    ),\n    text = element_text(family = \"Avenir Next\")\n  )\n\n#### 4. Display plot ####\nplot_female_nonminority\nggsave(\n  filename = here(\"tc_lta\",\"figures\", \"transitions_female_nonminority.png\"),\n  plot     = plot_female_nonminority,\n  width    = 8,\n  height   = 8,\n  dpi      = 300\n)"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"examine-table-of-covariate-results-for-the-step-3-lta","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.12.6 Examine table of covariate results for the Step 3 LTA","text":"next examine covariate results Step 3 LTA, model student characteristics predict profile membership transitions profiles time. table reports logit estimates, standard errors, odds ratios confidence intervals covariate effect. results allow us identify background factors significantly associated students’ starting profile Grade 7, factors relate likelihood transitioning different profiles across timepoints.Save covariate table","code":"\n# Define class labels\nclass_names <- c(\n  \"1\" = \"Low Belonging\",\n  \"2\" = \"Moderate Belonging\",\n  \"3\" = \"High Support\",\n  \"4\" = \"Engaged & Connected\"\n)\n\n# Build covariate comparison data\ncovariate_data <- step3_full$parameters$unstandardized %>%\n  filter(\n    str_detect(paramHeader, \"^C[1-3]#\\\\d+\\\\.ON$\"),\n    param %in% c(\"MINORITY\", \"FEMALE\", \"MATHG7\", \"MATHG10\", \"MATHG12\")\n  ) %>%\n  mutate(\n    LatentClass = str_extract(paramHeader, \"C[1-3]#\\\\d+\"),\n    ClassNum = str_extract(LatentClass, \"#\\\\d+\") %>% str_remove(\"#\"),\n    ClassName = class_names[ClassNum],\n    RefName = class_names[\"4\"],\n    Comparison = glue(\"{ClassName} vs. {RefName}\"),\n    Timepoint = case_when(\n      str_detect(LatentClass, \"C1\") ~ \"Grade 7\",\n      str_detect(LatentClass, \"C2\") ~ \"Grade 10\",\n      str_detect(LatentClass, \"C3\") ~ \"Grade 12\"\n    ),\n    Timepoint = factor(Timepoint, levels = c(\"Grade 7\", \"Grade 10\", \"Grade 12\")),\n    OR = exp(est),\n    OR_lower = exp(est - 1.96 * se),\n    OR_upper = exp(est + 1.96 * se),\n    OR_CI = sprintf(\"%.2f [%.2f, %.2f]\", OR, OR_lower, OR_upper),\n    Logit = sprintf(\"%.2f\", est),\n    SE = sprintf(\"%.2f\", se),\n    pval_fmt = ifelse(pval < .001, \"<.001\", sprintf(\"%.3f\", pval)),\n    pval_num = ifelse(pval < .001, 0.0009, pval)  # for styling logic\n  ) %>%\n  arrange(Timepoint, ClassNum) %>%\n  mutate(RowGroup = glue(\"{Timepoint}: {Comparison}\")) %>%\n  select(RowGroup, Covariate = param, Logit, SE, `Odds Ratio [95% CI]` = OR_CI, `p-value` = pval_fmt, pval_num)\n\n# Build gt table\ncovariate_table <- covariate_data %>%\n  select(-pval_num) %>%\n  gt(groupname_col = \"RowGroup\") %>%\n  tab_header(\n    title = \"Step 3 LTA Covariate Effects\",\n    subtitle = \"Log Odds of Class Membership Relative to Engaged & Connected\"\n  ) %>%\n  cols_label(\n    Covariate = \"Covariate\",\n    Logit = \"Logit\",\n    SE = \"SE\",\n    `Odds Ratio [95% CI]` = \"Odds Ratio [95% CI]\",\n    `p-value` = \"p-value\"\n  ) %>%\n  tab_style(\n    style = list(\n      cell_text(weight = \"bold\", style = \"italic\")\n    ),\n    locations = cells_row_groups()\n  ) %>%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_body(\n      columns = vars(`p-value`),\n      rows = covariate_data$pval_num < 0.05\n    )\n  )\n\n\ncovariate_table\ngtsave(\n  data = covariate_table,\n  filename = here(\"tc_lta\",\"figures\", \"step3_covariate_table.png\")\n)"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"examine-outcome-variables","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.13 Examine Outcome Variables","text":"modern three-step LTA workflow, distal outcomes achievement typically incorporated directly third step using model constraints. However, line original paper, replicate approach examining achievement outcomes post hoc using ANOVAs. Specifically, test whether 12th-grade mathematics science achievement differ students’ final attitudinal profile. allows us assess external validity profiles linking meaningful academic indicators.","code":"\n# 1. Prepare variables\nmerged <- merged %>%\n  mutate(\n    traj_final = factor(N3, levels = 1:4),\n    stable_profile = (N1 == N2 & N2 == N3)\n  )\n\n# 2. Group means and SDs, exclude 9999 and NA in traj_final\ndistal_summary <- merged %>%\n  filter(\n    MATHG11 != 9999,\n    SCIG11 != 9999,\n    !is.na(traj_final)\n  ) %>%\n  group_by(traj_final) %>%\n  summarise(\n    n = n(),\n    math_mean = mean(MATHG11),\n    math_sd   = sd(MATHG11),\n    sci_mean  = mean(SCIG11),\n    sci_sd    = sd(SCIG11),\n    .groups = \"drop\"\n  )\n\n# 3. Run ANOVAs (excluding 9999)\naov_math <- aov(MATHG11 ~ traj_final, data = merged %>% filter(MATHG11 != 9999))\naov_sci  <- aov(SCIG11  ~ traj_final, data = merged %>% filter(SCIG11 != 9999))\n\n# 4. Extract test stats\nf_math <- summary(aov_math)[[1]]$`F value`[1]\np_math <- summary(aov_math)[[1]]$`Pr(>F)`[1]\ndf_math1 <- summary(aov_math)[[1]]$Df[1]\ndf_math2 <- summary(aov_math)[[1]]$Df[2]\n\nf_sci <- summary(aov_sci)[[1]]$`F value`[1]\np_sci <- summary(aov_sci)[[1]]$`Pr(>F)`[1]\ndf_sci1 <- summary(aov_sci)[[1]]$Df[1]\ndf_sci2 <- summary(aov_sci)[[1]]$Df[2]\n\n# 5. Format table\ngt(distal_summary) %>%\n  tab_header(title = \"12th Grade Achievement by Final Attitudinal Profile\") %>%\n  fmt_number(columns = ends_with(\"mean\"), decimals = 2) %>%\n  fmt_number(columns = ends_with(\"sd\"), decimals = 2) %>%\n  fmt_number(columns = \"n\", decimals = 0) %>%\n  cols_label(\n    traj_final = \"Final Profile\",\n    n = \"N\",\n    math_mean = \"Math M\",\n    math_sd   = \"Math SD\",\n    sci_mean  = \"Science M\",\n    sci_sd    = \"Science SD\"\n  ) %>%\n  tab_source_note(\n    source_note = glue::glue(\n      \"ANOVA — Math: F({df_math1}, {df_math2}) = {round(f_math, 2)}, p = {format.pval(p_math, digits = 3)}; \",\n      \"Science: F({df_sci1}, {df_sci2}) = {round(f_sci, 2)}, p = {format.pval(p_sci, digits = 3)}\"\n    )\n  )\n# 1. Class label mapping\nprofile_labels <- c(\n  \"1\" = \"Supportive\",\n  \"2\" = \"Disconnected\",\n  \"3\" = \"Moderate\",\n  \"4\" = \"Negative\"\n)\n\n# 2. Clean and label profiles\ndf <- merged %>%\n  filter(MATHG11 != 9999, SCIG11 != 9999) %>%\n  mutate(profile = factor(N3, levels = 1:4))\n\n# 3. ANOVA + Tukey — Math\naov_math <- aov(MATHG11 ~ profile, data = df)\ntukey_math <- TukeyHSD(aov_math)$profile\ntukey_math_df <- as.data.frame(tukey_math) %>%\n  tibble::rownames_to_column(\"Comparison\") %>%\n  mutate(\n    Outcome = \"Math\",\n    Group1 = str_match(Comparison, \"^(\\\\d+)-(\\\\d+)$\")[,2],\n    Group2 = str_match(Comparison, \"^(\\\\d+)-(\\\\d+)$\")[,3],\n    Comparison = glue(\"{profile_labels[Group2]} vs. {profile_labels[Group1]}\")\n  ) %>%\n  select(Outcome, Comparison, diff, lwr, upr, `p adj`) %>%\n  rename(\n    `Mean Difference` = diff,\n    `Lower CI`        = lwr,\n    `Upper CI`        = upr,\n    `Adjusted p`      = `p adj`\n  )\n\n# 4. ANOVA + Tukey — Science\naov_sci <- aov(SCIG11 ~ profile, data = df)\ntukey_sci <- TukeyHSD(aov_sci)$profile\ntukey_sci_df <- as.data.frame(tukey_sci) %>%\n  tibble::rownames_to_column(\"Comparison\") %>%\n  mutate(\n    Outcome = \"Science\",\n    Group1 = str_match(Comparison, \"^(\\\\d+)-(\\\\d+)$\")[,2],\n    Group2 = str_match(Comparison, \"^(\\\\d+)-(\\\\d+)$\")[,3],\n    Comparison = glue(\"{profile_labels[Group2]} vs. {profile_labels[Group1]}\")\n  ) %>%\n  select(Outcome, Comparison, diff, lwr, upr, `p adj`) %>%\n  rename(\n    `Mean Difference` = diff,\n    `Lower CI`        = lwr,\n    `Upper CI`        = upr,\n    `Adjusted p`      = `p adj`\n  )\n\n# 5. F values\nmath_f <- summary(aov_math)[[1]]\nsci_f  <- summary(aov_sci)[[1]]\n\n# 6. Build Math table\ngt_math_table <- gt(tukey_math_df) %>%\n  tab_header(title = \"Pairwise Comparisons for 12th Grade Math Achievement\") %>%\n  fmt_number(columns = c(\"Mean Difference\", \"Lower CI\", \"Upper CI\"), decimals = 2) %>%\n  text_transform(\n    locations = cells_body(columns = \"Adjusted p\"),\n    fn = function(x) ifelse(as.numeric(x) < 0.001, \"<.001\", formatC(as.numeric(x), format = \"f\", digits = 3))\n  ) %>%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_body(columns = \"Adjusted p\", rows = as.numeric(`Adjusted p`) < 0.05)\n  ) %>%\n  tab_source_note(source_note = md(glue(\n    \"*F*({math_f$Df[1]}, {math_f$Df[2]}) = {round(math_f$`F value`[1], 2)}, *p* = {format.pval(math_f$`Pr(>F)`[1], digits = 3, eps = .001)}\"\n  )))\n\n# 7. Build Science table\ngt_sci_table <- gt(tukey_sci_df) %>%\n  tab_header(title = \"Pairwise Comparisons for 12th Grade Science Achievement\") %>%\n  fmt_number(columns = c(\"Mean Difference\", \"Lower CI\", \"Upper CI\"), decimals = 2) %>%\n  text_transform(\n    locations = cells_body(columns = \"Adjusted p\"),\n    fn = function(x) ifelse(as.numeric(x) < 0.001, \"<.001\", formatC(as.numeric(x), format = \"f\", digits = 3))\n  ) %>%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_body(columns = \"Adjusted p\", rows = as.numeric(`Adjusted p`) < 0.05)\n  ) %>%\n  tab_source_note(source_note = md(glue(\n    \"*F*({sci_f$Df[1]}, {sci_f$Df[2]}) = {round(sci_f$`F value`[1], 2)}, *p* = {format.pval(sci_f$`Pr(>F)`[1], digits = 3, eps = .001)}\"\n  )))\n\n# 8. Render both tables\ngt_math_table\ngt_sci_table"},{"path":"the-importance-of-early-attitudes-toward-mathematics-and-science-ing-nylund-gibson-2017.html","id":"gender-differences-in-transition-probabilities","chapter":"19 The Importance of Early Attitudes Toward Mathematics and Science (Ing & Nylund-Gibson, 2017)","heading":"19.13.1 Gender Differences in Transition Probabilities","text":"test gender differences attitudinal profile transitions, computed pairwise z-tests comparing proportion female male students transitioning pair latent classes Grade 7 Grade 10. table presents cell counts, transition percentages gender, two-tailed z-tests equality proportions. Statistically significant differences (p < .05) bolded highlight transitions gender-based patterns emerge","code":"\n\n# Step 1: Drop invalids and label properly\ngender_data <- merged %>%\n  filter(N1 %in% 1:4, N2 %in% 1:4, FEMALE %in% 0:1) %>%\n  mutate(\n    N1 = factor(N1, levels = 1:4),\n    N2 = factor(N2, levels = 1:4),\n    FEMALE = factor(FEMALE, levels = 0:1, labels = c(\"MALE\", \"FEMALE\"))\n  )\n\n# Step 2: Count transitions\ngender_counts <- gender_data %>%\n  count(N1, N2, FEMALE) %>%\n  tidyr::pivot_wider(\n    names_from = FEMALE,\n    values_from = n,\n    values_fill = 0\n  ) %>%\n  rename(n_FEMALE = FEMALE, n_MALE = MALE) %>%\n  mutate(\n    p_FEMALE = n_FEMALE / sum(n_FEMALE + n_MALE),\n    p_MALE   = n_MALE / sum(n_FEMALE + n_MALE)\n  )\n\n# Step 3: z-tests\ngender_counts <- gender_counts %>%\n  mutate(\n    p1 = n_FEMALE / (n_FEMALE + n_MALE),\n    p2 = n_MALE / (n_FEMALE + n_MALE),\n    n1 = n_FEMALE + n_MALE,\n    se = sqrt((p1 * (1 - p1) / n1) + (p2 * (1 - p2) / n1)),\n    z = (p1 - p2) / se,\n    p_val_raw = 2 * (1 - pnorm(abs(z)))\n  )\n\n# Step 4: Recode class names directly as factors for clean display\ngender_counts <- gender_counts %>%\n  mutate(\n    N1 = fct_recode(N1,\n      \"Disengaged\"     = \"1\",\n      \"Moderate\"       = \"2\",\n      \"Positive\"       = \"3\",\n      \"Very Positive\"  = \"4\"\n    ),\n    N2 = fct_recode(N2,\n      \"Disengaged\"     = \"1\",\n      \"Moderate\"       = \"2\",\n      \"Positive\"       = \"3\",\n      \"Very Positive\"  = \"4\"\n    )\n  )\n\n# Step 5: Final display formatting\ngender_transition_table <- gender_counts %>%\n  mutate(\n    `Female %` = round(p1 * 100, 1),\n    `Male %`   = round(p2 * 100, 1),\n    `p-value`  = ifelse(p_val_raw < 0.001, \"<.001\", sub(\"^0\", \"\", sprintf(\"%.3f\", p_val_raw)))\n  ) %>%\n  select(\n    `From` = N1, `To` = N2,\n    n_FEMALE, n_MALE,\n    `Female %`, `Male %`,\n    z, `p-value`, p_val_raw\n  )\n\n# Step 6: Create gt table with italicized labels and bold significant p-values\ngt_gender_ztests <- gender_transition_table %>%\n  gt() %>%\n  tab_header(title = \"Z-Tests for Gender Differences in Transition Probabilities\") %>%\n  fmt_number(columns = c(\"z\"), decimals = 2) %>%\n  cols_label(\n    n_FEMALE = md(\"*n* (Female)\"),\n    n_MALE   = md(\"*n* (Male)\"),\n    z        = md(\"*z*\"),\n    `p-value` = md(\"*p*-value\")\n  ) %>%\n  opt_row_striping() %>%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_body(\n      columns = vars(`p-value`),\n      rows = p_val_raw < 0.05\n    )\n  ) %>%\n  cols_hide(columns = p_val_raw)\n\n# View table\ngt_gender_ztests"},{"path":"references-1.html","id":"references-1","chapter":"References","heading":"References","text":"","code":""}]
