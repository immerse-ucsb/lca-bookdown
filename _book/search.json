[{"path":"index.html","id":"mixture-modeling-with-mplusautomation","chapter":"Mixture Modeling with MplusAutomation","heading":"Mixture Modeling with MplusAutomation","text":" Welcome! collection resources teach apply mixture modeling using Mplus1 MplusAutomation2! resources serve comprehensive guide understanding applying LCA using Mplus automation capabilities MplusAutomation. , learn start finish apply mixture modeling using Mplus MplusAutomation package.","code":""},{"path":"index.html","id":"stay-in-touch","chapter":"Mixture Modeling with MplusAutomation","heading":"Stay in touch!","text":"Please visit website learn IMMERSE fellowship.Please visit website learn IMMERSE fellowship.Visit GitHub account access IMMERSE training materials.Visit GitHub account access IMMERSE training materials.Follow us BlueSky X stay--date fellowship!Follow us BlueSky X stay--date fellowship!","code":""},{"path":"index.html","id":"acknowledgements","chapter":"Mixture Modeling with MplusAutomation","heading":"Acknowledgements","text":"Institute Mixture Modeling Equity-Oriented Researchers, Scholars, Educators (IMMERSE) IES funded training grant (R305B220021) support education scholars integrating mixture modeling research.reference workshop: Institute Mixture Modeling Equity-Oriented Researchers, Scholars, Educators (2025).\nIMMERSE Online Resources (IES . 305B220021).\nInstitute Education Sciences.\nhttps://mixture-modeling.netlify.app/","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"introduction-to-r-and-rstudio","chapter":"1 Introduction to R and RStudio","heading":"1 Introduction to R and RStudio","text":"walkthrough presented IMMERSE team go common tasks carried R.\nmany free resources available get started R RStudio.\nOne favorites R Data Science.R3 free, open-source programming language environment widely used statistical computing, data analysis, data visualization.R3 free, open-source programming language environment widely used statistical computing, data analysis, data visualization.RStudio4 integrated development environment (IDE) R, providing intuitive interface makes coding, visualization, project management accessible.RStudio4 integrated development environment (IDE) R, providing intuitive interface makes coding, visualization, project management accessible.Mplus5 statistical modeling program used analyzing complex data, latent variable models, structural equation modeling, growth modeling.\nbook uses R package called MplusAutomation automate process running models, extracting results, generating data visualizations.Mplus5 statistical modeling program used analyzing complex data, latent variable models, structural equation modeling, growth modeling.\nbook uses R package called MplusAutomation automate process running models, extracting results, generating data visualizations.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"installation","chapter":"1 Introduction to R and RStudio","heading":"1.1 Installation","text":"","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"step-0-install-r-rstudio-and-mplus","chapter":"1 Introduction to R and RStudio","heading":"1.1.1 Step 0: Install R, RStudio, and Mplus","text":"find guide installing R R Studio.\ncan also install Mplus .Note: installation Mplus requires paid license mixture add-.\nIMMERSE fellows given copy Mplus use one year training.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"set-up","chapter":"1 Introduction to R and RStudio","heading":"1.2 Set-up","text":"","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"step-1-create-a-new-r-project-in-rstudio","chapter":"1 Introduction to R and RStudio","heading":"1.2.1 Step 1: Create a new R-project in RStudio","text":"R-projects help us organize folders , filepaths, scripts.\ncreate new R project:File –> New Project…Click “New Directory” –> New Project –> Name project","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"step-2-create-an-r-markdown-document","chapter":"1 Introduction to R and RStudio","heading":"1.2.2 Step 2: Create an R-markdown document","text":"R-markdown file provides authoring framework data science allows us organize reports using texts code chunks.\ndocument reading made using R-markdown!create R-markdown:File –> New File –> R Markdown…window pops , give R-markdown title “Introduction R RStudio” Click “OK.” see new markdown example text code chunks.\nwant clean document start delete everything line 10 .\nGo ahead save document R Project folder.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"step-3-load-packages","chapter":"1 Introduction to R and RStudio","heading":"1.2.3 Step 3: Load packages","text":"first code chunk given markdown packages using.\ninsert code chunk, etiher use keyboard shortcut ctrl + alt + Code –> Insert Chunk click green box letter C .\npackages want markdown read :reminder, function work receive error like : find function \"random_function\"; try load package receive error like : package called `random_package` , need install package using install.packages(\"random_package\") console (bottom-left window R studio).\ninstalled package never need install , however must always load packages beginning R markdown using library(random_package), shown document.style code package using called tidyverse6 .\nfunctions within tidyverse package , ’ve indicated packages used code chunk .","code":"\nlibrary(psych) # describe()\nlibrary(here) #helps with filepaths\nlibrary(gt) # create tables\nlibrary(tidyverse) #collection of R packages designed for data science"},{"path":"introduction-to-r-and-rstudio.html","id":"explore-the-data","chapter":"1 Introduction to R and RStudio","heading":"1.3 Explore the data","text":"","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"step-4-read-in-data","chapter":"1 Introduction to R and RStudio","heading":"1.3.1 Step 4: Read in data","text":"demonstrate mixture modeling training program online resource components IES grant utilize Civil Rights Data Collection (CRDC) (CRDC) data repository.\nCRDC federally mandated school-level data collection effort occurs every year.\npublic data currently available selected latent class indicators across 4 years (2011, 2013, 2015, 2017) US states.\nexample, use Arizona state sample.\nutilize six focal indicators constitute latent class model example; three variables report harassment/bullying schools based disability, race, sex, three variables full-time equivalent school staff hires (counselor, psychologist, law enforcement).\ndata source also includes covariates variety subjects distal outcomes reported 2018 math/reading assessments graduation rates.read data R:Ways view data R:click data Global Environment (upper right pane) use…summary() gives basic summary statistics & shows number NA values (great checking data read correctly)names() provides list column names. useful don’t memorized!head() prints top 6 rows dataframe","code":"\ndata <- read_csv(here(\"data\", \"crdc_lca_data.csv\")) \nView(data)\nsummary(data)\n#>     leaid             ncessch            report_dis    \n#>  Length:2027        Length:2027        Min.   :0.0000  \n#>  Class :character   Class :character   1st Qu.:0.0000  \n#>  Mode  :character   Mode  :character   Median :0.0000  \n#>                                        Mean   :0.0425  \n#>                                        3rd Qu.:0.0000  \n#>                                        Max.   :1.0000  \n#>                                        NA's   :27      \n#>   report_race      report_sex   counselors_fte  \n#>  Min.   :0.000   Min.   :0.00   Min.   :0.0000  \n#>  1st Qu.:0.000   1st Qu.:0.00   1st Qu.:0.0000  \n#>  Median :0.000   Median :0.00   Median :0.0000  \n#>  Mean   :0.103   Mean   :0.17   Mean   :0.4595  \n#>  3rd Qu.:0.000   3rd Qu.:0.00   3rd Qu.:1.0000  \n#>  Max.   :1.000   Max.   :1.00   Max.   :1.0000  \n#>  NA's   :27      NA's   :27     NA's   :27      \n#>    psych_fte         law_fte      \n#>  Min.   :0.0000   Min.   :0.0000  \n#>  1st Qu.:0.0000   1st Qu.:0.0000  \n#>  Median :0.0000   Median :0.0000  \n#>  Mean   :0.4742   Mean   :0.1255  \n#>  3rd Qu.:1.0000   3rd Qu.:0.0000  \n#>  Max.   :1.0000   Max.   :1.0000  \n#>  NA's   :30       NA's   :27\nnames(data)\n#> [1] \"leaid\"          \"ncessch\"        \"report_dis\"    \n#> [4] \"report_race\"    \"report_sex\"     \"counselors_fte\"\n#> [7] \"psych_fte\"      \"law_fte\"\nhead(data)\n#> # A tibble: 6 × 8\n#>   leaid   ncessch      report_dis report_race report_sex\n#>   <chr>   <chr>             <dbl>       <dbl>      <dbl>\n#> 1 0400001 040000100120          0           0          0\n#> 2 0400001 040000100616          0           0          1\n#> 3 0400001 040000101204          0           0          1\n#> 4 0400001 040000101871          0           1          1\n#> 5 0400001 040000101872          0           0          0\n#> 6 0400001 040000102344          0           0          0\n#> # ℹ 3 more variables: counselors_fte <dbl>,\n#> #   psych_fte <dbl>, law_fte <dbl>"},{"path":"introduction-to-r-and-rstudio.html","id":"step-5-descriptive-statistics","chapter":"1 Introduction to R and RStudio","heading":"1.3.2 Step 5: Descriptive Statistics","text":"Let’s look descriptive statistics variable.\nlooking ID variables’ (leaid) (necessch) descriptives unnecessary, use select() remove variable using minus (-) sign:Alternatively, can use psych::describe() function give information:want look subset data?\nexample, want subset data observe specific school district?\n(leaid) can use tidyverse::filter() subset data using certain criteria.Since binary data (0,1), helpful look proportions:","code":"\ndata %>% \n  select(-leaid, -ncessch) %>% \n  summary()\n#>    report_dis      report_race      report_sex  \n#>  Min.   :0.0000   Min.   :0.000   Min.   :0.00  \n#>  1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.00  \n#>  Median :0.0000   Median :0.000   Median :0.00  \n#>  Mean   :0.0425   Mean   :0.103   Mean   :0.17  \n#>  3rd Qu.:0.0000   3rd Qu.:0.000   3rd Qu.:0.00  \n#>  Max.   :1.0000   Max.   :1.000   Max.   :1.00  \n#>  NA's   :27       NA's   :27      NA's   :27    \n#>  counselors_fte     psych_fte         law_fte      \n#>  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n#>  1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n#>  Median :0.0000   Median :0.0000   Median :0.0000  \n#>  Mean   :0.4595   Mean   :0.4742   Mean   :0.1255  \n#>  3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n#>  Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n#>  NA's   :27       NA's   :30       NA's   :27\ndata %>% \n  select(-leaid, -ncessch) %>% \n  describe()\n#>                vars    n mean   sd median trimmed mad min\n#> report_dis        1 2000 0.04 0.20      0    0.00   0   0\n#> report_race       2 2000 0.10 0.30      0    0.00   0   0\n#> report_sex        3 2000 0.17 0.38      0    0.09   0   0\n#> counselors_fte    4 2000 0.46 0.50      0    0.45   0   0\n#> psych_fte         5 1997 0.47 0.50      0    0.47   0   0\n#> law_fte           6 2000 0.13 0.33      0    0.03   0   0\n#>                max range skew kurtosis   se\n#> report_dis       1     1 4.53    18.55 0.00\n#> report_race      1     1 2.61     4.82 0.01\n#> report_sex       1     1 1.76     1.08 0.01\n#> counselors_fte   1     1 0.16    -1.97 0.01\n#> psych_fte        1     1 0.10    -1.99 0.01\n#> law_fte          1     1 2.26     3.11 0.01\ndata %>% \n  filter(leaid == \"0408800\") %>% \n  describe() \n#>                vars  n  mean    sd median trimmed   mad min\n#> leaid*            1 86  1.00  0.00    1.0    1.00  0.00   1\n#> ncessch*          2 86 43.50 24.97   43.5   43.50 31.88   1\n#> report_dis        3 86  0.05  0.21    0.0    0.00  0.00   0\n#> report_race       4 86  0.15  0.36    0.0    0.07  0.00   0\n#> report_sex        5 86  0.19  0.39    0.0    0.11  0.00   0\n#> counselors_fte    6 86  0.95  0.21    1.0    1.00  0.00   0\n#> psych_fte         7 86  0.19  0.39    0.0    0.11  0.00   0\n#> law_fte           8 86  0.14  0.35    0.0    0.06  0.00   0\n#>                max range  skew kurtosis   se\n#> leaid*           1     0   NaN      NaN 0.00\n#> ncessch*        86    85  0.00    -1.24 2.69\n#> report_dis       1     1  4.23    16.10 0.02\n#> report_race      1     1  1.91     1.68 0.04\n#> report_sex       1     1  1.59     0.52 0.04\n#> counselors_fte   1     1 -4.23    16.10 0.02\n#> psych_fte        1     1  1.59     0.52 0.04\n#> law_fte          1     1  2.04     2.21 0.04\n\n\n#You can use any operator to filter: >, <, ==, >=, etc.\ndata %>% \n  drop_na() %>% \n  pivot_longer(report_dis:law_fte, names_to = \"variable\") %>% \n  group_by(variable) %>% \n  summarise(prop = sum(value)/n(),\n            n = n()) %>%\n  arrange(desc(prop))\n#> # A tibble: 6 × 3\n#>   variable         prop     n\n#>   <chr>           <dbl> <int>\n#> 1 psych_fte      0.481   1970\n#> 2 counselors_fte 0.459   1970\n#> 3 report_sex     0.173   1970\n#> 4 law_fte        0.127   1970\n#> 5 report_race    0.105   1970\n#> 6 report_dis     0.0431  1970"},{"path":"introduction-to-mplusautomation.html","id":"introduction-to-mplusautomation","chapter":"2 Introduction to MplusAutomation","heading":"2 Introduction to MplusAutomation","text":"MplusAutomation7 designed streamline use Mplus, powerful statistical software modeling complex data developed Muthen Muten (https://www.statmodel.com). MplusAutomation, researchers can automate process estimating latent variable models, running batches models, extracting results, generating data visualizations - within R environment.?MplusAutomation R packageIt “wraps around” Mplus programRequires R & Mplus softwareRequires learning basics 2 programming languagesCar metaphor: R/Rstudio steering wheel dashboard & Mplus engineWHY?MplusAutomation can provide clearly organized work procedures every research decision can documented single placeIncrease reproducibility, organization, efficiency, transparencyHOW?interface MplusAutomation entirely within R-Studio. need open MplusThe code presented repetitive designBelow template mplusObject() & mplusModeler() functions. Use template run statistical models Mplus.","code":"\nm_template  <- mplusObject(\n  \n  TITLE = \n    \"\", \n  \n  VARIABLE = \n    \"\",\n  \n  ANALYSIS = \n    \"\",\n  \n  PLOT = \n    \"\",\n  \n  OUTPUT = \n    \"\",\n \n  usevariables = colnames(), \n  rdata =  )\n\nm_template_fit <- mplusModeler(m_template, \n                  dataout=here(\"\", \".dat\"),\n                  modelout=here(\"\", \".inp\"),\n                  check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"lca-enumeration.html","id":"lca-enumeration","chapter":"3 LCA Enumeration","heading":"3 LCA Enumeration","text":"Example: Bullying SchoolsTo demonstrate mixture modeling training program online resource components IES grant utilize Civil Rights Data Collection (CRDC)8 data repository.\nCRDC federally mandated school-level data collection effort occurs every year.\npublic data currently available selected latent class indicators across 4 years (2011, 2013, 2015, 2017) US states.\nexample, use Arizona state sample.\nutilize six focal indicators constitute latent class model example; three variables report harassment/bullying schools based disability, race, sex, three variables full-time equivalent school staff hires (counselor, psychologist, law enforcement).\ndata source also includes covariates variety subjects distal outcomes reported 2018 math/reading assessments graduation rates.Load packages","code":"\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(glue)\nlibrary(MplusAutomation)\nlibrary(here)\nlibrary(janitor)\nlibrary(gt)\nlibrary(cowplot)\nlibrary(DiagrammeR) "},{"path":"lca-enumeration.html","id":"variable-description","chapter":"3 LCA Enumeration","heading":"3.1 Variable Description","text":"Variables transformed dichotomous indicators using following coding strategyHarassment bullying count variables recoded 1 school reported least one incident harassment (0 indicates reported incidents).\noriginal scale reported CDRC staff variables full time equivalent employees (FTE) represented 1 part time employees represented values 1 0.\nSchools greater one staff designated type represented values greater 1.\nvalues greater zero recorded 1s (e.g., .5, 1,3) indicating school staff present campus least part time.\nSchools staff designated type indicated 0 dichotomous variable.","code":""},{"path":"lca-enumeration.html","id":"prepare-data","chapter":"3 LCA Enumeration","heading":"3.2 Prepare Data","text":"","code":"\ndf_bully <- read_csv(here(\"data\", \"crdc_lca_data.csv\")) %>% \n  clean_names() %>% \n  dplyr::select(report_dis, report_race, report_sex, counselors_fte, psych_fte, law_fte) "},{"path":"lca-enumeration.html","id":"descriptive-statistics","chapter":"3 LCA Enumeration","heading":"3.3 Descriptive Statistics","text":"Save image","code":"\n# Set up data to find proportions of binary indicators\nds <- df_bully %>% \n  pivot_longer(c(report_dis, report_race, report_sex, counselors_fte, psych_fte, law_fte), names_to = \"variable\") \n\n\n# Create table of variables and counts, then find proportions and round to 3 decimal places\nprop_df <- ds %>%\n  count(variable, value) %>%\n  group_by(variable) %>%\n  mutate(prop = n / sum(n)) %>%\n  ungroup() %>%\n  mutate(prop = round(prop, 3))\n\n\n# Make it a gt() table\nprop_table <- prop_df %>% \n  gt(groupname_col = \"variable\", rowname_col = \"value\") %>%\n  tab_stubhead(label = md(\"*Values*\")) %>%\n  tab_header(\n    md(\n      \"Variable Proportions\"\n    )\n  ) %>%\n  cols_label(\n    variable = md(\"*Variable*\"),\n    value = md(\"*Value*\"),\n    n = md(\"*N*\"),\n    prop = md(\"*Proportion*\")\n  ) \n  \nprop_table\ngtsave(prop_table, here(\"figures\", \"prop_table.png\"))"},{"path":"lca-enumeration.html","id":"enumeration","chapter":"3 LCA Enumeration","heading":"3.4 Enumeration","text":"code uses mplusObject function MplusAutomation package saves model runs enum folder.IMPORTANT: moving forward, make sure open output document ensure models estimated normally.","code":"\n\nlca_6  <- lapply(1:6, function(k) {\n  lca_enum  <- mplusObject(\n      \n    TITLE = glue(\"{k}-Class\"), \n  \n    VARIABLE = glue(\n    \"categorical = report_dis-law_fte; \n     usevar = report_dis-law_fte;\n     classes = c({k}); \"),\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 200 100; \n    processors = 10;\",\n  \n  OUTPUT = \"sampstat residual tech11 tech14;\",\n  \n  PLOT = \n    \"type = plot3; \n    series = report_dis-law_fte(*);\",\n  \n  usevariables = colnames(df_bully),\n  rdata = df_bully)\n\nlca_enum_fit <- mplusModeler(lca_enum, \n                            dataout=glue(here(\"enum\", \"bully.dat\")),\n                            modelout=glue(here(\"enum\", \"c{k}_bully.inp\")) ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"lca-enumeration.html","id":"examine-and-extract-mplus-files","chapter":"3 LCA Enumeration","heading":"3.5 Examine and extract Mplus files","text":"Code Delwin Carter (2025)Check Models :WarningsErrorsConvergence Loglikelihood Replication Information","code":"\nsource(here(\"functions\", \"extract_mplus_info.R\"))\n\n# Define the directory where all of the .out files are located.\noutput_dir <- here(\"enum\")\n\n# Get all .out files\noutput_files <- list.files(output_dir, pattern = \"\\\\.out$\", full.names = TRUE)\n\n# Process all .out files into one dataframe\nfinal_data <- map_dfr(output_files, extract_mplus_info_extended)\n\n# Extract Sample_Size from final_data\nsample_size <- unique(final_data$Sample_Size)"},{"path":"lca-enumeration.html","id":"examine-mplus-warnings","chapter":"3 LCA Enumeration","heading":"3.5.1 Examine Mplus Warnings","text":"","code":"\nsource(here(\"functions\", \"extract_warnings.R\"))\n\nwarnings_table <- extract_warnings(final_data)\nwarnings_table\n\n# Save the warnings table\n#gtsave(warnings_table, here(\"figures\", \"warnings_table.png\"))"},{"path":"lca-enumeration.html","id":"examine-mplus-errors","chapter":"3 LCA Enumeration","heading":"3.5.2 Examine Mplus Errors","text":"","code":"\nsource(here(\"functions\", \"error_visualization.R\"))\n\n# Process errors\nerror_table_data <- process_error_data(final_data)\nerror_table_data\n\n# Save the errors table\n#gtsave(error_table, here(\"figures\", \"error_table.png\"))"},{"path":"lca-enumeration.html","id":"examine-convergence-and-loglikelihood-replications","chapter":"3 LCA Enumeration","heading":"3.5.3 Examine Convergence and Loglikelihood Replications","text":"N = 2027Random StartsFinal starting value sets convergingLL ReplicationSmallest ClassModelBest LLnparInitialFinalf%f%f%1-Class-5,443.4096200100100100%100100.0%2,027100.0%2-Class-5,194.136132001005757%57100.0%44421.9%3-Class-5,122.478202001009393%8086.0%21610.6%4-Class-5,111.757272001004646%2043.5%21210.5%5-Class-5,105.589342001003737%718.9%432.1%6-Class-5,099.881412001003232%412.5%361.8%","code":"\nsource(here(\"functions\", \"summary_table.R\"))\n\n# Print Table with Superheader & Heatmap\nsummary_table <- create_flextable(final_data, sample_size)\nsummary_table\n\n# Save the flextable as a PNG image\n#invisible(save_as_image(summary_table, path = here(\"figures\", \"housekeeping.png\")))"},{"path":"lca-enumeration.html","id":"check-for-loglikelihood-replication","chapter":"3 LCA Enumeration","heading":"3.5.4 Check for Loglikelihood Replication","text":"Visualize examine loglikelihood replication values ouput file individuallyVisualize examine loglikelihood replication output file together1-Class2-Class3-Class4-Class5-Class6-ClassLLN%LLN%LLN%LLN%LLN%LLN%-5443.409100100-5194.13657100-5122.4788086-5111.7572043.5-5105.589718.9-5,099.881412.5——————-5123.9451010.8-5111.75936.5-5105.66112.7-5,100.27213.1——————-5123.97933.2-5112.25336.5-5105.79138.1-5,100.84226.2—————————-5112.95512.2-5105.799410.8-5,100.87413.1—————————-5115.5321123.9-5106.74825.4-5,100.92826.2—————————-5115.53812.2-5106.98312.7-5,101.01713.1—————————-5115.88412.2-5107.16925.4-5,101.07126.2—————————-5116.98136.5-5107.172410.8-5,101.08913.1—————————-5117.82936.5-5107.4512.7-5,101.11713.1————————————-5107.45812.7-5,101.31613.1————————————-5107.51712.7-5,101.33213.1————————————-5107.72812.7-5,101.38913.1————————————-5107.95812.7-5,101.45213.1————————————-5108.05812.7-5,101.49413.1————————————-5108.09612.7-5,101.51213.1————————————-5108.86410.8-5,101.59213.1————————————-5109.00212.7-5,101.59313.1————————————-5110.47412.7-5,101.91313.1———————————————-5,102.07513.1———————————————-5,102.61313.1———————————————-5,102.61613.1———————————————-5,104.16713.1———————————————-5,104.46213.1———————————————-5,105.30913.1———————————————-5,107.30213.1———————————————-5,107.62413.1","code":"\n# Load the function for separate plots\nsource(here(\"functions\", \"ll_replication_plots.R\"))\n\n# Generate individual log-likelihood replication tables\nll_replication_tables <- generate_ll_replication_plots(final_data)\nll_replication_tables\nll_replication_table_all <- source(here(\"functions\", \"ll_replication_processing.R\"), local = TRUE)$value\nll_replication_table_all"},{"path":"lca-enumeration.html","id":"table-of-fit","chapter":"3 LCA Enumeration","heading":"3.6 Table of Fit","text":"First, extract data:, create table:Save table","code":"\noutput_enum <- readModels(here(\"enum\"), filefilter = \"bully\", quiet = TRUE)\n\n# Extract fit indices\nenum_extract <- LatexSummaryTable(\n  output_enum,\n  keepCols = c(\n    \"Title\",\n    \"Parameters\",\n    \"LL\",\n    \"BIC\",\n    \"aBIC\",\n    \"BLRT_PValue\",\n    \"T11_VLMR_PValue\",\n    \"Observations\"\n  ),\n  sortBy = \"Title\"\n)\n\n# Calculate additional fit indices\nallFit <- enum_extract %>%\n  mutate(CAIC = -2 * LL + Parameters * (log(Observations) + 1)) %>%\n  mutate(AWE = -2 * LL + 2 * Parameters * (log(Observations) + 1.5)) %>%\n  mutate(SIC = -.5 * BIC) %>%\n  mutate(expSIC = exp(SIC - max(SIC))) %>%\n  mutate(BF = exp(SIC - lead(SIC))) %>%\n  mutate(cmPk = expSIC / sum(expSIC)) %>%\n  dplyr::select(Title, Parameters, LL, BIC, aBIC, CAIC, AWE, BLRT_PValue, T11_VLMR_PValue, BF, cmPk) %>%\n  arrange(Parameters)\n\n# Merge columns with LL replications and class size from `final_data`\nmerged_table <- allFit %>%\n  mutate(Title = str_trim(Title)) %>%\n  left_join(\n    final_data %>%\n      select(\n        Class_Model,\n        Perc_Convergence,\n        Replicated_LL_Perc,\n        Smallest_Class,\n        Smallest_Class_Perc\n      ),\n    by = c(\"Title\" = \"Class_Model\")\n  ) %>%\n  mutate(Smallest_Class = coalesce(Smallest_Class, final_data$Smallest_Class[match(Title, final_data$Class_Model)])) %>%\n  relocate(Perc_Convergence, Replicated_LL_Perc, .after = LL) %>%\n  mutate(Smallest_Class_Combined = paste0(Smallest_Class, \"\\u00A0(\", Smallest_Class_Perc, \"%)\")) %>%\n  select(\n    Title,\n    Parameters,\n    LL,\n    Perc_Convergence,\n    Replicated_LL_Perc,\n    BIC,\n    aBIC,\n    CAIC,\n    AWE,\n    T11_VLMR_PValue,\n    BLRT_PValue,\n    Smallest_Class_Combined,\n    BF,\n    cmPk\n  )\nfit_table1 <- merged_table %>%\n  select(Title, Parameters, LL, Perc_Convergence, Replicated_LL_Perc, \n         BIC, aBIC, CAIC, AWE, \n         T11_VLMR_PValue, BLRT_PValue, \n         Smallest_Class_Combined) %>% \n  gt() %>%\n  tab_header(title = md(\"**Model Fit Summary Table**\")) %>%\n  tab_spanner(label = \"Model Fit Indices\", columns = c(BIC, aBIC, CAIC, AWE)) %>%\n  tab_spanner(label = \"LRTs\", columns = c(T11_VLMR_PValue, BLRT_PValue)) %>%\n  tab_spanner(label = md(\"Smallest\\u00A0Class\"), columns = c(Smallest_Class_Combined)) %>%\n  cols_label(\n    Title = \"Classes\",\n    Parameters = md(\"Par\"),\n    LL = md(\"*LL*\"),\n    Perc_Convergence = \"% Converged\",\n    Replicated_LL_Perc = \"% Replicated\",\n    BIC = \"BIC\",\n    aBIC = \"aBIC\",\n    CAIC = \"CAIC\",\n    AWE = \"AWE\",\n    T11_VLMR_PValue = \"VLMR\",\n    BLRT_PValue = \"BLRT\",\n    Smallest_Class_Combined = \"n (%)\"\n  ) %>%\n  tab_footnote(\n    footnote = md(\n      \"*Note.* Par = Parameters; *LL* = model log likelihood;\nBIC = Bayesian information criterion;\naBIC = sample size adjusted BIC; CAIC = consistent Akaike information criterion;\nAWE = approximate weight of evidence criterion;\nBLRT = bootstrapped likelihood ratio test p-value;\nVLMR = Vuong-Lo-Mendell-Rubin adjusted likelihood ratio test p-value;\n*cmPk* = approximate correct model probability.\"\n    ),\nlocations = cells_title()\n  ) %>%\n  tab_options(column_labels.font.weight = \"bold\") %>%\n  fmt_number(\n    columns = c(3, 6:9), \n    decimals = 2\n  ) %>%\n  sub_missing(1:11,\n              missing_text = \"--\") %>%\n  fmt(\n    c(T11_VLMR_PValue, BLRT_PValue),\n    fns = function(x)\n      ifelse(x < 0.001, \"<.001\",\n             scales::number(x, accuracy = .01))\n  ) %>%\n  fmt_percent(\n    columns = c(Perc_Convergence, Replicated_LL_Perc),\n    decimals = 0,\n    scale_values = FALSE\n  ) %>%\n  \n  cols_align(align = \"center\", columns = everything()) %>%  \n  tab_style(\n    style = list(cell_text(weight = \"bold\")),\n    locations = list(\n      cells_body(columns = BIC, row = BIC == min(BIC)),\n      cells_body(columns = aBIC, row = aBIC == min(aBIC)),\n      cells_body(columns = CAIC, row = CAIC == min(CAIC)),\n      cells_body(columns = AWE, row = AWE == min(AWE)),\n      cells_body(columns = T11_VLMR_PValue, \n                 row = ifelse(T11_VLMR_PValue < .05 & lead(T11_VLMR_PValue) > .05, T11_VLMR_PValue < .05, NA)),\n      cells_body(columns = BLRT_PValue, \n                 row = ifelse(BLRT_PValue < .05 & lead(BLRT_PValue) > .05, BLRT_PValue < .05, NA))\n    )\n  )\n\nfit_table1\ngtsave(fit_table1, here(\"figures\", \"fit_table.png\"))"},{"path":"lca-enumeration.html","id":"information-criteria-plot","chapter":"3 LCA Enumeration","heading":"3.7 Information Criteria Plot","text":"Save figure","code":"\nallFit %>%\n  dplyr::select(2:7) %>%\n  rowid_to_column() %>%\n  pivot_longer(`BIC`:`AWE`,\n               names_to = \"Index\",\n               values_to = \"ic_value\") %>%\n  mutate(Index = factor(Index,\n                        levels = c (\"AWE\", \"CAIC\", \"BIC\", \"aBIC\"))) %>%\n  ggplot(aes(\n    x = rowid,\n    y = ic_value,\n    color = Index,\n    shape = Index,\n    group = Index,\n    lty = Index\n  )) +\n  geom_point(size = 2.0) + geom_line(linewidth = .8) +\n  scale_x_continuous(breaks = 1:nrow(allFit)) +\n  scale_colour_grey(end = .5) +\n  theme_cowplot() +\n  labs(x = \"Number of Classes\", y = \"Information Criteria Value\", title = \"Information Criteria\") +\n  theme(\n    text = element_text(family = \"serif\", size = 12),\n    legend.text = element_text(family=\"serif\", size=12),\n    legend.key.width = unit(3, \"line\"),\n    legend.title = element_blank(),\n    legend.position = \"top\"  \n  )\nggsave(here(\"figures\", \"info_criteria.png\"), dpi=300, height=5, width=7, units=\"in\")"},{"path":"lca-enumeration.html","id":"compare-class-solutions","chapter":"3 LCA Enumeration","heading":"3.8 Compare Class Solutions","text":"Compare probability plots \\(K = 1:6\\) class solutionsSave figure:","code":"\nmodel_results <- data.frame()\n\nfor (i in 1:length(output_enum)) {\n  \n  temp <- output_enum[[i]]$parameters$probability.scale %>%                                       \n    mutate(model = paste(i,\"-Class Model\"))                                                  \n  \n  model_results <- rbind(model_results, temp)\n}\n\nrm(temp)\n\ncompare_plot <-\n  model_results %>%\n  filter(category == 2) %>%\n  dplyr::select(est, model, LatentClass, param) %>%\n  mutate(param = as.factor(str_to_lower(param))) \n\ncompare_plot$param <- fct_inorder(compare_plot$param)\n\nggplot(\n  compare_plot,\n  aes(\n    x = param,\n    y = est,\n    color = LatentClass,\n    shape = LatentClass,\n    group = LatentClass,\n    lty = LatentClass\n  )\n) +\n  geom_point() + \n  geom_line() +\n  scale_colour_viridis_d() +\n  facet_wrap( ~ model, ncol = 2) +\n  labs(title = \"Bullying Items\",\n       x = \" \", y = \"Probability\") +\n  theme_minimal() +\n  theme(panel.grid.major.y = element_blank(),\n                          axis.text.x = element_text(angle = -45, hjust = -.1))                            \nggsave(here(\"figures\", \"compare_kclass_plot.png\"), dpi=300, height=5, width=7, units=\"in\")"},{"path":"lca-enumeration.html","id":"class-probability-plot","chapter":"3 LCA Enumeration","heading":"3.9 3-Class Probability Plot","text":"Use plot_lca function provided folder plot item probability plot. function requires one argument:\n- model_name: name Mplus readModels object (e.g., output_enum$c3_bully.)Save figure:","code":"\nsource(\"plot_lca.txt\")\n\nplot_lca(model_name = output_enum$c3_bully.out)\nggsave(here(\"figures\", \"C3_bully_LCA_Plot.png\"), dpi=\"retina\", height=5, width=7, units=\"in\")"},{"path":"lca-enumeration.html","id":"observed-response-patterns","chapter":"3 LCA Enumeration","heading":"3.10 Observed Response Patterns","text":"Save response frequencies 3-class model previous lab response _____.dat SAVEDATA.Read observed response pattern data relabel columnsCreate table top 5 unconditional response pattern, top conditional response pattern modal class assignmentFinally, use gt make nicely formatted tableSave table:","code":"\n\npatterns  <- mplusObject(\n  \n  TITLE = \"C3 LCA - Save response patterns\", \n  \n  VARIABLE = \n  \"categorical = report_dis-law_fte; \n   usevar =  report_dis-law_fte;\n   classes = c(3);\",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 0;\n    processors = 10;\n    optseed = 802779;\",\n  \n  SAVEDATA = \n   \"File=savedata.dat;\n    Save=cprob;\n    \n    ! Code to save response frequency data \n    \n    response is resp_patterns.dat;\",\n  \n  OUTPUT = \"residual patterns tech11 tech14\",\n  \n  usevariables = colnames(df_bully),\n  rdata = df_bully)\n\npatterns_fit <- mplusModeler(patterns,\n                dataout=here(\"mplus\", \"bully.dat\"),\n                modelout=here(\"mplus\", \"patterns.inp\") ,\n                check=TRUE, run = TRUE, hashfilename = FALSE)\n# Read in response frequency data that we just created:\npatterns <- read_table(here(\"mplus\", \"resp_patterns.dat\"),\n                        col_names=FALSE, na = \"*\") \n\n# Extract the column names\nnames <- names(readModels(here(\"mplus\", \"patterns.out\"))[['savedata']]) \n\n# Add the names back to the dataset\ncolnames(patterns) <- c(\"Frequency\", names)  \n# Order responses by highest frequency\norder_highest <- patterns %>% \n  arrange(desc(Frequency)) \n\n# Loop `patterns` data to list top 5 conditional response patterns for each class\nloop_cond  <- lapply(1:max(patterns$C), function(k) {       \norder_cond <- patterns %>%                    \n  filter(C == k) %>%                    \n  arrange(desc(Frequency)) %>%                \n  head(5)                                     \n  })                                          \n\n# Convert loop into data frame\ntable_data <- as.data.frame(bind_rows(loop_cond))\n\n# Combine unconditional and conditional responses patterns\nresponse_patterns <-  rbind(order_highest[1:5,], table_data) \nresp_table <- response_patterns %>% \n  gt() %>%\n    tab_header(\n    title = \"Observed Response Patterns\",\n    subtitle = html(\"Response patterns, estimated frequencies, estimated posterior class probabilities and modal assignments\")) %>% \n    tab_source_note(\n    source_note = md(\"Data Source: **Civil Rights Data Collection (CRDC)**\")) %>%\n    cols_label(\n      Frequency = html(\"<i>f<\/i><sub>r<\/sub>\"),\n    REPORT_D = \"Harrassment: Disability\",\n    REPORT_R = \"Harrassment: Race\",\n    REPORT_S = \"Harrassment: Sex\",\n    COUNSELO = \"Staff: Counselor\",\n    PSYCH_FT = \"Staff: Psychologist\",\n    LAW_FTE = \"Staff: Law Enforcement\",\n    CPROB1 = html(\"P<sub><i>k<\/i><\/sub>=1\"),\n    CPROB2 = html(\"P<sub><i>k<\/i><\/sub>=2\"),\n    CPROB3 = html(\"P<sub><i>k<\/i><\/sub>=3\"),\n    C = md(\"*k*\")) %>% \n  tab_row_group(\n    label = \"Unconditional response patterns\",\n    rows = 1:5) %>%\n  tab_row_group(\n    label = md(\"*k* = 1 Conditional response patterns\"),\n    rows = 6:10) %>% #EDIT THESE VALUES BASED ON THE LAST COLUMN\n  tab_row_group(\n    label = md(\"*k* = 2 Conditional response patterns\"),\n    rows = 11:15)  %>% #EDIT THESE VALUES BASED ON THE LAST COLUMN\n  tab_row_group(\n    label = md(\"*k* = 3 Conditional response patterns\"),\n    rows = 16:20)  %>% #EDIT THESE VALUES BASED ON THE LAST COLUMN  \n    row_group_order(\n      groups = c(\"Unconditional response patterns\",\n                 md(\"*k* = 1 Conditional response patterns\"),\n                 md(\"*k* = 2 Conditional response patterns\"),\n                 md(\"*k* = 3 Conditional response patterns\"))) %>% \n    tab_footnote(\n    footnote = html(\n      \"<i>Note.<\/i> <i>f<\/i><sub>r<\/sub> = response pattern frequency; P<sub><i>k<\/i><\/sub> = posterior class probabilities\"\n    )\n  ) %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"Times New Roman\")\n\nresp_table\ngtsave(resp_table, here(\"figures\",\"resp_table.png\"))"},{"path":"lca-enumeration.html","id":"classification-diagnostics","chapter":"3 LCA Enumeration","heading":"3.11 Classification Diagnostics","text":"Use Mplus calculate k-class confidence intervals (Note: Change synax make chosen k-class model):Note: Ensure classes shift step (.g., Class 1 enumeration run now Class 4). Evaluate output compare class counts proportions latent classes. Using OPTSEED function ensures replication best loglikelihood value run.Read 3-class model:Now, use gt make nicely formatted table","code":"\nclassification  <- mplusObject(\n  \n  TITLE = \"C3 LCA - Calculated k-Class 95% CI\",\n  \n  VARIABLE =\n    \"categorical = report_dis-law_fte;\n   usevar =  report_dis-law_fte;\n   classes = c(3);\", \n  \n  ANALYSIS =\n    \"estimator = ml;\n    type = mixture;\n    starts = 0; \n    processors = 10;\n    optseed = 802779;\n    bootstrap = 1000;\",\n  \n  MODEL =\n    \"\n  !CHANGE THIS SECTION TO YOUR CHOSEN k-CLASS MODEL\n    \n  %OVERALL%\n  [C#1](c1);\n  \n  [C#2](C2);\n\n  Model Constraint:\n  New(p1 p2 p3);\n  \n  p1 = exp(c1)/(1+exp(c1)+exp(c2));\n  p2 = exp(c2)/(1+exp(c1)+exp(c2));\n  p3 = 1/(1+exp(c1)+exp(c2));\",\n\n  \n  OUTPUT = \"cinterval(bcbootstrap)\",\n  \n  usevariables = colnames(df_bully),\n  rdata = df_bully)\n\nclassification_fit <- mplusModeler(classification,\n                dataout=here(\"mplus\", \"bully.dat\"),\n                modelout=here(\"mplus\", \"class.inp\") ,\n                check=TRUE, run = TRUE, hashfilename = FALSE)\n# Read in the 3-class model and extract information needed\noutput_enum <- readModels(here(\"mplus\", \"class.out\"))\n\n# Entropy\nentropy <- c(output_enum$summaries$Entropy, rep(NA, output_enum$summaries$NLatentClasses-1))\n\n# 95% k-Class and k-class 95% Confidence Intervals\nk_ci <- output_enum$parameters$ci.unstandardized %>% \n  filter(paramHeader == \"New.Additional.Parameters\") %>% \n  unite(CI, c(low2.5,up2.5), sep=\", \", remove = TRUE) %>% \n  mutate(CI = paste0(\"[\", CI, \"]\")) %>% \n  rename(kclass=est) %>% \n  dplyr::select(kclass, CI)\n\n# AvePPk = Average Latent Class Probabilities for Most Likely Latent Class Membership (Row) by Latent Class (Column)\navePPk <- tibble(avePPk = diag(output_enum$class_counts$avgProbs.mostLikely))\n\n# mcaPk = modal class assignment proportion \nmcaPk <- round(output_enum$class_counts$mostLikely,3) %>% \n  mutate(model = paste0(\"Class \", class)) %>% \n  add_column(avePPk, k_ci) %>% \n  rename(mcaPk = proportion) %>% \n  dplyr::select(model, kclass, CI, mcaPk, avePPk)\n\n# OCCk = odds of correct classification\nOCCk <- mcaPk %>% \n  mutate(OCCk = round((avePPk/(1-avePPk))/(kclass/(1-kclass)),3))\n\n# Put everything together\nclass_table <- data.frame(OCCk, entropy)\nclass_table <- class_table %>% \n  gt() %>%\n    tab_header(\n    title = \"Model Classification Diagnostics for the 3-Class Solution\") %>%\n    cols_label(\n      model = md(\"*k*-Class\"),\n      kclass = md(\"*k*-Class Proportions\"),\n      CI = \"95% CI\",\n      mcaPk = html(\"McaP<sub>k<\/sub>\"),\n      avePPk = md(\"AvePP<sub>k<\/sub>\"),\n      OCCk = md(\"OCC<sub>k<\/sub>\"),\n      entropy = \"Entropy\") %>% \n    sub_missing(7,\n              missing_text = \"\") %>%\n    tab_footnote(\n    footnote = html(\n      \"<i>Note.<\/i> McaP<sub>k<\/sub> = Modal class assignment proportion; AvePP<sub>k<\/sub> = Average posterior class probabilities; OCC<sub>k<\/sub> = Odds of correct classification; \"\n    )\n  ) %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"Times New Roman\")\n\nclass_table"},{"path":"lpa-enumeration.html","id":"lpa-enumeration","chapter":"4 LPA Enumeration","heading":"4 LPA Enumeration","text":"Example: PISA Student DataThe first example closely follows vignette used demonstrate tidyLPA package (Rosenberg, 2019).model utilizes PISA data collected U.S. 2015. learn data see .access 2015 US PISA data & documentation R use following code:Variables:broad_interest\ncomposite measure students’ self reported broad interest\ncomposite measure students’ self reported broad interestenjoyment\ncomposite measure students’ self reported enjoyment\ncomposite measure students’ self reported enjoymentinstrumental_mot\ncomposite measure students’ self reported instrumental motivation\ncomposite measure students’ self reported instrumental motivationself_efficacy\ncomposite measure students’ self reported self efficacy\ncomposite measure students’ self reported self efficacy","code":"\n#devtools::install_github(\"jrosen48/pisaUSA15\")\n#library(pisaUSA15)"},{"path":"lpa-enumeration.html","id":"latent-profile-models","chapter":"4 LPA Enumeration","heading":"4.1 Latent Profile Models","text":"Latent Profile Analysis (LPA) statistical modeling approach estimating distinct profiles variables.\nsocial sciences educational research, profiles represent, example, different youth experience dimensions engaged (.e., cognitively, behaviorally, affectively) time.\nNote LPA works best continuous variables (, cases, ordinal variables), appropriate dichotomous (binary) variables.Many analysts carried LPA using latent variable modeling approach.\napproach, different parameters - means, variances, covariances - freely estimated across profiles, fixed across profiles, constrained zero.\nMPlus software commonly used estimate models (see ) using expectation-maximization (EM) algorithm obtain maximum likelihood estimates parameters.Different models (whether parameters estimated) can specified estimated.\nMPlus widely-used (powerful), costly, closed-source, can difficult use, particularly respect interpreting using output specified models part reproducible workflow.","code":""},{"path":"lpa-enumeration.html","id":"terminology-for-specifying-variance-covariance-matrix","chapter":"4 LPA Enumeration","heading":"4.2 Terminology for specifying variance-covariance matrix","text":"code used estimate LPA models walkthrough tidyLPA package.\nTidyLPA(source) R package designed estimate latent profile models using tidy framework.\ncan interface Mplus via MplusAutomation package, enabling estimation latent profile models different variance-covariance structures.model 1 Profile-invariant / Diagonal: Equal variances, covariances fixed 0model 2 Profile-varying / Diagonal: Free variances covariances fixed 0model 3 Profile-invariant / Non-Diagonal: Equal variances equal covariances\nNote: alternative Model 3 freely estimating covariances\nNote: alternative Model 3 freely estimating covariancesmodel 4 Free variances, equal covariancesmodel 5 Equal variances, free covariancesmodel 6 Profile Varying / Non-Diagonal: Free variances free covariances","code":""},{"path":"lpa-enumeration.html","id":"model-1","chapter":"4 LPA Enumeration","heading":"4.2.1 Model 1","text":"Profile-invariant/diagonal:Equal Variances: Variances fixed equality across profiles (.e., variances constrained equal profile).Equal Variances: Variances fixed equality across profiles (.e., variances constrained equal profile).Covariances fixed zero (.e., -diagonal cells matrix zero).Covariances fixed zero (.e., -diagonal cells matrix zero).parsimonious model restricted.\\[\n\\begin{pmatrix}\n\\sigma^2_1 & 0 & 0 \\\\\n0 & \\sigma^2_2 & 0 \\\\\n0 & 0 & \\sigma^2_3 \\\\\n\\end{pmatrix}\n\\]","code":""},{"path":"lpa-enumeration.html","id":"model-2","chapter":"4 LPA Enumeration","heading":"4.2.2 Model 2","text":"Profile-varying/diagonal:Free variances: Variances parameters freely estimated across profiles (.e., variances vary profile).Free variances: Variances parameters freely estimated across profiles (.e., variances vary profile).Covariances fixed zero (.e., -diagonal cells matrix zero).Covariances fixed zero (.e., -diagonal cells matrix zero).model flexible less parsimonious model 1.\\[\n\\begin{pmatrix}\n\\sigma^2_{1p} & 0 & 0 \\\\\n0 & \\sigma^2_{2p} & 0 \\\\\n0 & 0 & \\sigma^2_{3p} \\\\\n\\end{pmatrix}\n\\]","code":""},{"path":"lpa-enumeration.html","id":"model-3","chapter":"4 LPA Enumeration","heading":"4.2.3 Model 3","text":"Profile-invariant/ non-diagonal unrestricted:Equal variances: Variances fixed equality across profile.\n(.e., variances constrained profile).Equal variances: Variances fixed equality across profile.\n(.e., variances constrained profile).Equal Covariances: covariances now estimated constrained equal.\nalternative Model 3 freely estimating covariances (Model 5 ).\nEqual Covariances: covariances now estimated constrained equal.alternative Model 3 freely estimating covariances (Model 5 ).\\[\n\\begin{pmatrix}\n\\sigma^2_1 & \\sigma_{12} & \\sigma_{13} \\\\\n\\sigma_{12} & \\sigma^2_2 & \\sigma_{23} \\\\\n\\sigma_{13} & \\sigma_{23} & \\sigma^2_3 \\\\\n\\end{pmatrix}\n\\]","code":""},{"path":"lpa-enumeration.html","id":"model-4","chapter":"4 LPA Enumeration","heading":"4.2.4 Model 4","text":"Varying means, varying variances, equal covariances:Free variances: Variances parameters freely estimated across profiles (.e., variances vary profile).Free variances: Variances parameters freely estimated across profiles (.e., variances vary profile).Equal Covariances: Covariances constrained equal.Equal Covariances: Covariances constrained equal.model also considered extension Model 3.\\[\n\\begin{pmatrix}\n\\sigma^2_{1p} & \\sigma_{12} & \\sigma_{13} \\\\\n\\sigma_{12} & \\sigma^2_{2p} & \\sigma_{23} \\\\\n\\sigma_{13} & \\sigma_{23} & \\sigma^2_{3p} \\\\\n\\end{pmatrix}\n\\]","code":""},{"path":"lpa-enumeration.html","id":"model-5","chapter":"4 LPA Enumeration","heading":"4.2.5 Model 5","text":"Varying means, equal variances, varying covariances:Equal variances: Variances fixed equality across profiles.\n(.e., variances constrained profile).Equal variances: Variances fixed equality across profiles.\n(.e., variances constrained profile).Free Covariances: Covariances now freely estimated across profiles.Free Covariances: Covariances now freely estimated across profiles.model also considered extension Model 3.\\[\n\\begin{pmatrix}\n\\sigma^2_{1} & \\sigma_{12p} & \\sigma_{13p} \\\\\n\\sigma_{12p} & \\sigma^2_{2} & \\sigma_{23p} \\\\\n\\sigma_{13p} & \\sigma_{23p} & \\sigma^2_{3} \\\\\n\\end{pmatrix}\n\\]","code":""},{"path":"lpa-enumeration.html","id":"model-6","chapter":"4 LPA Enumeration","heading":"4.2.6 Model 6","text":"Profile-varying / Non-diagonal:Free variances: Variances parameters freely estimated across profiles (.e., variances vary profile).Free variances: Variances parameters freely estimated across profiles (.e., variances vary profile).Free Covariances: Covariances now freely estimated across profiles.Free Covariances: Covariances now freely estimated across profiles.complex unrestricted model.\nalso least parsimoniousNote: unrestricted model also sometimes known Model 4.\\[\n\\begin{pmatrix}\n\\sigma^2_{1p} & \\sigma_{12p} & \\sigma_{13p} \\\\\n\\sigma_{12p} & \\sigma^2_{2p} & \\sigma_{23p} \\\\\n\\sigma_{13p} & \\sigma_{23p} & \\sigma^2_{3p} \\\\\n\\end{pmatrix}\n\\]","code":""},{"path":"lpa-enumeration.html","id":"load-packages","chapter":"4 LPA Enumeration","heading":"4.3 Load packages","text":"","code":"\nlibrary(naniar)\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(glue)\nlibrary(MplusAutomation)\nlibrary(here)\nlibrary(janitor)\nlibrary(gt)\nlibrary(tidyLPA)\nlibrary(pisaUSA15)\nlibrary(cowplot)\nlibrary(filesstrings)\nlibrary(patchwork)\nlibrary(RcppAlgos)"},{"path":"lpa-enumeration.html","id":"prepare-data-1","chapter":"4 LPA Enumeration","heading":"4.4 Prepare Data","text":"","code":"\n\npisa <- pisaUSA15[1:500,] %>%\n  dplyr::select(broad_interest, enjoyment, instrumental_mot, self_efficacy)"},{"path":"lpa-enumeration.html","id":"descriptive-statistics-1","chapter":"4 LPA Enumeration","heading":"4.5 Descriptive Statistics","text":"Quick SummaryMean TableHistograms","code":"\nsummary(pisa)\n#>  broad_interest    enjoyment    instrumental_mot\n#>  Min.   :1.000   Min.   :1.00   Min.   :1.000   \n#>  1st Qu.:2.200   1st Qu.:2.40   1st Qu.:1.750   \n#>  Median :2.800   Median :3.00   Median :2.000   \n#>  Mean   :2.666   Mean   :2.82   Mean   :2.129   \n#>  3rd Qu.:3.200   3rd Qu.:3.00   3rd Qu.:2.500   \n#>  Max.   :5.000   Max.   :4.00   Max.   :4.000   \n#>  NA's   :23      NA's   :14     NA's   :21      \n#>  self_efficacy  \n#>  Min.   :1.000  \n#>  1st Qu.:1.750  \n#>  Median :2.000  \n#>  Mean   :2.125  \n#>  3rd Qu.:2.500  \n#>  Max.   :4.000  \n#>  NA's   :23\nds <- pisa %>% \n  pivot_longer(broad_interest:self_efficacy, names_to = \"variable\") %>% \n  group_by(variable) %>% \n  summarise(mean = mean(value, na.rm = TRUE),\n            sd = sd(value, na.rm = TRUE)) \n\nds %>% \n  gt () %>% \n  tab_header(title = md(\"**Descriptive Summary**\")) %>%\n  cols_label(\n    variable = \"Variable\",\n    mean = md(\"M\"),\n    sd = md(\"SD\")\n  ) %>%\n  fmt_number(c(2:3),\n             decimals = 2) %>% \n  cols_align(\n    align = \"center\",\n    columns = mean\n  ) \ndata_long <- pisa %>%\n  pivot_longer(broad_interest:self_efficacy, names_to = \"variable\")\n\nggplot(data_long, aes(x = value)) +\n  geom_histogram(binwidth = .3, fill = \"#69b3a2\", color = \"black\") +\n  facet_wrap(~ variable, scales = \"free_x\") +\n  labs(title = \"Histograms of Variables\", x = \"Value\", y = \"Frequency\") +\n  theme_cowplot()"},{"path":"lpa-enumeration.html","id":"enumeration-1","chapter":"4 LPA Enumeration","heading":"4.6 Enumeration","text":"","code":""},{"path":"lpa-enumeration.html","id":"tidylpa","chapter":"4 LPA Enumeration","heading":"4.6.1 tidyLPA","text":"Enumerate using estimate_profiles():Estimate models profiles \\(K = 1:5\\)Model 4 continuous indicatorsDefault variance-covariance specifications (model 1)Change variances covariances indicate model want specify, example, estimating six models.","code":"\n\n# Run LPA models \nlpa_fit <- pisa %>% \n    estimate_profiles(1:5,\n                      package = \"MplusAutomation\",\n                      ANALYSIS = \"starts = 500 100;\",\n                      OUTPUT = \"sampstat residual tech11 tech14\",\n                      variances = c(\"equal\", \"varying\", \"equal\", \"varying\", \"equal\", \"varying\"),\n                      covariances = c(\"zero\", \"zero\", \"equal\", \"equal\", \"varying\", \"varying\"),\n                      keepfiles = TRUE)\n\n# Compare fit statistics\nget_fit(lpa_fit)\n\n\n# Move files to folder \nfiles <- list.files(here(), pattern = \"^model\")\nmove_files(files, here(\"lpa\", \"tidyLPA\"), overwrite = TRUE)"},{"path":"lpa-enumeration.html","id":"mplus","chapter":"4 LPA Enumeration","heading":"4.6.2 Mplus","text":"Alternative method estimate_profiles(): Run enumeration using mplusObject methodYou can change model specification LPA using syntax provided lecture.","code":""},{"path":"lpa-enumeration.html","id":"model-1-1","chapter":"4 LPA Enumeration","heading":"4.6.2.1 Model 1","text":"estimating LPA Mplus, default variance/covariance specification restricted model (Model 1).\ndon’t specify anything .","code":"\n\nlpa_k14  <- lapply(1:5, function(k) {\n  lpa_enum  <- mplusObject(\n      \n    TITLE = glue(\"Profile {k}\"), \n  \n    VARIABLE = glue(\n    \"usevar = broad_interest-self_efficacy;\n     classes = c({k}); \"),\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 500 100;\",\n  \n  OUTPUT = \"sampstat svalues residual tech11 tech14;\",\n  \n  usevariables = colnames(pisa),\n  rdata = pisa)\n\nlpa_enum_fit <- mplusModeler(lpa_enum, \n                dataout=glue(here(\"lpa\", \"enum_lpa\", \"lpa_pisa\")),\n                modelout=glue(here(\"lpa\", \"enum_lpa\", \"c{k}_lpa_m1.inp\")) ,\n                check=TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"lpa-enumeration.html","id":"model-2-1","chapter":"4 LPA Enumeration","heading":"4.6.2.2 Model 2","text":", addition loop adds variance/covariance specifications class-specific statement.\nprofile-varying/diagonal specification, must specify variances freely estimated:broad_interest-self_efficacy;reference, Mplus syntax different specifications:Fixed covariance zero (DEFAULT):broad_interest enjoyment@0;Free covariance:broad_interest enjoyment;Equal covariances:%c#1%broad_interest enjoyment (1);%c#2%broad_interest enjoyment (1);Equal variance (DEFAULT):%c#1%broad_interest (1);%c#2%broad_interest (1);Free variance:mth_scor-bio_scor;can also open tidyLPA .inp files see specifications.","code":"\nlpa_m2_k14  <- lapply(1:5, function(k){ \n  \n  # This MODEL section changes the model specification\n  MODEL <- paste(sapply(1:k, function(i) {\n    glue(\"\n    %c#{i}%\n    broad_interest-self_efficacy;      ! variances are freely estimated\n    \")\n  }), collapse = \"\\n\")\n  \n  lpa_enum_m2  <- mplusObject(\n    TITLE = glue(\"Profile {k} - Model 2\"),\n    \n    VARIABLE = glue(\n      \"usevar = broad_interest-self_efficacy;\n     classes = c({k});\"),\n    \n    ANALYSIS = \n      \"estimator = mlr; \n    type = mixture;\n    starts = 500 100;\",\n    \n    MODEL = MODEL,\n    \n    \n    OUTPUT = \"sampstat svalues residual tech11 tech14;\",\n    \n    usevariables = colnames(pisa),\n    rdata = pisa)\n  \n  lpa_m2_fit <- mplusModeler(lpa_enum_m2,\n                             dataout = here(\"lpa\", \"enum_lpa\", \"lpa_pisa\"),\n                             modelout = glue(here(\"lpa\", \"enum_lpa\",\"c{k}_lpa_m2.inp\")),\n                             check = TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"lpa-enumeration.html","id":"table-of-fit-1","chapter":"4 LPA Enumeration","heading":"4.7 Table of Fit","text":"Evaluate model specification separately using fit indices.examining outputs (increasing random starts necessary), models excluded analysis:Model 2:Profile 4Profile 4Profile 5Profile 5Model 3:Profile 5Model 4:Profile 4Profile 4Profile 5Profile 5Model 5:Profile 3Profile 3Profile 4Profile 4Profile 5Profile 5Model 6:Profile 4Profile 4Profile 5Profile 5","code":"\nsource(here(\"functions\",\"enum_table_lpa.R\"))\n\n# Read in model\noutput_enum <- readModels(here(\"lpa\", \"tidyLPA\"), quiet = TRUE)\n\n# Preview with numbered rows\nenum_fit(output_enum)\n#> # A tibble: 29 × 12\n#>      row Title     Parameters     LL   BIC  aBIC  CAIC   AWE\n#>    <int> <chr>          <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl>\n#>  1     1 Model 1 …          8 -2089. 4227. 4201. 4235. 4300.\n#>  2     2 Model 1 …         13 -1997. 4074. 4032. 4087. 4193.\n#>  3     3 Model 1 …         18 -1953. 4017. 3960. 4035. 4183.\n#>  4     4 Model 1 …         23 -1889. 3921. 3848. 3944. 4133.\n#>  5     5 Model 1 …         28 -1871. 3915. 3826. 3943. 4172.\n#>  6     6 Model 2 …          8 -2089. 4227. 4201. 4235. 4300.\n#>  7     7 Model 2 …         17 -1989. 4083. 4029. 4100. 4239.\n#>  8     8 Model 2 …         26 -1878. 3917. 3834. 3943. 4156.\n#>  9     9 Model 2 …         35 -1851. 3919. 3808. 3954. 4241.\n#> 10    10 Model 2 …         44 -1825. 3923. 3783. 3967. 4327.\n#> # ℹ 19 more rows\n#> # ℹ 4 more variables: BLRT_PValue <dbl>,\n#> #   T11_VLMR_PValue <dbl>, BF <dbl>, cmPk <dbl>\n\n\nselect_models <-LatexSummaryTable(output_enum,                                 \n                keepCols=c(\"Title\", \"Parameters\", \"LL\", \"BIC\", \"aBIC\",\n                           \"BLRT_PValue\", \"T11_VLMR_PValue\",\"Observations\")) %>% \n  slice( # Remove the models that we don't want to consider!!!! Because we looked at every single output, we know which models did not converge, thus we exclude them.\n    # Model 2\n    -9, -10, \n    # Model 3\n    -15,\n    # Model 4\n    -19, -20,\n    # Model 5\n    -23, -24, -25,\n    # Model 6\n    -29, -30\n  )\n\n# Check to make sure that the rows of the models we don't want are removed\n#View(select_models)\n\nenum_table(select_models, 1:5, 6:8, 9:12, 13:15, 16:17, 18:20)"},{"path":"lpa-enumeration.html","id":"information-criteria-plot-1","chapter":"4 LPA Enumeration","heading":"4.8 Information Criteria Plot","text":"Look “elbow” help profile selectionBased fit indices, choosing following candidate models:Model 1: 2 ProfileModel 2: 3 ProfileModel 3: 4 ProfileModel 4: 3 ProfileModel 5: 2 ProfileModel 6: 2 Profile","code":"\nsource(here(\"functions\",\"ic_plot.R\"))\nic_plot(select_models)"},{"path":"lpa-enumeration.html","id":"compare-models","chapter":"4 LPA Enumeration","heading":"4.9 Compare models","text":"","code":""},{"path":"lpa-enumeration.html","id":"correct-model-probability-cmpk-recalculation","chapter":"4 LPA Enumeration","heading":"4.9.1 Correct Model Probability (cmpK) recalculation","text":"Take candidate models recalculate approximate correct model probabilities (Masyn, 2013)","code":"\n# CmpK recalculation:\nenum_fit1 <- enum_fit(output_enum)\n\nstage2_cmpk <- enum_fit1 %>% \n  slice(2, 8, 14, 18, 22, 27) %>% \n  mutate(SIC = -.5 * BIC,\n         expSIC = exp(SIC - max(SIC)),\n         cmPk = expSIC / sum(expSIC),\n         BF = exp(SIC - lead(SIC))) %>% \n  select(Title, Parameters, BIC:AWE, cmPk, BF)\n\n \n# Format Fit Table\nstage2_cmpk %>%\n  gt() %>% \n  tab_options(column_labels.font.weight = \"bold\") %>%\n  fmt_number(\n    7,\n    decimals = 2,\n    drop_trailing_zeros = TRUE,\n    suffixing = TRUE\n  ) %>%\n  fmt_number(c(3:6),\n             decimals = 2) %>% \n    fmt_number(8,decimals = 2,\n             drop_trailing_zeros=TRUE,\n             suffixing = TRUE) %>% \n  fmt(8, fns = function(x) \n    ifelse(x>100, \">100\",\n           scales::number(x, accuracy = .1))) %>% \n  tab_style(\n    style = list(\n      cell_text(weight = \"bold\")\n      ),\n    locations = list(cells_body(\n     columns = BIC,\n     row = BIC == min(BIC[1:nrow(stage2_cmpk)]) \n    ),\n    cells_body(\n     columns = aBIC,\n     row = aBIC == min(aBIC[1:nrow(stage2_cmpk)])\n    ),\n    cells_body(\n     columns = CAIC,\n     row = CAIC == min(CAIC[1:nrow(stage2_cmpk)])\n    ),\n    cells_body(\n     columns = AWE,\n     row = AWE == min(AWE[1:nrow(stage2_cmpk)])\n    ),\n    cells_body(\n     columns = cmPk,\n     row =  cmPk == max(cmPk[1:nrow(stage2_cmpk)])\n     ),\n    cells_body(\n     columns = BF, \n     row =  BF > 10)\n  )\n)"},{"path":"lpa-enumeration.html","id":"compare-loglikelihood","chapter":"4 LPA Enumeration","heading":"4.9.2 Compare loglikelihood","text":"can also compare models using nested model testing directly MplusAutomation.\nNote can compare across models profiles must stay ., Model 1 (restricted, fewer parameters) nested Model 2.\nchi-square difference test, assuming nested models, shows significant improvement fit Model 2 Model 1, despite added parameters.","code":"\n# MplusAutomation Method using `compareModels` \n\ncompareModels(output_enum[[\"model_2_class_3.out\"]],\n  output_enum[[\"model_4_class_3.out\"]], diffTest = TRUE)\n#> \n#> ==============\n#> \n#> Mplus model comparison\n#> ----------------------\n#> \n#> ------\n#> Model 1:  C:\\Users\\dnajiarch\\Box\\lca-bookdown\\lpa\\tidyLPA/model_2_class_3.out \n#> Model 2:  C:\\Users\\dnajiarch\\Box\\lca-bookdown\\lpa\\tidyLPA/model_4_class_3.out \n#> ------\n#> \n#> Model Summary Comparison\n#> ------------------------\n#> \n#>              m1                     m2                    \n#> Title        model 2 with 3 classes model 4 with 3 classes\n#> Observations 488                    488                   \n#> Estimator    MLR                    MLR                   \n#> Parameters   26                     32                    \n#> LL           -1877.965              -1859.466             \n#> AIC          3807.929               3782.932              \n#> BIC          3916.877               3917.022              \n#> \n#>   MLR Chi-Square Difference Test for Nested Models Based on Loglikelihood\n#>   -----------------------------------------------------------------------\n#> \n#>   Difference Test Scaling Correction:  1.177167 \n#>   Chi-square difference:  31.4297 \n#>   Diff degrees of freedom:  6 \n#>   P-value:  0 \n#> \n#>   Note: The chi-square difference test assumes that these models are nested.\n#>   It is up to you to verify this assumption.\n#> \n#>   MLR Chi-Square Difference test for nested models\n#>   --------------------------------------------\n#> \n#>   Difference Test Scaling Correction:  \n#>   Chi-square difference:  \n#>   Diff degrees of freedom:  \n#>   P-value:  \n#> \n#> Note: The chi-square difference test assumes that these models are nested.\n#>   It is up to you to verify this assumption.\n#> \n#> =========\n#> \n#> Model parameter comparison\n#> --------------------------\n#>   Parameters present in both models\n#> =========\n#> \n#>   Approximately equal in both models (param. est. diff <= 1e-04)\n#>   ----------------------------------------------\n#>  paramHeader     param LatentClass m1_est m2_est . m1_se\n#>        Means ENJOYMENT           1  2.968  2.968 | 0.012\n#>  m2_se . m1_est_se m2_est_se . m1_pval m2_pval\n#>  0.021 |   238.242   143.432 |       0       0\n#> \n#> \n#>   Parameter estimates that differ between models (param. est. diff > 1e-04)\n#>   ----------------------------------------------\n#>    paramHeader      param                  LatentClass\n#>  BROAD_IN.WITH  ENJOYMENT                            1\n#>  BROAD_IN.WITH  ENJOYMENT                            2\n#>  BROAD_IN.WITH  ENJOYMENT                            3\n#>  BROAD_IN.WITH INSTRUMENT                            1\n#>  BROAD_IN.WITH INSTRUMENT                            2\n#>  BROAD_IN.WITH INSTRUMENT                            3\n#>  BROAD_IN.WITH SELF_EFFIC                            1\n#>  BROAD_IN.WITH SELF_EFFIC                            2\n#>  BROAD_IN.WITH SELF_EFFIC                            3\n#>  ENJOYMEN.WITH INSTRUMENT                            1\n#>  ENJOYMEN.WITH INSTRUMENT                            2\n#>  ENJOYMEN.WITH INSTRUMENT                            3\n#>  ENJOYMEN.WITH SELF_EFFIC                            1\n#>  ENJOYMEN.WITH SELF_EFFIC                            2\n#>  ENJOYMEN.WITH SELF_EFFIC                            3\n#>  INSTRUME.WITH SELF_EFFIC                            1\n#>  INSTRUME.WITH SELF_EFFIC                            2\n#>  INSTRUME.WITH SELF_EFFIC                            3\n#>          Means BROAD_INTE                            1\n#>          Means BROAD_INTE                            2\n#>          Means BROAD_INTE                            3\n#>          Means       C1#1 Categorical.Latent.Variables\n#>          Means       C1#2 Categorical.Latent.Variables\n#>          Means  ENJOYMENT                            2\n#>          Means  ENJOYMENT                            3\n#>          Means INSTRUMENT                            1\n#>          Means INSTRUMENT                            2\n#>          Means INSTRUMENT                            3\n#>          Means SELF_EFFIC                            1\n#>          Means SELF_EFFIC                            2\n#>          Means SELF_EFFIC                            3\n#>      Variances BROAD_INTE                            1\n#>      Variances BROAD_INTE                            2\n#>      Variances BROAD_INTE                            3\n#>      Variances  ENJOYMENT                            1\n#>      Variances  ENJOYMENT                            2\n#>      Variances  ENJOYMENT                            3\n#>      Variances INSTRUMENT                            1\n#>      Variances INSTRUMENT                            2\n#>      Variances INSTRUMENT                            3\n#>      Variances SELF_EFFIC                            1\n#>      Variances SELF_EFFIC                            2\n#>      Variances SELF_EFFIC                            3\n#>  m1_est m2_est . m1_se m2_se . m1_est_se m2_est_se .\n#>   0.000  0.010 | 0.000 0.008 |   999.000     1.300 |\n#>   0.000  0.010 | 0.000 0.008 |   999.000     1.300 |\n#>   0.000  0.010 | 0.000 0.008 |   999.000     1.300 |\n#>   0.000 -0.013 | 0.000 0.024 |   999.000    -0.530 |\n#>   0.000 -0.013 | 0.000 0.024 |   999.000    -0.530 |\n#>   0.000 -0.013 | 0.000 0.024 |   999.000    -0.530 |\n#>   0.000 -0.029 | 0.000 0.023 |   999.000    -1.290 |\n#>   0.000 -0.029 | 0.000 0.023 |   999.000    -1.290 |\n#>   0.000 -0.029 | 0.000 0.023 |   999.000    -1.290 |\n#>   0.000 -0.034 | 0.000 0.015 |   999.000    -2.252 |\n#>   0.000 -0.034 | 0.000 0.015 |   999.000    -2.252 |\n#>   0.000 -0.034 | 0.000 0.015 |   999.000    -2.252 |\n#>   0.000 -0.029 | 0.000 0.008 |   999.000    -3.702 |\n#>   0.000 -0.029 | 0.000 0.008 |   999.000    -3.702 |\n#>   0.000 -0.029 | 0.000 0.008 |   999.000    -3.702 |\n#>   0.000  0.056 | 0.000 0.020 |   999.000     2.777 |\n#>   0.000  0.056 | 0.000 0.020 |   999.000     2.777 |\n#>   0.000  0.056 | 0.000 0.020 |   999.000     2.777 |\n#>   2.912  2.926 | 0.049 0.060 |    59.366    49.094 |\n#>   3.205  3.264 | 0.086 0.126 |    37.165    25.960 |\n#>   2.257  2.159 | 0.082 0.114 |    27.478    18.918 |\n#>  -0.285  0.143 | 0.188 0.341 |    -1.517     0.418 |\n#>  -0.891 -1.085 | 0.183 0.271 |    -4.877    -4.013 |\n#>   3.808  3.948 | 0.048 0.016 |    80.133   251.321 |\n#>   2.305  2.271 | 0.072 0.149 |    32.038    15.214 |\n#>   2.042  1.991 | 0.082 0.051 |    24.923    39.148 |\n#>   1.735  1.801 | 0.092 0.110 |    18.769    16.401 |\n#>   2.357  2.400 | 0.068 0.093 |    34.505    25.743 |\n#>   2.072  2.086 | 0.057 0.044 |    36.269    47.523 |\n#>   1.724  1.758 | 0.067 0.076 |    25.718    23.035 |\n#>   2.330  2.294 | 0.049 0.074 |    47.308    30.997 |\n#>   0.262  0.279 | 0.056 0.053 |     4.671     5.292 |\n#>   0.405  0.384 | 0.104 0.222 |     3.887     1.729 |\n#>   0.594  0.578 | 0.083 0.099 |     7.189     5.844 |\n#>   0.010  0.051 | 0.003 0.019 |     3.283     2.630 |\n#>   0.060  0.011 | 0.016 0.004 |     3.701     3.089 |\n#>   0.397  0.448 | 0.044 0.070 |     9.011     6.373 |\n#>   0.358  0.312 | 0.119 0.061 |     3.008     5.143 |\n#>   0.636  0.797 | 0.134 0.178 |     4.738     4.479 |\n#>   0.560  0.654 | 0.069 0.069 |     8.137     9.507 |\n#>   0.298  0.328 | 0.038 0.032 |     7.771    10.250 |\n#>   0.309  0.377 | 0.048 0.067 |     6.484     5.626 |\n#>   0.435  0.449 | 0.045 0.054 |     9.689     8.370 |\n#>  m1_pval m2_pval\n#>  999.000   0.194\n#>  999.000   0.194\n#>  999.000   0.194\n#>  999.000   0.596\n#>  999.000   0.596\n#>  999.000   0.596\n#>  999.000   0.197\n#>  999.000   0.197\n#>  999.000   0.197\n#>  999.000   0.024\n#>  999.000   0.024\n#>  999.000   0.024\n#>  999.000   0.000\n#>  999.000   0.000\n#>  999.000   0.000\n#>  999.000   0.005\n#>  999.000   0.005\n#>  999.000   0.005\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.129   0.676\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.084\n#>    0.000   0.000\n#>    0.001   0.009\n#>    0.000   0.002\n#>    0.000   0.000\n#>    0.003   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#> \n#> \n#>   P-values that differ between models (p-value diff > 1e-04)\n#>   -----------------------------------\n#>    paramHeader      param                  LatentClass\n#>  BROAD_IN.WITH  ENJOYMENT                            1\n#>  BROAD_IN.WITH  ENJOYMENT                            2\n#>  BROAD_IN.WITH  ENJOYMENT                            3\n#>  BROAD_IN.WITH INSTRUMENT                            1\n#>  BROAD_IN.WITH INSTRUMENT                            2\n#>  BROAD_IN.WITH INSTRUMENT                            3\n#>  BROAD_IN.WITH SELF_EFFIC                            1\n#>  BROAD_IN.WITH SELF_EFFIC                            2\n#>  BROAD_IN.WITH SELF_EFFIC                            3\n#>  ENJOYMEN.WITH INSTRUMENT                            1\n#>  ENJOYMEN.WITH INSTRUMENT                            2\n#>  ENJOYMEN.WITH INSTRUMENT                            3\n#>  ENJOYMEN.WITH SELF_EFFIC                            1\n#>  ENJOYMEN.WITH SELF_EFFIC                            2\n#>  ENJOYMEN.WITH SELF_EFFIC                            3\n#>  INSTRUME.WITH SELF_EFFIC                            1\n#>  INSTRUME.WITH SELF_EFFIC                            2\n#>  INSTRUME.WITH SELF_EFFIC                            3\n#>          Means       C1#1 Categorical.Latent.Variables\n#>      Variances BROAD_INTE                            2\n#>      Variances  ENJOYMENT                            1\n#>      Variances  ENJOYMENT                            2\n#>      Variances INSTRUMENT                            1\n#>  m1_est m2_est . m1_se m2_se . m1_est_se m2_est_se .\n#>   0.000  0.010 | 0.000 0.008 |   999.000     1.300 |\n#>   0.000  0.010 | 0.000 0.008 |   999.000     1.300 |\n#>   0.000  0.010 | 0.000 0.008 |   999.000     1.300 |\n#>   0.000 -0.013 | 0.000 0.024 |   999.000    -0.530 |\n#>   0.000 -0.013 | 0.000 0.024 |   999.000    -0.530 |\n#>   0.000 -0.013 | 0.000 0.024 |   999.000    -0.530 |\n#>   0.000 -0.029 | 0.000 0.023 |   999.000    -1.290 |\n#>   0.000 -0.029 | 0.000 0.023 |   999.000    -1.290 |\n#>   0.000 -0.029 | 0.000 0.023 |   999.000    -1.290 |\n#>   0.000 -0.034 | 0.000 0.015 |   999.000    -2.252 |\n#>   0.000 -0.034 | 0.000 0.015 |   999.000    -2.252 |\n#>   0.000 -0.034 | 0.000 0.015 |   999.000    -2.252 |\n#>   0.000 -0.029 | 0.000 0.008 |   999.000    -3.702 |\n#>   0.000 -0.029 | 0.000 0.008 |   999.000    -3.702 |\n#>   0.000 -0.029 | 0.000 0.008 |   999.000    -3.702 |\n#>   0.000  0.056 | 0.000 0.020 |   999.000     2.777 |\n#>   0.000  0.056 | 0.000 0.020 |   999.000     2.777 |\n#>   0.000  0.056 | 0.000 0.020 |   999.000     2.777 |\n#>  -0.285  0.143 | 0.188 0.341 |    -1.517     0.418 |\n#>   0.405  0.384 | 0.104 0.222 |     3.887     1.729 |\n#>   0.010  0.051 | 0.003 0.019 |     3.283     2.630 |\n#>   0.060  0.011 | 0.016 0.004 |     3.701     3.089 |\n#>   0.358  0.312 | 0.119 0.061 |     3.008     5.143 |\n#>  m1_pval m2_pval\n#>  999.000   0.194\n#>  999.000   0.194\n#>  999.000   0.194\n#>  999.000   0.596\n#>  999.000   0.596\n#>  999.000   0.596\n#>  999.000   0.197\n#>  999.000   0.197\n#>  999.000   0.197\n#>  999.000   0.024\n#>  999.000   0.024\n#>  999.000   0.024\n#>  999.000   0.000\n#>  999.000   0.000\n#>  999.000   0.000\n#>  999.000   0.005\n#>  999.000   0.005\n#>  999.000   0.005\n#>    0.129   0.676\n#>    0.000   0.084\n#>    0.001   0.009\n#>    0.000   0.002\n#>    0.003   0.000\n#> \n#> \n#>   Parameters unique to model 1: 0\n#>   -----------------------------\n#> \n#>   None\n#> \n#> \n#>   Parameters unique to model 2: 0\n#>   -----------------------------\n#> \n#>  None\n#> \n#> \n#> =============="},{"path":"lpa-enumeration.html","id":"plot-comparison","chapter":"4 LPA Enumeration","heading":"4.9.3 Plot comparison","text":"can also plot comparisons look error bars.NOTE: plotMixtures() function used plotting LPA models (.e., means & variances)","code":"\na <- plotMixtures(output_enum$model_2_class_3.out,\n  ci = 0.95, bw = FALSE) \n\nb <- plotMixtures(output_enum$model_4_class_3.out,\n  ci = 0.95, bw = FALSE) \n\na + labs(title = \"Model 2\") +\n    theme(plot.title = element_text(size = 12)) +\nb + labs(title = \"Model 4\") +\n    theme(plot.title = element_text(size = 12))"},{"path":"lpa-enumeration.html","id":"visualization","chapter":"4 LPA Enumeration","heading":"4.10 Visualization","text":"","code":""},{"path":"lpa-enumeration.html","id":"latent-profile-plot","chapter":"4 LPA Enumeration","heading":"4.10.1 Latent Profile Plot","text":"Save figure","code":"\nsource(here(\"functions\", \"plot_lpa.R\"))\n\nplot_lpa(model_name = output_enum$model_3_class_4.out)\nggsave(here(\"figures\", \"model3_profile4.png\"), dpi = \"retina\", bg = \"white\", height=5, width=8, units=\"in\")"},{"path":"lpa-enumeration.html","id":"plots-means-and-variances","chapter":"4 LPA Enumeration","heading":"4.10.2 Plots Means and Variances","text":"","code":"\nplotMixtures(output_enum$model_3_class_4.out, ci = 0.95, bw = FALSE) + \n  labs(title = \"Model 3: Equal Variances, Equal Covariances\")"},{"path":"lpa-enumeration.html","id":"model-evaluation","chapter":"4 LPA Enumeration","heading":"4.11 Model Evaluation","text":"","code":""},{"path":"lpa-enumeration.html","id":"classifications-diagnostics-table","chapter":"4 LPA Enumeration","heading":"4.11.1 Classifications Diagnostics Table","text":"Use Mplus calculate k-class confidence intervals (Note: Change syntax make chosen *k*-class model):Create table","code":"\nclassification  <- mplusObject(\n  \n  TITLE = \"LPA - Calculated k-Class 95% CI\",\n  \n  VARIABLE = \n  \"usevar =  broad_interest-self_efficacy;\n   classes = c1(4);\",\n  \n  ANALYSIS = \n   \"estimator = ml; \n    type = mixture;    \n    starts = 0; \n    processors = 10;\n    optseed = 468036; ! This seed is taken from chosen model output\n    bootstrap = 1000;\",\n  \n  MODEL =\n    \" \n    ! This is copied and pasted from the chosen model input\n  %c1#1%\n  broad_interest (vbroad_interest);\n  enjoyment (venjoyment);\n  instrumental_mot (vinstrumental_mot);\n  self_efficacy (vself_efficacy);\n\n  broad_interest WITH enjoyment (broad_interestWenjoyment);\n  broad_interest WITH instrumental_mot (broad_interestWinstrumental_mot);\n  broad_interest WITH self_efficacy (broad_interestWself_efficacy);\n  enjoyment WITH instrumental_mot (enjoymentWinstrumental_mot);\n  enjoyment WITH self_efficacy (enjoymentWself_efficacy);\n  instrumental_mot WITH self_efficacy (instrumental_motWself_efficacy);\n\n  %c1#2%\n  broad_interest (vbroad_interest);\n  enjoyment (venjoyment);\n  instrumental_mot (vinstrumental_mot);\n  self_efficacy (vself_efficacy);\n\n  broad_interest WITH enjoyment (broad_interestWenjoyment);\n  broad_interest WITH instrumental_mot (broad_interestWinstrumental_mot);\n  broad_interest WITH self_efficacy (broad_interestWself_efficacy);\n  enjoyment WITH instrumental_mot (enjoymentWinstrumental_mot);\n  enjoyment WITH self_efficacy (enjoymentWself_efficacy);\n  instrumental_mot WITH self_efficacy (instrumental_motWself_efficacy);\n\n  %c1#3%\n  broad_interest (vbroad_interest);\n  enjoyment (venjoyment);\n  instrumental_mot (vinstrumental_mot);\n  self_efficacy (vself_efficacy);\n\n  broad_interest WITH enjoyment (broad_interestWenjoyment);\n  broad_interest WITH instrumental_mot (broad_interestWinstrumental_mot);\n  broad_interest WITH self_efficacy (broad_interestWself_efficacy);\n  enjoyment WITH instrumental_mot (enjoymentWinstrumental_mot);\n  enjoyment WITH self_efficacy (enjoymentWself_efficacy);\n  instrumental_mot WITH self_efficacy (instrumental_motWself_efficacy);\n\n  %c1#4%\n  broad_interest (vbroad_interest);\n  enjoyment (venjoyment);\n  instrumental_mot (vinstrumental_mot);\n  self_efficacy (vself_efficacy);\n\n  broad_interest WITH enjoyment (broad_interestWenjoyment);\n  broad_interest WITH instrumental_mot (broad_interestWinstrumental_mot);\n  broad_interest WITH self_efficacy (broad_interestWself_efficacy);\n  enjoyment WITH instrumental_mot (enjoymentWinstrumental_mot);\n  enjoyment WITH self_efficacy (enjoymentWself_efficacy);\n  instrumental_mot WITH self_efficacy (instrumental_motWself_efficacy);\n  \n  \n  !CHANGE THIS SECTION TO YOUR CHOSEN k-CLASS MODEL\n    \n  %OVERALL%\n  [C1#1](c1);\n  [C1#2](c2);\n  [C1#3](c3);\n\n  Model Constraint:\n  New(p1 p2 p3 p4);\n  \n  p1 = exp(c1)/(1+exp(c1)+exp(c2)+exp(c3));\n  p2 = exp(c2)/(1+exp(c1)+exp(c2)+exp(c3));\n  p3 = exp(c3)/(1+exp(c1)+exp(c2)+exp(c3));  \n  p4 = 1/(1+exp(c1)+exp(c2)+exp(c3));\",\n\n  \n  OUTPUT = \"cinterval(bcbootstrap)\",\n  \n  usevariables = colnames(pisa),\n  rdata = pisa)\n\nclassification_fit <- mplusModeler(classification,\n                dataout=here(\"lpa\", \"mplus\", \"class.dat\"),\n                modelout=here(\"lpa\", \"mplus\", \"class.inp\") ,\n                check=TRUE, run = TRUE, hashfilename = FALSE)\nsource(here(\"functions\", \"diagnostics_table.R\"))\n\nclass_output <- readModels(here(\"mplus\", \"class.out\"))\n\ndiagnostics_table(class_output)"},{"path":"lpa-enumeration.html","id":"profile-homogeneity","chapter":"4 LPA Enumeration","heading":"4.11.2 Profile Homogeneity","text":"Profile homogeneity model-estimated within-profile variances indicator m across k-profiles comparing total overall sample variance:\\[ \\frac{\\theta_{mk}}{\\theta_{m}} \\]","code":"\n# Change this to look at your chosen LPA model\n\nchosen_model <- output_enum$model_3_class_4.out\n\n\n# Extract overall and profile-specifc variances\noverall_variances <- data.frame(chosen_model$sampstat$univariate.sample.statistics) %>% \n  select(Variance) %>% \n  rownames_to_column(var = \"item\") %>% \n  clean_names() %>% \n  mutate(item = str_sub(item, 1, 8))\n\nprofile_variances <- data.frame(chosen_model$parameters$unstandardized) %>% \n  filter(paramHeader == \"Variances\") %>% \n  select(param, est, LatentClass) %>% \n  rename(item = param,\n         variance = est,\n         k = LatentClass) %>% \n  mutate(k = paste0(\"Profile \", k)) %>% \n  mutate(item = str_sub(item, 1, 8))\n\n  \n# Evaluate the ratio\nhomogeneity <- profile_variances %>%\n  left_join(overall_variances, by = \"item\") %>%\n  mutate(homogeneity_ratio = round((variance.x / variance.y),2))\n\n# Create a gt table\nhomogeneity %>%\n  select(homogeneity_ratio, k, item) %>% \n  pivot_wider(\n    names_from = k, \n    values_from = homogeneity_ratio\n  ) %>%\n  gt() %>%\n  cols_label(\n    item = \"Item\"\n  ) %>%\n  fmt_number(\n    columns = everything(),\n    decimals = 3\n  ) %>%\n  tab_header(\n    title = \"Profile Homogeneity Table\"\n  ) %>%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  )"},{"path":"lpa-enumeration.html","id":"profile-separation","chapter":"4 LPA Enumeration","heading":"4.11.3 Profile Separation","text":"can evaluate degree profile separation assessing actual distance profile-specific means.\nquantify profile separation Profile j Profile k respect particular item m, compute standard mean difference:\\[\\hat{d}_{mjk}= \\frac{{\\hat{\\alpha_{mj}}-\\hat{\\alpha_{mk}}}}{\\sigma_{mjk}}\\] Pooled variance:\\[\\hat{\\sigma}_{mj k} = \\sqrt{\\frac{(\\hat{\\pi}_j)(n)(\\hat{\\theta}_{mj}) + (\\hat{\\pi}_k)(n)(\\hat{\\theta}_{mk})}{(\\hat{\\pi}_j + \\hat{\\pi}_k) n}}\\]","code":"\n# Change this to look at your chosen LPA model\nchosen_model <- output_enum$model_3_class_4.out\n\n\n# Profile-specific means\nprofile_means <- data.frame(chosen_model$parameters$unstandardized) %>% \n  filter(paramHeader == \"Means\") %>%\n  filter(!str_detect(param, \"#\")) %>% \n  select(param, est, LatentClass) %>% \n  rename(item = param,\n         means = est,\n         k = LatentClass) %>% \n  mutate(k = paste0(\"Profile \", k)) %>% \n  mutate(item = str_sub(item, 1, 8))\n\n# Relative profile sizes\nprofile_sizes <- data.frame(chosen_model$class_counts$modelEstimated) %>% \n  rename(size = proportion,\n         k = class) %>% \n  mutate(k = paste0(\"Profile \", k)) %>% \n  select(-count)\n\n# Sample size\nn <- chosen_model$summaries$Observations\n\n# Profile-specific variance\nprofile_variances <- data.frame(chosen_model$parameters$unstandardized) %>% \n  filter(paramHeader == \"Variances\") %>% \n  select(param, est, LatentClass) %>% \n  rename(item = param,\n         variance = est,\n         k = LatentClass) %>% \n  mutate(k = paste0(\"Profile \", k)) %>% \n  mutate(item = str_sub(item, 1, 8))\n\n# Combine profile variances with profile sizes\nvariance_with_sizes <- profile_variances %>%\n  left_join(profile_sizes, by = \"k\")\n\n# Create the combinations\nprofile_combinations <- data.frame(comboGrid(k1 = unique(profile_sizes$k), k2 = unique(profile_sizes$k))) %>%\n  filter(k1 != k2) %>%  \n  arrange(k1, k2)\n\n# Calculate pooled variance for each item across the profiles\ncombined_results <- profile_combinations %>%\n  rowwise() %>%\n  do({\n    pair <- .\n    k1 <- pair$k1\n    k2 <- pair$k2\n    \n    # Filter for the two profiles (variance)\n    data_k1_var <- variance_with_sizes %>% filter(k == k1)\n    data_k2_var <- variance_with_sizes %>% filter(k == k2)\n    \n    # Filter for the two profiles (means)\n    data_k1_mean <- profile_means %>% filter(k == k1)\n    data_k2_mean <- profile_means %>% filter(k == k2)\n    \n    # Combine variance data for the two profiles\n    variance_data <- data_k1_var %>%\n      inner_join(data_k2_var, by = \"item\", suffix = c(\"_k1\", \"_k2\")) %>%\n      mutate(\n        pooled_variance = sqrt(\n          ((size_k1 * n * variance_k1) + (size_k2 * n * variance_k2)) / ((size_k1 + size_k2) * n)\n        ),\n        comparison = paste(k1, \"vs\", k2)\n      ) %>%\n      select(item, pooled_variance, comparison)\n    \n    # Combine mean data for the two profilees\n    mean_data <- data_k1_mean %>%\n      inner_join(data_k2_mean, by = \"item\", suffix = c(\"_k1\", \"_k2\")) %>%\n      mutate(\n        mean_diff = means_k1 - means_k2,\n        comparison = paste(k1, \"vs\", k2)\n      ) %>%\n      select(item, mean_diff, comparison)\n    \n    # Combine both variance and mean differences data\n    combined_data <- variance_data %>%\n      left_join(mean_data, by = c(\"item\", \"comparison\")) %>% \n      mutate(\n         mean_diff_by_pooled_variance = mean_diff / pooled_variance\n      )\n    \n    combined_data\n  }) %>%\n  bind_rows()\n\n# Create a gt table\ncombined_results %>%\n  select(mean_diff_by_pooled_variance, comparison ,item) %>% \n  pivot_wider(\n    names_from = comparison, \n    values_from = mean_diff_by_pooled_variance\n  ) %>%\n  gt() %>%\n  cols_label(\n    item = \"Item\"\n  ) %>%\n  fmt_number(\n    columns = everything(),\n    decimals = 3\n  ) %>%\n  tab_header(\n    title = \"Profile Separation Table\"\n  ) %>%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  )"},{"path":"including-auxiliary-variables.html","id":"including-auxiliary-variables","chapter":"5 Including Auxiliary Variables","heading":"5 Including Auxiliary Variables","text":"Example: PISA Student DataData source:first example utilizes dataset undergraduate Cheating available poLCA package (Dayton, 1998): See documentation hereThe first example utilizes dataset undergraduate Cheating available poLCA package (Dayton, 1998): See documentation hereThe second examples utilizes public-use dataset, Longitudinal Survey American Youth (LSAY): See documentation hereThe second examples utilizes public-use dataset, Longitudinal Survey American Youth (LSAY): See documentation ","code":""},{"path":"including-auxiliary-variables.html","id":"load-packages-1","chapter":"5 Including Auxiliary Variables","heading":"5.1 Load packages","text":"","code":"\nlibrary(MplusAutomation)\nlibrary(tidyverse) #collection of R packages designed for data science\nlibrary(here) #helps with filepaths\nlibrary(janitor) #clean_names\nlibrary(gt) # create tables\nlibrary(cowplot) # a ggplot theme\nlibrary(DiagrammeR) # create path diagrams\nlibrary(glue) # allows us to paste expressions into R code\nlibrary(data.table) # used for `melt()` function  \nlibrary(poLCA)\nlibrary(reshape2)"},{"path":"including-auxiliary-variables.html","id":"automated-three-step","chapter":"5 Including Auxiliary Variables","heading":"5.2 Automated Three-Step","text":"Note: Prior adding covariates distals enumeration must conducted.\nSee Lab 6 examples enumeration MplusAutomation.Application: Undergraduate Cheating behavior“Dichotomous self-report responses 319 undergraduates four questions cheating behavior” (poLCA, 2016).Prepare data","code":"\n\ndata(cheating)\n\ncheating <- cheating %>% clean_names() \n\ndf_cheat <-  cheating %>%                                  \n  dplyr::select(1:4) %>%                                  \n  mutate_all(funs(.-1)) %>%                                \n  mutate(gpa = cheating$gpa)\n\n# Detaching packages that mask the dpylr functions \ndetach(package:poLCA, unload = TRUE)\ndetach(package:MASS, unload = TRUE)"},{"path":"including-auxiliary-variables.html","id":"du3step","chapter":"5 Including Auxiliary Variables","heading":"5.2.1 DU3STEP","text":"DU3STEP incorporates distal outcome variables (assumed unequal means variances) mixture models.","code":""},{"path":"including-auxiliary-variables.html","id":"run-the-du3step-model-with-gpa-as-distal-outcome","chapter":"5 Including Auxiliary Variables","heading":"5.2.1.1 Run the DU3step model with gpa as distal outcome","text":"","code":"\n\nm_stepdu  <- mplusObject(\n  TITLE = \"DU3STEP - GPA as Distal\", \n  VARIABLE = \n   \"categorical = lieexam-copyexam; \n    usevar = lieexam-copyexam;\n    auxiliary = gpa (du3step);\n    classes = c(2);\",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 500 100; \n    processors = 10;\",\n  \n  OUTPUT = \"sampstat patterns tech11 tech14;\",\n  \n  PLOT = \n    \"type = plot3; \n     series = lieexam-copyexam(*);\",\n  \n  usevariables = colnames(df_cheat),\n  rdata = df_cheat)\n\nm_stepdu_fit <- mplusModeler(m_stepdu, \n                            dataout=here(\"three_step\", \"auto_3step\", \"du3step.dat\"),\n                            modelout=here(\"three_step\", \"auto_3step\", \"c2_du3step.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"including-auxiliary-variables.html","id":"plot-distal-outcome","chapter":"5 Including Auxiliary Variables","heading":"5.2.1.2 Plot Distal Outcome","text":"","code":"\nmodelParams <- readModels(here(\"three_step\", \"auto_3step\", \"c2_du3step.out\"))\n\n# Extract class size \nc_size <- as.data.frame(modelParams[[\"class_counts\"]][[\"modelEstimated\"]][[\"proportion\"]]) %>% \n  rename(\"cs\" = 1) %>% \n  mutate(cs = round(cs*100, 2))\n\nc_size_val <- paste0(\"C\", 1:nrow(c_size), glue(\" ({c_size[1:nrow(c_size),]}%)\"))\n\n\n# Extract information as data frame\nestimates <- as.data.frame(modelParams[[\"lcCondMeans\"]][[\"overall\"]]) %>%\n  reshape2::melt(id.vars = \"var\") %>%\n  mutate(variable = as.character(variable),\n         LatentClass = case_when(\n           endsWith(variable, \"1\") ~ c_size_val[1],\n           endsWith(variable, \"2\") ~ c_size_val[2])) %>% #Add to this based on the number of classes you have\n  head(-3) %>% \n  pivot_wider(names_from = variable, values_from = value) %>% \n  unite(\"mean\", contains(\"m\"), na.rm = TRUE) %>% \n  unite(\"se\", contains(\"se\"), na.rm = TRUE) %>% \n  mutate(across(c(mean, se), as.numeric))\n\n# Add labels (NOTE: You must change the labels to match the significance testing!!) \nvalue_labels <- paste0(estimates$mean, c(\" a\",\" b\"))\n\n# Plot bar graphs\nestimates %>%\n  ggplot(aes(fill = LatentClass, y = mean, x = LatentClass)) +\n  geom_bar(position = \"dodge\", stat = \"identity\", color = \"black\") +\n  geom_errorbar(aes(ymin=mean-se, ymax=mean+se),\n                size=.3,    \n                width=.2,\n                position=position_dodge(.9)) +\n  geom_text(aes(y = mean, label = value_labels), \n            family = \"serif\", size = 4,\n            position=position_dodge(.9),\n            vjust = 8) +\n  #scale_fill_grey(start = .5, end = .7) +\n  labs(y=\"GPA\", x=\"\") +\n  theme_cowplot() +\n  theme(text = element_text(family = \"serif\", size = 12),\n        axis.text.x = element_text(size=12),\n        legend.position=\"none\") +\n  coord_cartesian(expand = FALSE, \n                  ylim=c(0,max(estimates$mean*1.5))) # Change ylim based on distal outcome rang\n\n\n# Save plot\nggsave(here(\"figures\",\"Du3STEP_plot.jpeg\"),                \n       dpi=300, width=10, height = 7, units=\"in\")  "},{"path":"including-auxiliary-variables.html","id":"r3step","chapter":"5 Including Auxiliary Variables","heading":"5.2.2 R3STEP","text":"R3STEP incorporates latent class predictors mixture models.","code":""},{"path":"including-auxiliary-variables.html","id":"run-the-r3step-model-with-gpa-as-the-latent-class-predictor","chapter":"5 Including Auxiliary Variables","heading":"5.2.2.1 Run the R3STEP model with gpa as the latent class predictor","text":"","code":"\n\nm_stepr  <- mplusObject(\n  TITLE = \"R3STEP - GPA as Predictor\", \n  VARIABLE = \n   \"categorical = lieexam-copyexam; \n    usevar = lieexam-copyexam;\n    auxiliary = gpa (R3STEP);\n    classes = c(2);\",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 500 100; \n    processors = 10;\",\n  \n  OUTPUT = \"sampstat patterns tech11 tech14;\",\n  \n  PLOT = \n    \"type = plot3; \n     series = lieexam-copyexam(*);\",\n  \n  usevariables = colnames(df_cheat),\n  rdata = df_cheat)\n\nm_stepr_fit <- mplusModeler(m_stepr, \n                            dataout=here(\"three_step\", \"auto_3step\", \"r3step.dat\"),\n                            modelout=here(\"three_step\", \"auto_3step\", \"c2_r3step.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"including-auxiliary-variables.html","id":"regression-slopes-and-odds-ratios","chapter":"5 Including Auxiliary Variables","heading":"5.2.2.2 Regression slopes and odds ratios","text":"","code":"TESTS OF CATEGORICAL LATENT VARIABLE MULTINOMIAL LOGISTIC REGRESSIONS USING\nTHE 3-STEP PROCEDURE\n\n   WARNING:  LISTWISE DELETION IS APPLIED TO THE AUXILIARY VARIABLES IN THE\n   ANALYSIS.  TO AVOID LISTWISE DELETION, DATA IMPUTATION CAN BE USED\n   FOR THE AUXILIARY VARIABLES FOLLOWED BY ANALYSIS WITH TYPE=IMPUTATION.\n   NUMBER OF DELETED OBSERVATIONS:  4\n   NUMBER OF OBSERVATIONS USED:  315\n\n                                                    Two-Tailed\n                    Estimate       S.E.  Est./S.E.    P-Value\n\n C#1        ON\n    GPA               -0.698      0.255     -2.739      0.006\n\n Intercepts\n    C#1               -0.241      0.460     -0.523      0.601\n\nParameterization using Reference Class 1\n\n C#2        ON\n    GPA                0.698      0.255      2.739      0.006\n\n Intercepts\n    C#2                0.241      0.460      0.523      0.601\n\n\nODDS RATIOS FOR TESTS OF CATEGORICAL LATENT VARIABLE MULTINOMIAL LOGISTIC REGRESSIONS\nUSING THE 3-STEP PROCEDURE\n\n                                                95% C.I.\n                    Estimate       S.E. Lower 2.5% Upper 2.5%\n\n C#1        ON\n    GPA                0.498      0.127      0.302      0.820\n\n\nParameterization using Reference Class 1\n\n C#2        ON\n    GPA                2.009      0.512      1.220      3.310"},{"path":"including-auxiliary-variables.html","id":"manual-three-step","chapter":"5 Including Auxiliary Variables","heading":"5.3 Manual Three-step","text":"Integrate covariates distals mixture modelApplication: Longitudinal Study American Youth, Science AttitudesThe data can found data folder called lsay_subset.csv.","code":"\nlsay_data <- read_csv(here(\"three_step\",\"data\",\"lsay_subset.csv\")) %>% \n  clean_names() %>%   # make variable names lowercase\n  mutate(female = recode(gender, `1` = 0, `2` = 1)) # relabel values from 1,2 to 0,1"},{"path":"including-auxiliary-variables.html","id":"ml-method","chapter":"5 Including Auxiliary Variables","heading":"5.3.1 ML Method","text":"","code":""},{"path":"including-auxiliary-variables.html","id":"step-1---class-enumeration-w-auxiliary-specification","chapter":"5 Including Auxiliary Variables","heading":"5.3.1.1 Step 1 - Class Enumeration w/ Auxiliary Specification","text":"step done class enumeration (selected best latent class model). example, four class model best. Now, re-estimating four-class model using optseed efficiency. difference SAVEDATA command, can save posterior probabilities modal class assignment steps two three.","code":"\n\nstep1  <- mplusObject(\n  TITLE = \"Step 1 - Three-Step using LSAL\", \n  VARIABLE = \n  \"categorical = enjoy useful logical job adult; \n   usevar = enjoy useful logical job adult;\n    \n   classes = c(4); \n    \n   auxiliary =   ! list all potential covariates and distals here\n   female        ! covariate\n   math_irt;      ! distal math test score in 12th grade \",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 0;\n    optseed = 568405;\",\n  \n  SAVEDATA = \n   \"File=3step_savedata.dat;\n    Save=cprob;\",\n  \n  OUTPUT = \"residual tech11 tech14\",\n  \n  PLOT = \n    \"type = plot3; \n    series = enjoy-adult(*);\",\n  \n  usevariables = colnames(lsay_data),\n  rdata = lsay_data)\n\nstep1_fit <- mplusModeler(step1,\n                            dataout=here(\"three_step\", \"manual_3step\", \"Step1.dat\"),\n                            modelout=here(\"three_step\", \"manual_3step\", \"one.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\nsource(\"plot_lca.txt\")\noutput_lsay <- readModels(here(\"three_step\", \"manual_3step\",\"one.out\"))\n\nplot_lca(model_name = output_lsay)"},{"path":"including-auxiliary-variables.html","id":"step-2---determine-measurement-error","chapter":"5 Including Auxiliary Variables","heading":"5.3.1.2 Step 2 - Determine Measurement Error","text":"Extract logits classification probabilities likely latent classExtract saved dataset part mplusObject “step1_fit”Rename column savedata named “C” change “N”","code":"\nlogit_cprobs <- as.data.frame(output_lsay[[\"class_counts\"]]\n                                       [[\"logitProbs.mostLikely\"]])\nsavedata <- as.data.frame(output_lsay[[\"savedata\"]])\ncolnames(savedata)[colnames(savedata)==\"C\"] <- \"N\""},{"path":"including-auxiliary-variables.html","id":"step-3---add-auxiliary-variables","chapter":"5 Including Auxiliary Variables","heading":"5.3.1.3 Step 3 - Add Auxiliary Variables","text":"Model 1 covariate 1 distal outcome","code":"\nstep3  <- mplusObject(\n  TITLE = \"Step3 - 3step LSAY\", \n  \n  VARIABLE = \n \"nominal=N;\n  usevar = n;\n  \n  classes = c(4);\n  \n  usevar = female math_irt;\" ,\n  \n  ANALYSIS = \n \"estimator = mlr; \n  type = mixture; \n  starts = 0;\",\n \n  DEFINE = \n   \"center female (grandmean);\",\n  \n  MODEL =\n  glue(\n \" %OVERALL%\n \n  math_irt on female; ! covariate as a predictor of the distal outcome\n  C on female;        ! covariate as predictor of C\n\n     %C#1%\n  [n#1@{logit_cprobs[1,1]}]; ! MUST EDIT if you do not have a 4-class model. \n  [n#2@{logit_cprobs[1,2]}];\n  [n#3@{logit_cprobs[1,3]}];\n  \n  [math_irt](m1);    ! conditional distal mean \n  math_irt;          ! conditional distal variance (freely estimated)\n\n  %C#2%\n  [n#1@{logit_cprobs[2,1]}];\n  [n#2@{logit_cprobs[2,2]}];\n  [n#3@{logit_cprobs[2,3]}];\n  \n  [math_irt](m2);\n  math_irt;\n  \n  %C#3%\n  [n#1@{logit_cprobs[3,1]}];\n  [n#2@{logit_cprobs[3,2]}];\n  [n#3@{logit_cprobs[3,3]}];\n  \n  [math_irt](m3);\n  math_irt;\n\n  %C#4%\n  [n#1@{logit_cprobs[4,1]}];\n  [n#2@{logit_cprobs[4,2]}];\n  [n#3@{logit_cprobs[4,3]}];\n  \n  [math_irt](m4);\n  math_irt; \"),\n  \n  MODELCONSTRAINT = \n   \"New (diff12 diff13 diff23 \n    diff14 diff24 diff34);\n  \n    diff12 = m1-m2;  ! test pairwise distal mean differences\n    diff13 = m1-m3;\n    diff23 = m2-m3;\n    diff14 = m1-m4;\n    diff24 = m2-m4;\n    diff34 = m3-m4;\",\n  \n  MODELTEST = \"     ! omnibus test of distal means \n    m1=m2;\n    m2=m3;\n    m3=m4;\",\n \n  usevariables = colnames(savedata), \n  rdata = savedata)\n\nstep3_fit <- mplusModeler(step3,\n               dataout=here(\"three_step\", \"manual_3step\", \"Step3.dat\"), \n               modelout=here(\"three_step\", \"manual_3step\", \"three.inp\"), \n               check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"including-auxiliary-variables.html","id":"wald-test-table","chapter":"5 Including Auxiliary Variables","heading":"5.3.1.3.1 Wald Test Table","text":"Save figure","code":"\nmodelParams <- readModels(here(\"three_step\", \"manual_3step\", \"three.out\"))\n\n# Extract information as data frame\nwald <- as.data.frame(modelParams[[\"summaries\"]]) %>%\n  dplyr::select(WaldChiSq_Value:WaldChiSq_PValue) %>% \n  mutate(WaldChiSq_DF = paste0(\"(\", WaldChiSq_DF, \")\")) %>% \n  unite(wald_test, WaldChiSq_Value, WaldChiSq_DF, sep = \" \") %>% \n  rename(pval = WaldChiSq_PValue) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001))))\n\n# Create table\n\nwald_table <- wald %>% \n  gt() %>%\n    tab_header(\n    title = \"Wald Test of Paramter Constraints (Math)\") %>%\n    cols_label(\n      wald_test = md(\"Wald Test (*df*)\"),\n      pval = md(\"*p*-value\")) %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\")\n\nwald_table\ngtsave(wald_table, here(\"figures\",\"wald_table.docx\"))"},{"path":"including-auxiliary-variables.html","id":"table-of-distal-outcome-differences","chapter":"5 Including Auxiliary Variables","heading":"5.3.1.3.2 Table of Distal Outcome Differences","text":"","code":"\n# Extract information as data frame\ndiff <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n  filter(grepl(\"DIFF\", param)) %>% \n  dplyr::select(param:pval) %>% \n  mutate(se = paste0(\"(\", format(round(se,2), nsmall =2), \")\")) %>% \n  unite(estimate, est, se, sep = \" \") %>% \n  mutate(param = str_remove(param, \"DIFF\"),\n         param = as.numeric(param)) %>% \n  separate(param, into = paste0(\"Group\", 1:2), sep = 1) %>% \n  mutate(class = paste0(\"Class \", Group1, \" vs \", Group2)) %>% \n  select(class, estimate, pval) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001))))\n\n# Create table\n\ndiff %>% \n  gt() %>%\n    tab_header(\n    title = \"Distal Outcome Differences\") %>%\n    cols_label(\n      class = \"Class\",\n      estimate = md(\"Mean (*se*)\"),\n      pval = md(\"*p*-value\")) %>% \n    sub_missing(1:3,\n              missing_text = \"\") %>%\n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\")"},{"path":"including-auxiliary-variables.html","id":"plot-distal-outcome-1","chapter":"5 Including Auxiliary Variables","heading":"5.3.1.3.3 Plot Distal Outcome","text":"","code":"\n# Extract class size \nc_size <- as.data.frame(modelParams[[\"class_counts\"]][[\"modelEstimated\"]][[\"proportion\"]]) %>% \n  rename(\"cs\" = 1) %>% \n  mutate(cs = round(cs*100, 2))\n\nc_size_val <- paste0(\"C\", 1:nrow(c_size), glue(\" ({c_size[1:nrow(c_size),]}%)\"))\n\n\n# Extract information as data frame\nestimates <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n  filter(paramHeader == \"Intercepts\") %>%\n  dplyr::select(param, est, se) %>% \n  filter(param == \"MATH_IRT\") %>% \n  mutate(across(c(est, se), as.numeric)) %>% \n  mutate(LatentClass = c_size_val)\n\n# Add labels (NOTE: You must change the labels to match the significance testing!!) \n#value_labels <- paste0(estimates$est, c(\"a\",\" bc\",\" abd\",\" cd\"))\n\nestimates$LatentClass <- fct_inorder(estimates$LatentClass)\n\n# Plot bar graphs\nestimates %>%\n  ggplot(aes(x=LatentClass, y = est, fill = LatentClass)) +\n  geom_col(position = \"dodge\", stat = \"identity\", color = \"black\") +\n  geom_errorbar(aes(ymin=est-se, ymax=est+se),\n                size=.3,    # Thinner lines\n                width=.2,\n                position=position_dodge(.9)) +\n  geom_text(aes(label = est), \n            family = \"serif\", size = 4,\n            position=position_dodge(.9),\n            vjust = 8) +  \n # scale_fill_grey(start = .4, end = .7) + # Remove for colorful bars\n  labs(y=\"Math Scores\", x=\"\") +\n  theme_cowplot() +\n  theme(text = element_text(family = \"serif\", size = 15),\n        axis.text.x = element_text(size=15),\n        legend.position=\"none\")\n\n# Save plot\nggsave(here(\"figures\",\"ManualDistal_Plot.jpeg\"),              \n       dpi=300, width=10, height = 7, units=\"in\") "},{"path":"including-auxiliary-variables.html","id":"d-on-x","chapter":"5 Including Auxiliary Variables","heading":"5.3.1.3.4 D on X","text":"relation distal outcome (Math IRT Scores) covariate (Gender)?","code":"\n# Extract information as data frame\ncov <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n  filter(param == \"FEMALE\") %>% \n  mutate(param = str_replace(param, \"FEMALE\", \"Gender\"))%>% \n  mutate(LatentClass = sub(\"^\",\"Class \", LatentClass)) %>%  \n  dplyr::select(!paramHeader) %>% \n  mutate(se = paste0(\"(\", format(round(se,2), nsmall =2), \")\")) %>% \n    unite(estimate, est, se, sep = \" \") %>% \n  select(param, estimate, pval) %>% \n  distinct(param, .keep_all=TRUE) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001))))\n\n\n# Create table\n\ncov %>% \n  gt(groupname_col = \"LatentClass\", rowname_col = \"param\") %>%\n  tab_header(\n    title = \"Gender Predicting Math Scores\") %>%\n  cols_label(\n    estimate = md(\"Estimate (*se*)\"),\n    pval = md(\"*p*-value\")) %>% \n  sub_missing(1:3,\n              missing_text = \"\") %>%\n  sub_values(values = c(\"999.000\"), replacement = \"-\") %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\")"},{"path":"polytomous-lca.html","id":"polytomous-lca","chapter":"6 Polytomous LCA","heading":"6 Polytomous LCA","text":"Polytomous LCA deals variables two categories, survey questions responses like never, sometimes, always. workflow polytomous LCA model similar LCA model binary indicators. However, polytomous LCA captures complex response patterns, can make interpretation bit trickier. following code demonstrates example, along visualization model.","code":""},{"path":"polytomous-lca.html","id":"example-elections","chapter":"6 Polytomous LCA","heading":"6.1 Example: Elections","text":"“Two sets six questions four responses , asking respondents’ opinions well various traits describe presidential candidates Al Gore George W. Bush. Also potential covariates vote choice, age, education, gender, party ID. Source: National Election Studies (2000).” (poLCA, 2016) See documentation hereTwo sets six questions four responses , asking respondents’ opinions well various traits describe presidential candidates Al Gore George W. Bush. election data set, respondents 2000 American National Election Study public opinion poll asked evaluate well series traits—moral, caring, knowledgeable, good leader, dishonest, intelligent—described presidential candidates Al Gore George W. Bush. question four possible choices: (1) extremely well; (2) quite well; (3) well; (4) well .Load packages","code":"\n\nlibrary(poLCA)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(gt)\nlibrary(MplusAutomation)\nlibrary(here)\nlibrary(glue)"},{"path":"polytomous-lca.html","id":"prepare-data-2","chapter":"6 Polytomous LCA","heading":"6.2 Prepare Data","text":"","code":"\ndata(election)\n\n# Detaching packages that mask the dpylr functions \ndetach(package:poLCA, unload = TRUE)\ndetach(package:MASS, unload = TRUE)\n\ndf_election <-  election %>% \n  clean_names() %>% \n  select(moralb:intelb) %>% \n  mutate(across(everything(), \n                ~ as.factor(as.numeric(gsub(\"\\\\D\", \"\", .))), \n                .names = \"{.col}1\")) \n\n# Quick summary\nsummary(df_election)\n#>                moralb                  caresb   \n#>  1 Extremely well :340   1 Extremely well :155  \n#>  2 Quite well     :841   2 Quite well     :625  \n#>  3 Not too well   :330   3 Not too well   :562  \n#>  4 Not well at all: 98   4 Not well at all:342  \n#>  NA's             :176   NA's             :101  \n#>                knowb                   leadb    \n#>  1 Extremely well :274   1 Extremely well :266  \n#>  2 Quite well     :933   2 Quite well     :842  \n#>  3 Not too well   :379   3 Not too well   :407  \n#>  4 Not well at all:133   4 Not well at all:166  \n#>  NA's             : 66   NA's             :104  \n#>               dishonb                  intelb    moralb1   \n#>  1 Extremely well : 70   1 Extremely well :329   1   :340  \n#>  2 Quite well     :288   2 Quite well     :967   2   :841  \n#>  3 Not too well   :653   3 Not too well   :306   3   :330  \n#>  4 Not well at all:574   4 Not well at all:110   4   : 98  \n#>  NA's             :200   NA's             : 73   NA's:176  \n#>  caresb1     knowb1     leadb1    dishonb1   intelb1   \n#>  1   :155   1   :274   1   :266   1   : 70   1   :329  \n#>  2   :625   2   :933   2   :842   2   :288   2   :967  \n#>  3   :562   3   :379   3   :407   3   :653   3   :306  \n#>  4   :342   4   :133   4   :166   4   :574   4   :110  \n#>  NA's:101   NA's: 66   NA's:104   NA's:200   NA's: 73"},{"path":"polytomous-lca.html","id":"descriptive-statistics-2","chapter":"6 Polytomous LCA","heading":"6.3 Descriptive Statistics","text":"","code":"\nds <- df_election %>% \n  pivot_longer(moralb1:intelb1, names_to = \"variable\") %>% \n  count(variable, value) %>%  # Count occurrences of each value for each variable\n  group_by(variable) %>%\n  mutate(prop = n / sum(n)) %>% \n  arrange(desc(variable))\n\n# Create the table\nprop_table <- ds %>% \n  gt() %>% \n  tab_header(title = md(\"**Descriptive Summary**\")) %>%\n  cols_label(\n    variable = \"Variable\",\n    n = md(\"*N*\"),\n    prop = md(\"Proportion\")\n  ) %>%\n  fmt_number(c(\"n\", \"prop\"), decimals = 2) %>%  # Format both n and prop columns\n  cols_align(\n    align = \"center\",\n    columns = c(prop, n)\n  ) \n\n# View the table\nprop_table\n\n# Save as a Word doc\n#gtsave(prop_table, here(\"figures\", \"prop_table.docx\"))"},{"path":"polytomous-lca.html","id":"enumeration-2","chapter":"6 Polytomous LCA","heading":"6.4 Enumeration","text":"code uses mplusObject function MplusAutomation package.","code":"\n\nlca_enumeration  <- lapply(1:6, function(k) {\n  lca_enum  <- mplusObject(\n      \n    TITLE = glue(\"{k}-Class\"), \n  \n    VARIABLE = glue(\n    \"categorical = moralb1-intelb1; \n     usevar = moralb1-intelb1;\n     classes = c({k}); \"),\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 500 100; \n    processors = 10;\",\n  \n  OUTPUT = \"sampstat residual tech11 tech14 svalues;\",\n\n  \n  usevariables = colnames(df_election),\n  rdata = df_election)\n\nlca_enum_fit <- mplusModeler(lca_enum, \n                            dataout=glue(here(\"poLCA\", \"election.dat\")),\n                            modelout=glue(here(\"poLCA\", \"c{k}_election.inp\")) ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"polytomous-lca.html","id":"table-of-fit-2","chapter":"6 Polytomous LCA","heading":"6.4.1 Table of Fit","text":"Save table:","code":"\nsource(here(\"functions\",\"enum_table.R\"))\n\noutput_election <- readModels(here(\"poLCA\"), filefilter = \"election\", quiet = TRUE)\n\n# To see rows:\n#seeRows(output_election)\n\n# Arguments for `enum_table`\n# 1. readModels objects\n# 2-5. Rows of successfully estimated models \nfit_table <- enum_table(output_election, 1:6)\nfit_table\ngtsave(fit_table, here(\"figures\", \"fit_table.png\"))"},{"path":"polytomous-lca.html","id":"information-criteria-plot-2","chapter":"6 Polytomous LCA","heading":"6.4.2 Information Criteria Plot","text":"Save figure:","code":"\nic_plot(output_election)\nggsave(here(\"figures\", \"info_criteria.png\"), dpi=\"retina\", bg = \"white\", height=5, width=7, units=\"in\")"},{"path":"polytomous-lca.html","id":"class-probability-plot-1","chapter":"6 Polytomous LCA","heading":"6.4.3 4-Class Probability Plot","text":"functions poLCA_stacked poLCA_grouped create visualizations class probabilities LCA polytomous indicators. function takes following arguments:model_name: LCA model read R using readModels function MplusAutomation package.category_labels: character vector category labels response options (e.g., survey answers).Note: Double check labels correct order!Alternative plot","code":"\nsource(here(\"functions\",\"poLCA_plot.R\"))\n\npoLCA_stacked(output_election$c5_election.out, category_labels = c(\"1\" = \"1: Extremely well\", \n                                                                   \"2\" = \"2: Quite Well\", \n                                                                   \"3\" = \"3: Not Too Well\", \n                                                                   \"4\" = \"4: Not Well at All\"))\npoLCA_grouped(output_election$c5_election.out, category_labels = c(\"1\" = \"1: Extremely well\", \n                                                                   \"2\" = \"2: Quite Well\", \n                                                                   \"3\" = \"3: Not Too Well\", \n                                                                   \"4\" = \"4: Not Well at All\"))"},{"path":"polytomous-lca.html","id":"apa-formatted-plot","chapter":"6 Polytomous LCA","heading":"6.4.4 APA-formatted Plot","text":"","code":"\n# Model \nmodel <- output_election$c5_election.out\n\n\n# Title\ntitle <- \"2000 Descriptions of Presidential Candidate George W. Bush; Item Probabilities by Class\"\n\n# Item names\nitem_labels <- c(\"CARESB1\" = \"Caring\",\n                 \"DISHONB1\" = \"Dishonest\",\n                 \"INTELB1\" = \"Intelligent\",\n                 \"KNOWB1\" = \"Knowledgeable\",\n                 \"LEADB1\" = \"Good Leader\",\n                 \"MORALB1\" = \"Moral\")\n\n# Item Category\ncategory_labels <- c(\"1\" = \"1: Extremely well\", \n                     \"2\" = \"2: Quite Well\", \n                     \"3\" = \"3: Not Too Well\", \n                     \"4\" = \"4: Not Well at All\")\n\n# Class labels\nclass_labels <- c(\"1\" = \"Poor Decsription (9.95%)\",\n                  \"2\" = \"Mostly Poor Description (22.40%)\",\n                  \"3\" = \"In-Between (24.06%)\",\n                  \"4\" = \"Mostly Well-Described But Not Intelligent (28.29%)\",\n                  \"5\" = \"Well-Described But Not Intelligent (15.30%)\")\n\n\n#### END EDIT ####"},{"path":"polytomous-lca.html","id":"extract-data-needed-for-plotting","chapter":"6 Polytomous LCA","heading":"6.4.4.1 Extract data needed for plotting","text":"","code":"\n# Extract data needed for plotting \nplot_data <- data.frame(model$parameters$probability.scale) %>%\n  dplyr::select(est, LatentClass, param, category) %>%\n  mutate(\n    items = factor(param, labels = item_labels),\n    class = factor(LatentClass, labels = class_labels),\n    cat = factor(category, labels = category_labels)\n  ) %>% \n  mutate(class = factor(class, levels = rev(levels(factor(class))))) "},{"path":"polytomous-lca.html","id":"final-grouped-bar-plot","chapter":"6 Polytomous LCA","heading":"6.4.4.2 Final grouped bar plot","text":"Save figure:","code":"\n\n## Plot data\nplot_data %>%\n  ggplot(aes(\n    x = items,\n    y = est,\n    fill = cat,\n    group = cat\n  )) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_text(aes(label = sub(\"^0\\\\.\", \".\", sprintf(\"%.2f\", est))), \n            position = position_dodge(width = 0.9), \n            vjust = -0.5, size = 3) +\n  facet_wrap(~ class) + \n  ylim(0, 1) +\n  scale_x_discrete(\n    \"\",\n    labels = function(x)\n      str_wrap(x, width = 10)\n  ) +\n  labs(title = \"Figure 1\",\n       subtitle = title,\n       y = \"Probability\") +\n  theme_bw(12) +\n  scale_fill_grey(start = 0.8, end = 0.2) + # Gives different shades\n  theme(\n    text = element_text(family = \"sans\", size = 12),\n    legend.text = element_text(family = \"sans\", size = 12, color = \"black\"),\n    legend.title = element_blank(),\n    legend.position = \"bottom\",\n    legend.justification = \"center\", \n    axis.text.x = element_text(vjust = 1),\n    plot.subtitle = element_text(face = \"italic\", size = 15),\n    plot.title = element_text(size = 15),\n    strip.background = element_rect(fill = \"grey90\", color = \"black\", size = 1),\n    strip.text = element_text(size = 12)\n  ) \nggsave(here(\"figures\", \"APA_plot1.png\"), dpi=\"retina\", bg = \"white\", height=9, width=15, units=\"in\")"},{"path":"polytomous-lca.html","id":"alternative","chapter":"6 Polytomous LCA","heading":"6.4.4.3 Alternative","text":"Save figure:","code":"\n## Plot data\nplot_data %>%\n  ggplot(aes(\n    x = items,\n    y = est,\n    fill = cat,\n    group = cat\n  )) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_text(aes(label = sub(\"^0\\\\.\", \".\", sprintf(\"%.2f\", est))), \n            position = position_dodge(width = 0.9), \n            vjust = -0.5, size = 3) +\n  facet_wrap(~ class) + \n  ylim(0, 1) +\n  scale_x_discrete(\n    \"\",\n    labels = function(x)\n      str_wrap(x, width = 10)\n  ) +\n  labs(title = \"Figure 1\",\n       subtitle = title,\n       y = \"Probability\") +\n  theme_cowplot(12) +\n  scale_fill_grey(start = 0.8, end = 0.2) + # Gives different shades\n  theme(\n    text = element_text(family = \"sans\", size = 12),\n    legend.text = element_text(family = \"sans\", size = 12, color = \"black\"),\n    legend.title = element_blank(),\n    legend.position = \"bottom\",\n    legend.justification = \"center\", \n    axis.text.x = element_text(vjust = 1),\n    plot.subtitle = element_text(face = \"italic\", size = 15),\n    plot.title = element_text(size = 15),\n    strip.background = element_rect(fill = \"grey90\", color = \"black\", size = 1),\n    strip.text = element_text(size = 12)\n  )\nggsave(here(\"figures\", \"APA_plot2.png\"), dpi=\"retina\", bg = \"white\", height=10, width=17, units=\"in\")"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
