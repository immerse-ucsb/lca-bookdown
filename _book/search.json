[{"path":"index.html","id":"mixture-modeling-with-mplusautomation","chapter":"Mixture Modeling with MplusAutomation","heading":"Mixture Modeling with MplusAutomation","text":" Welcome! collection resources teach apply mixture modeling using Mplus1 MplusAutomation2! resources serve comprehensive guide understanding applying LCA using Mplus automation capabilities MplusAutomation. , learn start finish apply mixture modeling using Mplus MplusAutomation package.","code":""},{"path":"index.html","id":"stay-in-touch","chapter":"Mixture Modeling with MplusAutomation","heading":"Stay in touch!","text":"Please visit website learn IMMERSE fellowship.\ncode materials found Bookdown, see .\nPlease visit website learn IMMERSE fellowship.code materials found Bookdown, see .Visit GitHub account access IMMERSE training materials.Visit GitHub account access IMMERSE training materials.Follow us BlueSky X stay--date fellowship!Follow us BlueSky X stay--date fellowship!","code":""},{"path":"index.html","id":"acknowledgements","chapter":"Mixture Modeling with MplusAutomation","heading":"Acknowledgements","text":"reference workshop: Institute Mixture Modeling Equity-Oriented Researchers, Scholars, Educators (2025). IMMERSE Online Resources (IES . 305B220021). Institute Education Sciences. https://mixture-modeling.netlify.app/","code":""},{"path":"index.html","id":"authors-contributors","chapter":"Mixture Modeling with MplusAutomation","heading":"Authors & Contributors","text":"resource developed IMMERSE team:Dina Arch, PhD, Postdoctoral ScholarDina Arch, PhD, Postdoctoral ScholarKaren Nylund-Gibson, PhD, Principal InvestigatorKaren Nylund-Gibson, PhD, Principal InvestigatorMarsha Ing, PhD, Co-Principal InvestigatorMarsha Ing, PhD, Co-Principal InvestigatorAdditional code contributors:Adam Garber, PhDAdam Garber, PhDDelwin Carter, PhDDelwin Carter, PhDWe also thank IMMERSE fellows provided feedback development materials.","code":""},{"path":"r-and-rstudio.html","id":"r-and-rstudio","chapter":"1 R and RStudio","heading":"1 R and RStudio","text":"walkthrough presented IMMERSE team go common tasks carried R.\nmany free resources available get started R RStudio.\nOne favorites R Data Science.R3 free, open-source programming language environment widely used statistical computing, data analysis, data visualization.R3 free, open-source programming language environment widely used statistical computing, data analysis, data visualization.RStudio4 integrated development environment (IDE) R, providing intuitive interface makes coding, visualization, project management accessible.RStudio4 integrated development environment (IDE) R, providing intuitive interface makes coding, visualization, project management accessible.Mplus5 statistical modeling program used analyzing complex data, latent variable models, structural equation modeling, growth modeling.\nbook uses R package called MplusAutomation automate process running models, extracting results, generating data visualizations.Mplus5 statistical modeling program used analyzing complex data, latent variable models, structural equation modeling, growth modeling.\nbook uses R package called MplusAutomation automate process running models, extracting results, generating data visualizations.","code":""},{"path":"r-and-rstudio.html","id":"installation","chapter":"1 R and RStudio","heading":"1.1 Installation","text":"","code":""},{"path":"r-and-rstudio.html","id":"step-0-install-r-rstudio-and-mplus","chapter":"1 R and RStudio","heading":"1.1.1 Step 0: Install R, RStudio, and Mplus","text":"find guide installing R R Studio.\ncan also install Mplus .Note: installation Mplus requires paid license mixture add-.\nIMMERSE fellows given copy Mplus use one year training.","code":""},{"path":"r-and-rstudio.html","id":"set-up","chapter":"1 R and RStudio","heading":"1.2 Set-up","text":"","code":""},{"path":"r-and-rstudio.html","id":"step-1-create-a-new-r-project-in-rstudio","chapter":"1 R and RStudio","heading":"1.2.1 Step 1: Create a new R-project in RStudio","text":"R-projects help us organize folders , filepaths, scripts.\ncreate new R project:File –> New Project…Click “New Directory” –> New Project –> Name project","code":""},{"path":"r-and-rstudio.html","id":"step-2-create-an-r-markdown-document","chapter":"1 R and RStudio","heading":"1.2.2 Step 2: Create an R-markdown document","text":"R-markdown file provides authoring framework data science allows us organize reports using texts code chunks.\ndocument reading made using R-markdown!create R-markdown:File –> New File –> R Markdown…window pops , give R-markdown title “Introduction R RStudio” Click “OK.” see new markdown example text code chunks.\nwant clean document start delete everything line 10 .\nGo ahead save document R Project folder.","code":""},{"path":"r-and-rstudio.html","id":"step-3-load-packages","chapter":"1 R and RStudio","heading":"1.2.3 Step 3: Load packages","text":"first code chunk given markdown packages using.\ninsert code chunk, etiher use keyboard shortcut ctrl + alt + Code –> Insert Chunk click green box letter C .\npackages want markdown read :reminder, function work receive error like : find function \"random_function\"; try load package receive error like : package called `random_package` , need install package using install.packages(\"random_package\") console (bottom-left window R studio).\ninstalled package never need install , however must always load packages beginning R markdown using library(random_package), shown document.style code package using called tidyverse6 .\nfunctions within tidyverse package , ’ve indicated packages used code chunk .","code":"\nlibrary(psych) # describe()\nlibrary(here) #helps with filepaths\nlibrary(gt) # create tables\nlibrary(tidyverse) #collection of R packages designed for data science"},{"path":"r-and-rstudio.html","id":"explore-the-data","chapter":"1 R and RStudio","heading":"1.3 Explore the data","text":"","code":""},{"path":"r-and-rstudio.html","id":"step-4-read-in-data","chapter":"1 R and RStudio","heading":"1.3.1 Step 4: Read in data","text":"demonstrate mixture modeling training program online resource components IES grant utilize Civil Rights Data Collection (CRDC) (CRDC) data repository.\nCRDC federally mandated school-level data collection effort occurs every year.\npublic data currently available selected latent class indicators across 4 years (2011, 2013, 2015, 2017) US states.\nexample, use Arizona state sample.\nutilize six focal indicators constitute latent class model example; three variables report harassment/bullying schools based disability, race, sex, three variables full-time equivalent school staff hires (counselor, psychologist, law enforcement).\ndata source also includes covariates variety subjects distal outcomes reported 2018 math/reading assessments graduation rates.read data R:Ways view data R:click data Global Environment (upper right pane) use…summary() gives basic summary statistics & shows number NA values (great checking data read correctly)names() provides list column names. useful don’t memorized!head() prints top 6 rows dataframe","code":"\ndata <- read_csv(here(\"data\", \"crdc_lca_data.csv\")) \nView(data)\nsummary(data)\n#>     leaid             ncessch            report_dis    \n#>  Length:2027        Length:2027        Min.   :0.0000  \n#>  Class :character   Class :character   1st Qu.:0.0000  \n#>  Mode  :character   Mode  :character   Median :0.0000  \n#>                                        Mean   :0.0425  \n#>                                        3rd Qu.:0.0000  \n#>                                        Max.   :1.0000  \n#>                                        NA's   :27      \n#>   report_race      report_sex   counselors_fte  \n#>  Min.   :0.000   Min.   :0.00   Min.   :0.0000  \n#>  1st Qu.:0.000   1st Qu.:0.00   1st Qu.:0.0000  \n#>  Median :0.000   Median :0.00   Median :0.0000  \n#>  Mean   :0.103   Mean   :0.17   Mean   :0.4595  \n#>  3rd Qu.:0.000   3rd Qu.:0.00   3rd Qu.:1.0000  \n#>  Max.   :1.000   Max.   :1.00   Max.   :1.0000  \n#>  NA's   :27      NA's   :27     NA's   :27      \n#>    psych_fte         law_fte      \n#>  Min.   :0.0000   Min.   :0.0000  \n#>  1st Qu.:0.0000   1st Qu.:0.0000  \n#>  Median :0.0000   Median :0.0000  \n#>  Mean   :0.4742   Mean   :0.1255  \n#>  3rd Qu.:1.0000   3rd Qu.:0.0000  \n#>  Max.   :1.0000   Max.   :1.0000  \n#>  NA's   :30       NA's   :27\nnames(data)\n#> [1] \"leaid\"          \"ncessch\"        \"report_dis\"    \n#> [4] \"report_race\"    \"report_sex\"     \"counselors_fte\"\n#> [7] \"psych_fte\"      \"law_fte\"\nhead(data)\n#> # A tibble: 6 × 8\n#>   leaid   ncessch      report_dis report_race report_sex\n#>   <chr>   <chr>             <dbl>       <dbl>      <dbl>\n#> 1 0400001 040000100120          0           0          0\n#> 2 0400001 040000100616          0           0          1\n#> 3 0400001 040000101204          0           0          1\n#> 4 0400001 040000101871          0           1          1\n#> 5 0400001 040000101872          0           0          0\n#> 6 0400001 040000102344          0           0          0\n#> # ℹ 3 more variables: counselors_fte <dbl>,\n#> #   psych_fte <dbl>, law_fte <dbl>"},{"path":"r-and-rstudio.html","id":"step-5-descriptive-statistics","chapter":"1 R and RStudio","heading":"1.3.2 Step 5: Descriptive Statistics","text":"Let’s look descriptive statistics variable.\nlooking ID variables’ (leaid) (necessch) descriptives unnecessary, use select() remove variable using minus (-) sign:Alternatively, can use psych::describe() function give information:want look subset data?\nexample, want subset data observe specific school district?\n(leaid) can use tidyverse::filter() subset data using certain criteria.Since binary data (0,1), helpful look proportions:","code":"\ndata %>% \n  select(-leaid, -ncessch) %>% \n  summary()\n#>    report_dis      report_race      report_sex  \n#>  Min.   :0.0000   Min.   :0.000   Min.   :0.00  \n#>  1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.00  \n#>  Median :0.0000   Median :0.000   Median :0.00  \n#>  Mean   :0.0425   Mean   :0.103   Mean   :0.17  \n#>  3rd Qu.:0.0000   3rd Qu.:0.000   3rd Qu.:0.00  \n#>  Max.   :1.0000   Max.   :1.000   Max.   :1.00  \n#>  NA's   :27       NA's   :27      NA's   :27    \n#>  counselors_fte     psych_fte         law_fte      \n#>  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n#>  1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n#>  Median :0.0000   Median :0.0000   Median :0.0000  \n#>  Mean   :0.4595   Mean   :0.4742   Mean   :0.1255  \n#>  3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n#>  Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n#>  NA's   :27       NA's   :30       NA's   :27\ndata %>% \n  select(-leaid, -ncessch) %>% \n  describe()\n#>                vars    n mean   sd median trimmed mad min\n#> report_dis        1 2000 0.04 0.20      0    0.00   0   0\n#> report_race       2 2000 0.10 0.30      0    0.00   0   0\n#> report_sex        3 2000 0.17 0.38      0    0.09   0   0\n#> counselors_fte    4 2000 0.46 0.50      0    0.45   0   0\n#> psych_fte         5 1997 0.47 0.50      0    0.47   0   0\n#> law_fte           6 2000 0.13 0.33      0    0.03   0   0\n#>                max range skew kurtosis   se\n#> report_dis       1     1 4.53    18.55 0.00\n#> report_race      1     1 2.61     4.82 0.01\n#> report_sex       1     1 1.76     1.08 0.01\n#> counselors_fte   1     1 0.16    -1.97 0.01\n#> psych_fte        1     1 0.10    -1.99 0.01\n#> law_fte          1     1 2.26     3.11 0.01\ndata %>% \n  filter(leaid == \"0408800\") %>% \n  describe() \n#>                vars  n  mean    sd median trimmed   mad min\n#> leaid*            1 86  1.00  0.00    1.0    1.00  0.00   1\n#> ncessch*          2 86 43.50 24.97   43.5   43.50 31.88   1\n#> report_dis        3 86  0.05  0.21    0.0    0.00  0.00   0\n#> report_race       4 86  0.15  0.36    0.0    0.07  0.00   0\n#> report_sex        5 86  0.19  0.39    0.0    0.11  0.00   0\n#> counselors_fte    6 86  0.95  0.21    1.0    1.00  0.00   0\n#> psych_fte         7 86  0.19  0.39    0.0    0.11  0.00   0\n#> law_fte           8 86  0.14  0.35    0.0    0.06  0.00   0\n#>                max range  skew kurtosis   se\n#> leaid*           1     0   NaN      NaN 0.00\n#> ncessch*        86    85  0.00    -1.24 2.69\n#> report_dis       1     1  4.23    16.10 0.02\n#> report_race      1     1  1.91     1.68 0.04\n#> report_sex       1     1  1.59     0.52 0.04\n#> counselors_fte   1     1 -4.23    16.10 0.02\n#> psych_fte        1     1  1.59     0.52 0.04\n#> law_fte          1     1  2.04     2.21 0.04\n\n\n#You can use any operator to filter: >, <, ==, >=, etc.\ndata %>% \n  drop_na() %>% \n  pivot_longer(report_dis:law_fte, names_to = \"variable\") %>% \n  group_by(variable) %>% \n  summarise(prop = sum(value)/n(),\n            n = n()) %>%\n  arrange(desc(prop))\n#> # A tibble: 6 × 3\n#>   variable         prop     n\n#>   <chr>           <dbl> <int>\n#> 1 psych_fte      0.481   1970\n#> 2 counselors_fte 0.459   1970\n#> 3 report_sex     0.173   1970\n#> 4 law_fte        0.127   1970\n#> 5 report_race    0.105   1970\n#> 6 report_dis     0.0431  1970"},{"path":"mplusautomation.html","id":"mplusautomation","chapter":"2 MplusAutomation","heading":"2 MplusAutomation","text":"MplusAutomation7 designed streamline use Mplus, powerful statistical software modeling complex data developed Muthen Muten (https://www.statmodel.com). MplusAutomation, researchers can automate process estimating latent variable models, running batches models, extracting results, generating data visualizations - within R environment.?MplusAutomation R packageIt “wraps around” Mplus programRequires R & Mplus softwareRequires learning basics 2 programming languagesCar metaphor: R/Rstudio steering wheel dashboard & Mplus engineWHY?MplusAutomation can provide clearly organized work procedures every research decision can documented single placeIncrease reproducibility, organization, efficiency, transparencyHOW?interface MplusAutomation entirely within R-Studio. need open MplusThe code presented repetitive designBelow template mplusObject() & mplusModeler() functions. Use template run statistical models Mplus.","code":"\nm_template  <- mplusObject(\n  \n  TITLE = \n    \"\", \n  \n  VARIABLE = \n    \"\",\n  \n  ANALYSIS = \n    \"\",\n  \n  PLOT = \n    \"\",\n  \n  OUTPUT = \n    \"\",\n \n  usevariables = colnames(), \n  rdata =  )\n\nm_template_fit <- mplusModeler(m_template, \n                  dataout=here(\"\", \".dat\"),\n                  modelout=here(\"\", \".inp\"),\n                  check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"enumeration.html","id":"enumeration","chapter":"3 Enumeration","heading":"3 Enumeration","text":"Example: Bullying SchoolsTo demonstrate mixture modeling training program online resource components IES grant utilize Civil Rights Data Collection (CRDC)8 data repository.\nCRDC federally mandated school-level data collection effort occurs every year.\npublic data currently available selected latent class indicators across 4 years (2011, 2013, 2015, 2017) US states.\nexample, use Arizona state sample.\nutilize six focal indicators constitute latent class model example; three variables report harassment/bullying schools based disability, race, sex, three variables full-time equivalent school staff hires (counselor, psychologist, law enforcement).\ndata source also includes covariates variety subjects distal outcomes reported 2018 math/reading assessments graduation rates.Load packages","code":"\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(glue)\nlibrary(MplusAutomation)\nlibrary(here)\nlibrary(janitor)\nlibrary(gt)\nlibrary(cowplot)\nlibrary(DiagrammeR) "},{"path":"enumeration.html","id":"variable-description","chapter":"3 Enumeration","heading":"3.1 Variable Description","text":"Variables transformed dichotomous indicators using following coding strategyHarassment bullying count variables recoded 1 school reported least one incident harassment (0 indicates reported incidents).\noriginal scale reported CDRC staff variables full time equivalent employees (FTE) represented 1 part time employees represented values 1 0.\nSchools greater one staff designated type represented values greater 1.\nvalues greater zero recorded 1s (e.g., .5, 1,3) indicating school staff present campus least part time.\nSchools staff designated type indicated 0 dichotomous variable.","code":""},{"path":"enumeration.html","id":"prepare-data","chapter":"3 Enumeration","heading":"3.2 Prepare Data","text":"","code":"\ndf_bully <- read_csv(here(\"data\", \"crdc_lca_data.csv\")) %>% \n  clean_names() %>% \n  dplyr::select(report_dis, report_race, report_sex, counselors_fte, psych_fte, law_fte) "},{"path":"enumeration.html","id":"descriptive-statistics","chapter":"3 Enumeration","heading":"3.3 Descriptive Statistics","text":"Save image","code":"\n# Set up data to find proportions of binary indicators\nds <- df_bully %>% \n  pivot_longer(c(report_dis, report_race, report_sex, counselors_fte, psych_fte, law_fte), names_to = \"variable\") \n\n\n# Create table of variables and counts, then find proportions and round to 3 decimal places\nprop_df <- ds %>%\n  count(variable, value) %>%\n  group_by(variable) %>%\n  mutate(prop = n / sum(n)) %>%\n  ungroup() %>%\n  mutate(prop = round(prop, 3))\n\n\n# Make it a gt() table\nprop_table <- prop_df %>% \n  gt(groupname_col = \"variable\", rowname_col = \"value\") %>%\n  tab_stubhead(label = md(\"*Values*\")) %>%\n  tab_header(\n    md(\n      \"Variable Proportions\"\n    )\n  ) %>%\n  cols_label(\n    variable = md(\"*Variable*\"),\n    value = md(\"*Value*\"),\n    n = md(\"*N*\"),\n    prop = md(\"*Proportion*\")\n  ) \n  \nprop_table\ngtsave(prop_table, here(\"figures\", \"prop_table.png\"))"},{"path":"enumeration.html","id":"enumeration-1","chapter":"3 Enumeration","heading":"3.4 Enumeration","text":"code uses mplusObject function MplusAutomation package saves model runs enum folder.IMPORTANT: moving forward, make sure open output document ensure models estimated normally.","code":"\n\nlca_6  <- lapply(1:6, function(k) {\n  lca_enum  <- mplusObject(\n      \n    TITLE = glue(\"{k}-Class\"), \n  \n    VARIABLE = glue(\n    \"categorical = report_dis-law_fte; \n     usevar = report_dis-law_fte;\n     classes = c({k}); \"),\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 200 100; \n    processors = 10;\",\n  \n  OUTPUT = \"sampstat residual tech11 tech14;\",\n  \n  PLOT = \n    \"type = plot3; \n    series = report_dis-law_fte(*);\",\n  \n  usevariables = colnames(df_bully),\n  rdata = df_bully)\n\nlca_enum_fit <- mplusModeler(lca_enum, \n                            dataout=glue(here(\"enum\", \"bully.dat\")),\n                            modelout=glue(here(\"enum\", \"c{k}_bully.inp\")) ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"enumeration.html","id":"examine-and-extract-mplus-files","chapter":"3 Enumeration","heading":"3.5 Examine and extract Mplus files","text":"Code Delwin Carter (2025)Check Models :WarningsErrorsConvergence Loglikelihood Replication Information","code":"\nsource(here(\"functions\", \"extract_mplus_info.R\"))\n\n# Define the directory where all of the .out files are located.\noutput_dir <- here(\"enum\")\n\n# Get all .out files\noutput_files <- list.files(output_dir, pattern = \"\\\\.out$\", full.names = TRUE)\n\n# Process all .out files into one dataframe\nfinal_data <- map_dfr(output_files, extract_mplus_info_extended)\n\n# Extract Sample_Size from final_data\nsample_size <- unique(final_data$Sample_Size)"},{"path":"enumeration.html","id":"examine-mplus-warnings","chapter":"3 Enumeration","heading":"3.5.1 Examine Mplus Warnings","text":"","code":"\nsource(here(\"functions\", \"extract_warnings.R\"))\n\nwarnings_table <- extract_warnings(final_data)\nwarnings_table\n\n# Save the warnings table\n#gtsave(warnings_table, here(\"figures\", \"warnings_table.png\"))"},{"path":"enumeration.html","id":"examine-mplus-errors","chapter":"3 Enumeration","heading":"3.5.2 Examine Mplus Errors","text":"","code":"\nsource(here(\"functions\", \"error_visualization.R\"))\n\n# Process errors\nerror_table_data <- process_error_data(final_data)\nerror_table_data\n\n# Save the errors table\n#gtsave(error_table, here(\"figures\", \"error_table.png\"))"},{"path":"enumeration.html","id":"examine-convergence-and-loglikelihood-replications","chapter":"3 Enumeration","heading":"3.5.3 Examine Convergence and Loglikelihood Replications","text":"N = 2027Random StartsFinal starting value sets convergingLL ReplicationSmallest ClassModelBest LLnparInitialFinalf%f%f%1-Class-5,443.4096200100100100%100100.0%2,027100.0%2-Class-5,194.136132001005757%57100.0%44421.9%3-Class-5,122.478202001009393%8086.0%21610.6%4-Class-5,111.757272001004646%2043.5%21210.5%5-Class-5,105.589342001003737%718.9%432.1%6-Class-5,099.881412001003232%412.5%361.8%","code":"\nsource(here(\"functions\", \"summary_table.R\"))\n\n# Print Table with Superheader & Heatmap\nsummary_table <- create_flextable(final_data, sample_size)\nsummary_table\n\n# Save the flextable as a PNG image\n#invisible(save_as_image(summary_table, path = here(\"figures\", \"housekeeping.png\")))"},{"path":"enumeration.html","id":"check-for-loglikelihood-replication","chapter":"3 Enumeration","heading":"3.5.4 Check for Loglikelihood Replication","text":"Visualize examine loglikelihood replication values ouput file individuallyVisualize examine loglikelihood replication output file together1-Class2-Class3-Class4-Class5-Class6-ClassLLN%LLN%LLN%LLN%LLN%LLN%-5443.409100100-5194.13657100-5122.4788086-5111.7572043.5-5105.589718.9-5,099.881412.5——————-5123.9451010.8-5111.75936.5-5105.66112.7-5,100.27213.1——————-5123.97933.2-5112.25336.5-5105.79138.1-5,100.84226.2—————————-5112.95512.2-5105.799410.8-5,100.87413.1—————————-5115.5321123.9-5106.74825.4-5,100.92826.2—————————-5115.53812.2-5106.98312.7-5,101.01713.1—————————-5115.88412.2-5107.16925.4-5,101.07126.2—————————-5116.98136.5-5107.172410.8-5,101.08913.1—————————-5117.82936.5-5107.4512.7-5,101.11713.1————————————-5107.45812.7-5,101.31613.1————————————-5107.51712.7-5,101.33213.1————————————-5107.72812.7-5,101.38913.1————————————-5107.95812.7-5,101.45213.1————————————-5108.05812.7-5,101.49413.1————————————-5108.09612.7-5,101.51213.1————————————-5108.86410.8-5,101.59213.1————————————-5109.00212.7-5,101.59313.1————————————-5110.47412.7-5,101.91313.1———————————————-5,102.07513.1———————————————-5,102.61313.1———————————————-5,102.61613.1———————————————-5,104.16713.1———————————————-5,104.46213.1———————————————-5,105.30913.1———————————————-5,107.30213.1———————————————-5,107.62413.1","code":"\n# Load the function for separate plots\nsource(here(\"functions\", \"ll_replication_plots.R\"))\n\n# Generate individual log-likelihood replication tables\nll_replication_tables <- generate_ll_replication_plots(final_data)\nll_replication_tables\nll_replication_table_all <- source(here(\"functions\", \"ll_replication_processing.R\"), local = TRUE)$value\nll_replication_table_all"},{"path":"enumeration.html","id":"table-of-fit","chapter":"3 Enumeration","heading":"3.6 Table of Fit","text":"First, extract data:, create table:Save table","code":"\noutput_enum <- readModels(here(\"enum\"), filefilter = \"bully\", quiet = TRUE)\n\n# Extract fit indices\nenum_extract <- LatexSummaryTable(\n  output_enum,\n  keepCols = c(\n    \"Title\",\n    \"Parameters\",\n    \"LL\",\n    \"BIC\",\n    \"aBIC\",\n    \"BLRT_PValue\",\n    \"T11_VLMR_PValue\",\n    \"Observations\"\n  ),\n  sortBy = \"Title\"\n)\n\n# Calculate additional fit indices\nallFit <- enum_extract %>%\n  mutate(CAIC = -2 * LL + Parameters * (log(Observations) + 1)) %>%\n  mutate(AWE = -2 * LL + 2 * Parameters * (log(Observations) + 1.5)) %>%\n  mutate(SIC = -.5 * BIC) %>%\n  mutate(expSIC = exp(SIC - max(SIC))) %>%\n  mutate(BF = exp(SIC - lead(SIC))) %>%\n  mutate(cmPk = expSIC / sum(expSIC)) %>%\n  dplyr::select(Title, Parameters, LL, BIC, aBIC, CAIC, AWE, BLRT_PValue, T11_VLMR_PValue, BF, cmPk) %>%\n  arrange(Parameters)\n\n# Merge columns with LL replications and class size from `final_data`\nmerged_table <- allFit %>%\n  mutate(Title = str_trim(Title)) %>%\n  left_join(\n    final_data %>%\n      select(\n        Class_Model,\n        Perc_Convergence,\n        Replicated_LL_Perc,\n        Smallest_Class,\n        Smallest_Class_Perc\n      ),\n    by = c(\"Title\" = \"Class_Model\")\n  ) %>%\n  mutate(Smallest_Class = coalesce(Smallest_Class, final_data$Smallest_Class[match(Title, final_data$Class_Model)])) %>%\n  relocate(Perc_Convergence, Replicated_LL_Perc, .after = LL) %>%\n  mutate(Smallest_Class_Combined = paste0(Smallest_Class, \"\\u00A0(\", Smallest_Class_Perc, \"%)\")) %>%\n  select(\n    Title,\n    Parameters,\n    LL,\n    Perc_Convergence,\n    Replicated_LL_Perc,\n    BIC,\n    aBIC,\n    CAIC,\n    AWE,\n    T11_VLMR_PValue,\n    BLRT_PValue,\n    Smallest_Class_Combined,\n    BF,\n    cmPk\n  )\nfit_table1 <- merged_table %>%\n  select(Title, Parameters, LL, Perc_Convergence, Replicated_LL_Perc, \n         BIC, aBIC, CAIC, AWE, \n         T11_VLMR_PValue, BLRT_PValue, \n         Smallest_Class_Combined) %>% \n  gt() %>%\n  tab_header(title = md(\"**Model Fit Summary Table**\")) %>%\n  tab_spanner(label = \"Model Fit Indices\", columns = c(BIC, aBIC, CAIC, AWE)) %>%\n  tab_spanner(label = \"LRTs\", columns = c(T11_VLMR_PValue, BLRT_PValue)) %>%\n  tab_spanner(label = md(\"Smallest\\u00A0Class\"), columns = c(Smallest_Class_Combined)) %>%\n  cols_label(\n    Title = \"Classes\",\n    Parameters = md(\"Par\"),\n    LL = md(\"*LL*\"),\n    Perc_Convergence = \"% Converged\",\n    Replicated_LL_Perc = \"% Replicated\",\n    BIC = \"BIC\",\n    aBIC = \"aBIC\",\n    CAIC = \"CAIC\",\n    AWE = \"AWE\",\n    T11_VLMR_PValue = \"VLMR\",\n    BLRT_PValue = \"BLRT\",\n    Smallest_Class_Combined = \"n (%)\"\n  ) %>%\n  tab_footnote(\n    footnote = md(\n      \"*Note.* Par = Parameters; *LL* = model log likelihood;\nBIC = Bayesian information criterion;\naBIC = sample size adjusted BIC; CAIC = consistent Akaike information criterion;\nAWE = approximate weight of evidence criterion;\nBLRT = bootstrapped likelihood ratio test p-value;\nVLMR = Vuong-Lo-Mendell-Rubin adjusted likelihood ratio test p-value;\n*cmPk* = approximate correct model probability.\"\n    ),\nlocations = cells_title()\n  ) %>%\n  tab_options(column_labels.font.weight = \"bold\") %>%\n  fmt_number(\n    columns = c(3, 6:9), \n    decimals = 2\n  ) %>%\n  sub_missing(1:11,\n              missing_text = \"--\") %>%\n  fmt(\n    c(T11_VLMR_PValue, BLRT_PValue),\n    fns = function(x)\n      ifelse(x < 0.001, \"<.001\",\n             scales::number(x, accuracy = .01))\n  ) %>%\n  fmt_percent(\n    columns = c(Perc_Convergence, Replicated_LL_Perc),\n    decimals = 0,\n    scale_values = FALSE\n  ) %>%\n  \n  cols_align(align = \"center\", columns = everything()) %>%  \n  tab_style(\n    style = list(cell_text(weight = \"bold\")),\n    locations = list(\n      cells_body(columns = BIC, row = BIC == min(BIC)),\n      cells_body(columns = aBIC, row = aBIC == min(aBIC)),\n      cells_body(columns = CAIC, row = CAIC == min(CAIC)),\n      cells_body(columns = AWE, row = AWE == min(AWE)),\n      cells_body(columns = T11_VLMR_PValue, \n                 row = ifelse(T11_VLMR_PValue < .05 & lead(T11_VLMR_PValue) > .05, T11_VLMR_PValue < .05, NA)),\n      cells_body(columns = BLRT_PValue, \n                 row = ifelse(BLRT_PValue < .05 & lead(BLRT_PValue) > .05, BLRT_PValue < .05, NA))\n    )\n  )\n\nfit_table1\ngtsave(fit_table1, here(\"figures\", \"fit_table.png\"))"},{"path":"enumeration.html","id":"information-criteria-plot","chapter":"3 Enumeration","heading":"3.7 Information Criteria Plot","text":"Save figure","code":"\nallFit %>%\n  dplyr::select(2:7) %>%\n  rowid_to_column() %>%\n  pivot_longer(`BIC`:`AWE`,\n               names_to = \"Index\",\n               values_to = \"ic_value\") %>%\n  mutate(Index = factor(Index,\n                        levels = c (\"AWE\", \"CAIC\", \"BIC\", \"aBIC\"))) %>%\n  ggplot(aes(\n    x = rowid,\n    y = ic_value,\n    color = Index,\n    shape = Index,\n    group = Index,\n    lty = Index\n  )) +\n  geom_point(size = 2.0) + geom_line(linewidth = .8) +\n  scale_x_continuous(breaks = 1:nrow(allFit)) +\n  scale_colour_grey(end = .5) +\n  theme_cowplot() +\n  labs(x = \"Number of Classes\", y = \"Information Criteria Value\", title = \"Information Criteria\") +\n  theme(\n    text = element_text(family = \"serif\", size = 12),\n    legend.text = element_text(family=\"serif\", size=12),\n    legend.key.width = unit(3, \"line\"),\n    legend.title = element_blank(),\n    legend.position = \"top\"  \n  )\nggsave(here(\"figures\", \"info_criteria.png\"), dpi=300, height=5, width=7, units=\"in\")"},{"path":"enumeration.html","id":"compare-class-solutions","chapter":"3 Enumeration","heading":"3.8 Compare Class Solutions","text":"Compare probability plots \\(K = 1:6\\) class solutionsSave figure:","code":"\nmodel_results <- data.frame()\n\nfor (i in 1:length(output_enum)) {\n  \n  temp <- output_enum[[i]]$parameters$probability.scale %>%                                       \n    mutate(model = paste(i,\"-Class Model\"))                                                  \n  \n  model_results <- rbind(model_results, temp)\n}\n\nrm(temp)\n\ncompare_plot <-\n  model_results %>%\n  filter(category == 2) %>%\n  dplyr::select(est, model, LatentClass, param) %>%\n  mutate(param = as.factor(str_to_lower(param))) \n\ncompare_plot$param <- fct_inorder(compare_plot$param)\n\nggplot(\n  compare_plot,\n  aes(\n    x = param,\n    y = est,\n    color = LatentClass,\n    shape = LatentClass,\n    group = LatentClass,\n    lty = LatentClass\n  )\n) +\n  geom_point() + \n  geom_line() +\n  scale_colour_viridis_d() +\n  facet_wrap( ~ model, ncol = 2) +\n  labs(title = \"Bullying Items\",\n       x = \" \", y = \"Probability\") +\n  theme_minimal() +\n  theme(panel.grid.major.y = element_blank(),\n                          axis.text.x = element_text(angle = -45, hjust = -.1))                            \nggsave(here(\"figures\", \"compare_kclass_plot.png\"), dpi=300, height=5, width=7, units=\"in\")"},{"path":"enumeration.html","id":"class-probability-plot","chapter":"3 Enumeration","heading":"3.9 3-Class Probability Plot","text":"Use plot_lca function provided folder plot item probability plot. function requires one argument:\n- model_name: name Mplus readModels object (e.g., output_enum$c3_bully.)Save figure:","code":"\nsource(here(\"functions\", \"plot_lca.R\"))\n\nplot_lca(model_name = output_enum$c3_bully.out)\nggsave(here(\"figures\", \"C3_bully_LCA_Plot.png\"), dpi=\"retina\", height=5, width=7, units=\"in\")"},{"path":"enumeration.html","id":"observed-response-patterns","chapter":"3 Enumeration","heading":"3.10 Observed Response Patterns","text":"Save response frequencies 3-class model previous lab response _____.dat SAVEDATA.Read observed response pattern data relabel columnsCreate table top 5 unconditional response pattern, top conditional response pattern modal class assignmentFinally, use gt make nicely formatted tableSave table:","code":"\n\npatterns  <- mplusObject(\n  \n  TITLE = \"C3 LCA - Save response patterns\", \n  \n  VARIABLE = \n  \"categorical = report_dis-law_fte; \n   usevar =  report_dis-law_fte;\n   classes = c(3);\",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 0;\n    processors = 10;\n    optseed = 802779;\",\n  \n  SAVEDATA = \n   \"File=savedata.dat;\n    Save=cprob;\n    \n    ! Code to save response frequency data \n    \n    response is resp_patterns.dat;\",\n  \n  OUTPUT = \"residual patterns tech11 tech14\",\n  \n  usevariables = colnames(df_bully),\n  rdata = df_bully)\n\npatterns_fit <- mplusModeler(patterns,\n                dataout=here(\"mplus\", \"bully.dat\"),\n                modelout=here(\"mplus\", \"patterns.inp\") ,\n                check=TRUE, run = TRUE, hashfilename = FALSE)\n# Read in response frequency data that we just created:\npatterns <- read_table(here(\"mplus\", \"resp_patterns.dat\"),\n                        col_names=FALSE, na = \"*\") \n\n# Extract the column names\nnames <- names(readModels(here(\"mplus\", \"patterns.out\"))[['savedata']]) \n\n# Add the names back to the dataset\ncolnames(patterns) <- c(\"Frequency\", names)  \n# Order responses by highest frequency\norder_highest <- patterns %>% \n  arrange(desc(Frequency)) \n\n# Loop `patterns` data to list top 5 conditional response patterns for each class\nloop_cond  <- lapply(1:max(patterns$C), function(k) {       \norder_cond <- patterns %>%                    \n  filter(C == k) %>%                    \n  arrange(desc(Frequency)) %>%                \n  head(5)                                     \n  })                                          \n\n# Convert loop into data frame\ntable_data <- as.data.frame(bind_rows(loop_cond))\n\n# Combine unconditional and conditional responses patterns\nresponse_patterns <-  rbind(order_highest[1:5,], table_data) \nresp_table <- response_patterns %>% \n  gt() %>%\n    tab_header(\n    title = \"Observed Response Patterns\",\n    subtitle = html(\"Response patterns, estimated frequencies, estimated posterior class probabilities and modal assignments\")) %>% \n    tab_source_note(\n    source_note = md(\"Data Source: **Civil Rights Data Collection (CRDC)**\")) %>%\n    cols_label(\n      Frequency = html(\"<i>f<\/i><sub>r<\/sub>\"),\n    REPORT_D = \"Harrassment: Disability\",\n    REPORT_R = \"Harrassment: Race\",\n    REPORT_S = \"Harrassment: Sex\",\n    COUNSELO = \"Staff: Counselor\",\n    PSYCH_FT = \"Staff: Psychologist\",\n    LAW_FTE = \"Staff: Law Enforcement\",\n    CPROB1 = html(\"P<sub><i>k<\/i><\/sub>=1\"),\n    CPROB2 = html(\"P<sub><i>k<\/i><\/sub>=2\"),\n    CPROB3 = html(\"P<sub><i>k<\/i><\/sub>=3\"),\n    C = md(\"*k*\")) %>% \n  tab_row_group(\n    label = \"Unconditional response patterns\",\n    rows = 1:5) %>%\n  tab_row_group(\n    label = md(\"*k* = 1 Conditional response patterns\"),\n    rows = 6:10) %>% #EDIT THESE VALUES BASED ON THE LAST COLUMN\n  tab_row_group(\n    label = md(\"*k* = 2 Conditional response patterns\"),\n    rows = 11:15)  %>% #EDIT THESE VALUES BASED ON THE LAST COLUMN\n  tab_row_group(\n    label = md(\"*k* = 3 Conditional response patterns\"),\n    rows = 16:20)  %>% #EDIT THESE VALUES BASED ON THE LAST COLUMN  \n    row_group_order(\n      groups = c(\"Unconditional response patterns\",\n                 md(\"*k* = 1 Conditional response patterns\"),\n                 md(\"*k* = 2 Conditional response patterns\"),\n                 md(\"*k* = 3 Conditional response patterns\"))) %>% \n    tab_footnote(\n    footnote = html(\n      \"<i>Note.<\/i> <i>f<\/i><sub>r<\/sub> = response pattern frequency; P<sub><i>k<\/i><\/sub> = posterior class probabilities\"\n    )\n  ) %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"Times New Roman\")\n\nresp_table\ngtsave(resp_table, here(\"figures\",\"resp_table.png\"))"},{"path":"enumeration.html","id":"classification-diagnostics","chapter":"3 Enumeration","heading":"3.11 Classification Diagnostics","text":"Use Mplus calculate k-class confidence intervals (Note: Change synax make chosen k-class model):Note: Ensure classes shift step (.g., Class 1 enumeration run now Class 4). Evaluate output compare class counts proportions latent classes. Using OPTSEED function ensures replication best loglikelihood value run.Read 3-class model:Now, use gt make nicely formatted table","code":"\nclassification  <- mplusObject(\n  \n  TITLE = \"C3 LCA - Calculated k-Class 95% CI\",\n  \n  VARIABLE =\n    \"categorical = report_dis-law_fte;\n   usevar =  report_dis-law_fte;\n   classes = c(3);\", \n  \n  ANALYSIS =\n    \"estimator = ml;\n    type = mixture;\n    starts = 0; \n    processors = 10;\n    optseed = 802779;\n    bootstrap = 1000;\",\n  \n  MODEL =\n    \"\n  !CHANGE THIS SECTION TO YOUR CHOSEN k-CLASS MODEL\n    \n  %OVERALL%\n  [C#1](c1);\n  \n  [C#2](C2);\n\n  Model Constraint:\n  New(p1 p2 p3);\n  \n  p1 = exp(c1)/(1+exp(c1)+exp(c2));\n  p2 = exp(c2)/(1+exp(c1)+exp(c2));\n  p3 = 1/(1+exp(c1)+exp(c2));\",\n\n  \n  OUTPUT = \"cinterval(bcbootstrap)\",\n  \n  usevariables = colnames(df_bully),\n  rdata = df_bully)\n\nclassification_fit <- mplusModeler(classification,\n                dataout=here(\"mplus\", \"bully.dat\"),\n                modelout=here(\"mplus\", \"class.inp\") ,\n                check=TRUE, run = TRUE, hashfilename = FALSE)\n# Read in the 3-class model and extract information needed\noutput_enum <- readModels(here(\"mplus\", \"class.out\"))\n\n# Entropy\nentropy <- c(output_enum$summaries$Entropy, rep(NA, output_enum$summaries$NLatentClasses-1))\n\n# 95% k-Class and k-class 95% Confidence Intervals\nk_ci <- output_enum$parameters$ci.unstandardized %>% \n  filter(paramHeader == \"New.Additional.Parameters\") %>% \n  unite(CI, c(low2.5,up2.5), sep=\", \", remove = TRUE) %>% \n  mutate(CI = paste0(\"[\", CI, \"]\")) %>% \n  rename(kclass=est) %>% \n  dplyr::select(kclass, CI)\n\n# AvePPk = Average Latent Class Probabilities for Most Likely Latent Class Membership (Row) by Latent Class (Column)\navePPk <- tibble(avePPk = diag(output_enum$class_counts$avgProbs.mostLikely))\n\n# mcaPk = modal class assignment proportion \nmcaPk <- round(output_enum$class_counts$mostLikely,3) %>% \n  mutate(model = paste0(\"Class \", class)) %>% \n  add_column(avePPk, k_ci) %>% \n  rename(mcaPk = proportion) %>% \n  dplyr::select(model, kclass, CI, mcaPk, avePPk)\n\n# OCCk = odds of correct classification\nOCCk <- mcaPk %>% \n  mutate(OCCk = round((avePPk/(1-avePPk))/(kclass/(1-kclass)),3))\n\n# Put everything together\nclass_table <- data.frame(OCCk, entropy)\nclass_table <- class_table %>% \n  gt() %>%\n    tab_header(\n    title = \"Model Classification Diagnostics for the 3-Class Solution\") %>%\n    cols_label(\n      model = md(\"*k*-Class\"),\n      kclass = md(\"*k*-Class Proportions\"),\n      CI = \"95% CI\",\n      mcaPk = html(\"McaP<sub>k<\/sub>\"),\n      avePPk = md(\"AvePP<sub>k<\/sub>\"),\n      OCCk = md(\"OCC<sub>k<\/sub>\"),\n      entropy = \"Entropy\") %>% \n    sub_missing(7,\n              missing_text = \"\") %>%\n    tab_footnote(\n    footnote = html(\n      \"<i>Note.<\/i> McaP<sub>k<\/sub> = Modal class assignment proportion; AvePP<sub>k<\/sub> = Average posterior class probabilities; OCC<sub>k<\/sub> = Odds of correct classification; \"\n    )\n  ) %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"Times New Roman\")\n\nclass_table"},{"path":"polytomous-lca.html","id":"polytomous-lca","chapter":"4 Polytomous LCA","heading":"4 Polytomous LCA","text":"Polytomous LCA deals variables two categories, survey questions responses like never, sometimes, always. workflow polytomous LCA model similar LCA model binary indicators. However, polytomous LCA captures complex response patterns, can make interpretation bit trickier. following code demonstrates example, along visualization model.","code":""},{"path":"polytomous-lca.html","id":"example-elections","chapter":"4 Polytomous LCA","heading":"4.1 Example: Elections","text":"“Two sets six questions four responses , asking respondents’ opinions well various traits describe presidential candidates Al Gore George W. Bush. Also potential covariates vote choice, age, education, gender, party ID. Source: National Election Studies (2000).” (poLCA, 2016) See documentation hereTwo sets six questions four responses , asking respondents’ opinions well various traits describe presidential candidates Al Gore George W. Bush. election data set, respondents 2000 American National Election Study public opinion poll asked evaluate well series traits—moral, caring, knowledgeable, good leader, dishonest, intelligent—described presidential candidates Al Gore George W. Bush. question four possible choices: (1) extremely well; (2) quite well; (3) well; (4) well .Load packages","code":"\n\nlibrary(poLCA)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(gt)\nlibrary(MplusAutomation)\nlibrary(here)\nlibrary(glue)"},{"path":"polytomous-lca.html","id":"prepare-data-1","chapter":"4 Polytomous LCA","heading":"4.2 Prepare Data","text":"","code":"\ndata(election)\n\n# Detaching packages that mask the dpylr functions \ndetach(package:poLCA, unload = TRUE)\ndetach(package:MASS, unload = TRUE)\n\ndf_election <-  election %>% \n  clean_names() %>% \n  select(moralb:intelb) %>% \n  mutate(across(everything(), \n                ~ as.factor(as.numeric(gsub(\"\\\\D\", \"\", .))), \n                .names = \"{.col}1\")) \n\n# Quick summary\nsummary(df_election)\n#>                moralb                  caresb   \n#>  1 Extremely well :340   1 Extremely well :155  \n#>  2 Quite well     :841   2 Quite well     :625  \n#>  3 Not too well   :330   3 Not too well   :562  \n#>  4 Not well at all: 98   4 Not well at all:342  \n#>  NA's             :176   NA's             :101  \n#>                knowb                   leadb    \n#>  1 Extremely well :274   1 Extremely well :266  \n#>  2 Quite well     :933   2 Quite well     :842  \n#>  3 Not too well   :379   3 Not too well   :407  \n#>  4 Not well at all:133   4 Not well at all:166  \n#>  NA's             : 66   NA's             :104  \n#>               dishonb                  intelb    moralb1   \n#>  1 Extremely well : 70   1 Extremely well :329   1   :340  \n#>  2 Quite well     :288   2 Quite well     :967   2   :841  \n#>  3 Not too well   :653   3 Not too well   :306   3   :330  \n#>  4 Not well at all:574   4 Not well at all:110   4   : 98  \n#>  NA's             :200   NA's             : 73   NA's:176  \n#>  caresb1     knowb1     leadb1    dishonb1   intelb1   \n#>  1   :155   1   :274   1   :266   1   : 70   1   :329  \n#>  2   :625   2   :933   2   :842   2   :288   2   :967  \n#>  3   :562   3   :379   3   :407   3   :653   3   :306  \n#>  4   :342   4   :133   4   :166   4   :574   4   :110  \n#>  NA's:101   NA's: 66   NA's:104   NA's:200   NA's: 73"},{"path":"polytomous-lca.html","id":"descriptive-statistics-1","chapter":"4 Polytomous LCA","heading":"4.3 Descriptive Statistics","text":"","code":"\nds <- df_election %>% \n  pivot_longer(moralb1:intelb1, names_to = \"variable\") %>% \n  count(variable, value) %>%  # Count occurrences of each value for each variable\n  group_by(variable) %>%\n  mutate(prop = n / sum(n)) %>% \n  arrange(desc(variable))\n\n# Create the table\nprop_table <- ds %>% \n  gt() %>% \n  tab_header(title = md(\"**Descriptive Summary**\")) %>%\n  cols_label(\n    variable = \"Variable\",\n    n = md(\"*N*\"),\n    prop = md(\"Proportion\")\n  ) %>%\n  fmt_number(c(\"n\", \"prop\"), decimals = 2) %>%  # Format both n and prop columns\n  cols_align(\n    align = \"center\",\n    columns = c(prop, n)\n  ) \n\n# View the table\nprop_table\n\n# Save as a Word doc\n#gtsave(prop_table, here(\"figures\", \"prop_table.docx\"))"},{"path":"polytomous-lca.html","id":"enumeration-2","chapter":"4 Polytomous LCA","heading":"4.4 Enumeration","text":"code uses mplusObject function MplusAutomation package.","code":"\n\nlca_enumeration  <- lapply(1:6, function(k) {\n  lca_enum  <- mplusObject(\n      \n    TITLE = glue(\"{k}-Class\"), \n  \n    VARIABLE = glue(\n    \"categorical = moralb1-intelb1; \n     usevar = moralb1-intelb1;\n     classes = c({k}); \"),\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 500 100; \n    processors = 10;\",\n  \n  OUTPUT = \"sampstat residual tech11 tech14 svalues;\",\n\n  \n  usevariables = colnames(df_election),\n  rdata = df_election)\n\nlca_enum_fit <- mplusModeler(lca_enum, \n                            dataout=glue(here(\"poLCA\", \"election.dat\")),\n                            modelout=glue(here(\"poLCA\", \"c{k}_election.inp\")) ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"polytomous-lca.html","id":"table-of-fit-1","chapter":"4 Polytomous LCA","heading":"4.4.1 Table of Fit","text":"Save table:","code":"\nsource(here(\"functions\",\"enum_table.R\"))\n\noutput_election <- readModels(here(\"poLCA\"), filefilter = \"election\", quiet = TRUE)\n\n# To see rows:\n#seeRows(output_election)\n\n# Arguments for `enum_table`\n# 1. readModels objects\n# 2-5. Rows of successfully estimated models \nfit_table <- enum_table(output_election, 1:6)\nfit_table\ngtsave(fit_table, here(\"figures\", \"fit_table.png\"))"},{"path":"polytomous-lca.html","id":"information-criteria-plot-1","chapter":"4 Polytomous LCA","heading":"4.4.2 Information Criteria Plot","text":"Save figure:","code":"\nic_plot(output_election)\nggsave(here(\"figures\", \"info_criteria.png\"), dpi=\"retina\", bg = \"white\", height=5, width=7, units=\"in\")"},{"path":"polytomous-lca.html","id":"class-probability-plot-1","chapter":"4 Polytomous LCA","heading":"4.4.3 4-Class Probability Plot","text":"functions poLCA_stacked poLCA_grouped create visualizations class probabilities LCA polytomous indicators. function takes following arguments:model_name: LCA model read R using readModels function MplusAutomation package.category_labels: character vector category labels response options (e.g., survey answers).Note: Double check labels correct order!Alternative plot","code":"\nsource(here(\"functions\",\"poLCA_plot.R\"))\n\npoLCA_stacked(output_election$c5_election.out, category_labels = c(\"1\" = \"1: Extremely well\", \n                                                                   \"2\" = \"2: Quite Well\", \n                                                                   \"3\" = \"3: Not Too Well\", \n                                                                   \"4\" = \"4: Not Well at All\"))\npoLCA_grouped(output_election$c5_election.out, category_labels = c(\"1\" = \"1: Extremely well\", \n                                                                   \"2\" = \"2: Quite Well\", \n                                                                   \"3\" = \"3: Not Too Well\", \n                                                                   \"4\" = \"4: Not Well at All\"))"},{"path":"polytomous-lca.html","id":"apa-formatted-plot","chapter":"4 Polytomous LCA","heading":"4.4.4 APA-formatted Plot","text":"","code":"\n# Model \nmodel <- output_election$c5_election.out\n\n\n# Title\ntitle <- \"2000 Descriptions of Presidential Candidate George W. Bush; Item Probabilities by Class\"\n\n# Item names\nitem_labels <- c(\"CARESB1\" = \"Caring\",\n                 \"DISHONB1\" = \"Dishonest\",\n                 \"INTELB1\" = \"Intelligent\",\n                 \"KNOWB1\" = \"Knowledgeable\",\n                 \"LEADB1\" = \"Good Leader\",\n                 \"MORALB1\" = \"Moral\")\n\n# Item Category\ncategory_labels <- c(\"1\" = \"1: Extremely well\", \n                     \"2\" = \"2: Quite Well\", \n                     \"3\" = \"3: Not Too Well\", \n                     \"4\" = \"4: Not Well at All\")\n\n# Class labels\nclass_labels <- c(\"1\" = \"Poor Decsription (9.95%)\",\n                  \"2\" = \"Mostly Poor Description (22.40%)\",\n                  \"3\" = \"In-Between (24.06%)\",\n                  \"4\" = \"Mostly Well-Described But Not Intelligent (28.29%)\",\n                  \"5\" = \"Well-Described But Not Intelligent (15.30%)\")\n\n\n#### END EDIT ####"},{"path":"polytomous-lca.html","id":"extract-data-needed-for-plotting","chapter":"4 Polytomous LCA","heading":"4.4.4.1 Extract data needed for plotting","text":"","code":"\n# Extract data needed for plotting \nplot_data <- data.frame(model$parameters$probability.scale) %>%\n  dplyr::select(est, LatentClass, param, category) %>%\n  mutate(\n    items = factor(param, labels = item_labels),\n    class = factor(LatentClass, labels = class_labels),\n    cat = factor(category, labels = category_labels)\n  ) %>% \n  mutate(class = factor(class, levels = rev(levels(factor(class))))) "},{"path":"polytomous-lca.html","id":"final-grouped-bar-plot","chapter":"4 Polytomous LCA","heading":"4.4.4.2 Final grouped bar plot","text":"Save figure:","code":"\n\n## Plot data\nplot_data %>%\n  ggplot(aes(\n    x = items,\n    y = est,\n    fill = cat,\n    group = cat\n  )) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_text(aes(label = sub(\"^0\\\\.\", \".\", sprintf(\"%.2f\", est))), \n            position = position_dodge(width = 0.9), \n            vjust = -0.5, size = 3) +\n  facet_wrap(~ class) + \n  ylim(0, 1) +\n  scale_x_discrete(\n    \"\",\n    labels = function(x)\n      str_wrap(x, width = 10)\n  ) +\n  labs(title = \"Figure 1\",\n       subtitle = title,\n       y = \"Probability\") +\n  theme_bw(12) +\n  scale_fill_grey(start = 0.8, end = 0.2) + # Gives different shades\n  theme(\n    text = element_text(family = \"sans\", size = 12),\n    legend.text = element_text(family = \"sans\", size = 12, color = \"black\"),\n    legend.title = element_blank(),\n    legend.position = \"bottom\",\n    legend.justification = \"center\", \n    axis.text.x = element_text(vjust = 1),\n    plot.subtitle = element_text(face = \"italic\", size = 15),\n    plot.title = element_text(size = 15),\n    strip.background = element_rect(fill = \"grey90\", color = \"black\", size = 1),\n    strip.text = element_text(size = 12)\n  ) \nggsave(here(\"figures\", \"APA_plot1.png\"), dpi=\"retina\", bg = \"white\", height=9, width=15, units=\"in\")"},{"path":"polytomous-lca.html","id":"alternative","chapter":"4 Polytomous LCA","heading":"4.4.4.3 Alternative","text":"Save figure:","code":"\n## Plot data\nplot_data %>%\n  ggplot(aes(\n    x = items,\n    y = est,\n    fill = cat,\n    group = cat\n  )) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_text(aes(label = sub(\"^0\\\\.\", \".\", sprintf(\"%.2f\", est))), \n            position = position_dodge(width = 0.9), \n            vjust = -0.5, size = 3) +\n  facet_wrap(~ class) + \n  ylim(0, 1) +\n  scale_x_discrete(\n    \"\",\n    labels = function(x)\n      str_wrap(x, width = 10)\n  ) +\n  labs(title = \"Figure 1\",\n       subtitle = title,\n       y = \"Probability\") +\n  theme_cowplot(12) +\n  scale_fill_grey(start = 0.8, end = 0.2) + # Gives different shades\n  theme(\n    text = element_text(family = \"sans\", size = 12),\n    legend.text = element_text(family = \"sans\", size = 12, color = \"black\"),\n    legend.title = element_blank(),\n    legend.position = \"bottom\",\n    legend.justification = \"center\", \n    axis.text.x = element_text(vjust = 1),\n    plot.subtitle = element_text(face = \"italic\", size = 15),\n    plot.title = element_text(size = 15),\n    strip.background = element_rect(fill = \"grey90\", color = \"black\", size = 1),\n    strip.text = element_text(size = 12)\n  )\nggsave(here(\"figures\", \"APA_plot2.png\"), dpi=\"retina\", bg = \"white\", height=10, width=17, units=\"in\")"},{"path":"including-auxiliary-variables.html","id":"including-auxiliary-variables","chapter":"5 Including Auxiliary Variables","heading":"5 Including Auxiliary Variables","text":"Example: PISA Student DataData source:first example utilizes dataset undergraduate Cheating available poLCA package (Dayton, 1998): See documentation hereThe first example utilizes dataset undergraduate Cheating available poLCA package (Dayton, 1998): See documentation hereThe second examples utilizes public-use dataset, Longitudinal Survey American Youth (LSAY): See documentation hereThe second examples utilizes public-use dataset, Longitudinal Survey American Youth (LSAY): See documentation ","code":""},{"path":"including-auxiliary-variables.html","id":"load-packages","chapter":"5 Including Auxiliary Variables","heading":"5.1 Load packages","text":"","code":"\nlibrary(MplusAutomation)\nlibrary(tidyverse) #collection of R packages designed for data science\nlibrary(here) #helps with filepaths\nlibrary(janitor) #clean_names\nlibrary(gt) # create tables\nlibrary(cowplot) # a ggplot theme\nlibrary(DiagrammeR) # create path diagrams\nlibrary(glue) # allows us to paste expressions into R code\nlibrary(data.table) # used for `melt()` function  \nlibrary(poLCA)\nlibrary(reshape2)"},{"path":"including-auxiliary-variables.html","id":"automated-three-step","chapter":"5 Including Auxiliary Variables","heading":"5.2 Automated Three-Step","text":"Note: Prior adding covariates distals enumeration must conducted.\nSee Lab 6 examples enumeration MplusAutomation.Application: Undergraduate Cheating behavior“Dichotomous self-report responses 319 undergraduates four questions cheating behavior” (poLCA, 2016).Prepare data","code":"\n\ndata(cheating)\n\ncheating <- cheating %>% clean_names() \n\ndf_cheat <-  cheating %>%                                  \n  dplyr::select(1:4) %>%                                  \n  mutate_all(funs(.-1)) %>%                                \n  mutate(gpa = cheating$gpa)\n\n# Detaching packages that mask the dpylr functions \ndetach(package:poLCA, unload = TRUE)\ndetach(package:MASS, unload = TRUE)"},{"path":"including-auxiliary-variables.html","id":"du3step-in-mplus","chapter":"5 Including Auxiliary Variables","heading":"5.2.1 DU3STEP in Mplus","text":"DU3STEP incorporates distal outcome variables (assumed unequal means variances) mixture models.","code":""},{"path":"including-auxiliary-variables.html","id":"run-the-du3step-model-with-gpa-as-distal-outcome","chapter":"5 Including Auxiliary Variables","heading":"5.2.1.1 Run the DU3step model with gpa as distal outcome","text":"","code":"\n\nm_stepdu  <- mplusObject(\n  TITLE = \"DU3STEP - GPA as Distal\", \n  VARIABLE = \n   \"categorical = lieexam-copyexam; \n    usevar = lieexam-copyexam;\n    auxiliary = gpa (du3step);\n    classes = c(2);\",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 500 100; \n    processors = 10;\",\n  \n  OUTPUT = \"sampstat patterns tech11 tech14;\",\n  \n  PLOT = \n    \"type = plot3; \n     series = lieexam-copyexam(*);\",\n  \n  usevariables = colnames(df_cheat),\n  rdata = df_cheat)\n\nm_stepdu_fit <- mplusModeler(m_stepdu, \n                            dataout=here(\"three_step\", \"auto_3step\", \"du3step.dat\"),\n                            modelout=here(\"three_step\", \"auto_3step\", \"c2_du3step.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"including-auxiliary-variables.html","id":"plot-distal-outcome-mean-differences","chapter":"5 Including Auxiliary Variables","heading":"5.2.1.2 Plot Distal Outcome mean differences","text":"","code":"\nmodelParams <- readModels(here(\"three_step\", \"auto_3step\", \"c2_du3step.out\"))\n\n# Extract class size \nc_size <- as.data.frame(modelParams[[\"class_counts\"]][[\"modelEstimated\"]][[\"proportion\"]]) %>% \n  rename(\"cs\" = 1) %>% \n  mutate(cs = round(cs*100, 2))\n\nc_size_val <- paste0(\"C\", 1:nrow(c_size), glue(\" ({c_size[1:nrow(c_size),]}%)\"))\n\n\n# Extract information as data frame\nestimates <- as.data.frame(modelParams[[\"lcCondMeans\"]][[\"overall\"]]) %>%\n  reshape2::melt(id.vars = \"var\") %>%\n  mutate(variable = as.character(variable),\n         LatentClass = case_when(\n           endsWith(variable, \"1\") ~ c_size_val[1],\n           endsWith(variable, \"2\") ~ c_size_val[2])) %>% #Add to this based on the number of classes you have\n  head(-3) %>% \n  pivot_wider(names_from = variable, values_from = value) %>% \n  unite(\"mean\", contains(\"m\"), na.rm = TRUE) %>% \n  unite(\"se\", contains(\"se\"), na.rm = TRUE) %>% \n  mutate(across(c(mean, se), as.numeric))\n\n# Add labels (NOTE: You must change the labels to match the significance testing!!) \nvalue_labels <- paste0(estimates$mean, c(\" a\",\" b\"))\n\n# Plot bar graphs\nestimates %>%\n  ggplot(aes(fill = LatentClass, y = mean, x = LatentClass)) +\n  geom_bar(position = \"dodge\", stat = \"identity\", color = \"black\") +\n  geom_errorbar(aes(ymin=mean-se, ymax=mean+se),\n                size=.3,    \n                width=.2,\n                position=position_dodge(.9)) +\n  geom_text(aes(y = mean, label = value_labels), \n            family = \"serif\", size = 4,\n            position=position_dodge(.9),\n            vjust = 8) +\n  #scale_fill_grey(start = .5, end = .7) +\n  labs(y=\"GPA\", x=\"\") +\n  theme_cowplot() +\n  theme(text = element_text(family = \"serif\", size = 12),\n        axis.text.x = element_text(size=12),\n        legend.position=\"none\") +\n  coord_cartesian(expand = FALSE, \n                  ylim=c(0,max(estimates$mean*1.5))) # Change ylim based on distal outcome rang\n\n\n# Save plot\nggsave(here(\"figures\",\"Du3STEP_plot.jpeg\"),                \n       dpi=300, width=10, height = 7, units=\"in\")  "},{"path":"including-auxiliary-variables.html","id":"r3step","chapter":"5 Including Auxiliary Variables","heading":"5.2.2 R3STEP","text":"R3STEP incorporates latent class predictors mixture models.","code":""},{"path":"including-auxiliary-variables.html","id":"run-the-r3step-model-with-gpa-as-the-latent-class-predictor","chapter":"5 Including Auxiliary Variables","heading":"5.2.2.1 Run the R3STEP model with gpa as the latent class predictor","text":"","code":"\n\nm_stepr  <- mplusObject(\n  TITLE = \"R3STEP - GPA as Predictor\", \n  VARIABLE = \n   \"categorical = lieexam-copyexam; \n    usevar = lieexam-copyexam;\n    auxiliary = gpa (R3STEP);\n    classes = c(2);\",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 500 100; \n    processors = 10;\",\n  \n  OUTPUT = \"sampstat patterns tech11 tech14;\",\n  \n  PLOT = \n    \"type = plot3; \n     series = lieexam-copyexam(*);\",\n  \n  usevariables = colnames(df_cheat),\n  rdata = df_cheat)\n\nm_stepr_fit <- mplusModeler(m_stepr, \n                            dataout=here(\"three_step\", \"auto_3step\", \"r3step.dat\"),\n                            modelout=here(\"three_step\", \"auto_3step\", \"c2_r3step.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"including-auxiliary-variables.html","id":"regression-slopes-and-odds-ratios","chapter":"5 Including Auxiliary Variables","heading":"5.2.2.2 Regression slopes and odds ratios","text":"","code":"TESTS OF CATEGORICAL LATENT VARIABLE MULTINOMIAL LOGISTIC REGRESSIONS USING\nTHE 3-STEP PROCEDURE\n\n   WARNING:  LISTWISE DELETION IS APPLIED TO THE AUXILIARY VARIABLES IN THE\n   ANALYSIS.  TO AVOID LISTWISE DELETION, DATA IMPUTATION CAN BE USED\n   FOR THE AUXILIARY VARIABLES FOLLOWED BY ANALYSIS WITH TYPE=IMPUTATION.\n   NUMBER OF DELETED OBSERVATIONS:  4\n   NUMBER OF OBSERVATIONS USED:  315\n\n                                                    Two-Tailed\n                    Estimate       S.E.  Est./S.E.    P-Value\n\n C#1        ON\n    GPA               -0.698      0.255     -2.739      0.006\n\n Intercepts\n    C#1               -0.241      0.460     -0.523      0.601\n\nParameterization using Reference Class 1\n\n C#2        ON\n    GPA                0.698      0.255      2.739      0.006\n\n Intercepts\n    C#2                0.241      0.460      0.523      0.601\n\n\nODDS RATIOS FOR TESTS OF CATEGORICAL LATENT VARIABLE MULTINOMIAL LOGISTIC REGRESSIONS\nUSING THE 3-STEP PROCEDURE\n\n                                                95% C.I.\n                    Estimate       S.E. Lower 2.5% Upper 2.5%\n\n C#1        ON\n    GPA                0.498      0.127      0.302      0.820\n\n\nParameterization using Reference Class 1\n\n C#2        ON\n    GPA                2.009      0.512      1.220      3.310"},{"path":"including-auxiliary-variables.html","id":"manual-ml-three-step","chapter":"5 Including Auxiliary Variables","heading":"5.3 Manual ML Three-step","text":"","code":""},{"path":"including-auxiliary-variables.html","id":"unlike-the-automatic-three-step-the-manual-ml-three-step-can-relate-the-latent-class-variable-to-both-distal-outcomes-and-covarites.","chapter":"5 Including Auxiliary Variables","heading":"5.3.1 Unlike the automatic three-step, the manual ML three-step can relate the latent class variable to both distal outcomes and covarites.","text":"Integrate covariates distals mixture modelApplication: Longitudinal Study American Youth, Science AttitudesThe data can found data folder called lsay_subset.csv.","code":"\nlsay_data <- read_csv(here(\"three_step\",\"data\",\"lsay_subset.csv\")) %>% \n  clean_names() %>%   # make variable names lowercase\n  mutate(female = recode(gender, `1` = 0, `2` = 1)) # relabel values from 1,2 to 0,1"},{"path":"including-auxiliary-variables.html","id":"ml-3-step-method","chapter":"5 Including Auxiliary Variables","heading":"5.3.2 ML 3-Step Method","text":"","code":""},{"path":"including-auxiliary-variables.html","id":"step-1---class-enumeration-w-auxiliary-specification","chapter":"5 Including Auxiliary Variables","heading":"5.3.2.1 Step 1 - Class Enumeration w/ Auxiliary Specification","text":"step done class enumeration (selected best latent class model). example, four class model best. Now, re-estimate four-class model using optseed efficiency. difference SAVEDATA command, can save posterior probabilities modal class assignment used steps two three.","code":"\n\nstep1  <- mplusObject(\n  TITLE = \"Step 1 - Three-Step using LSAL\", \n  VARIABLE = \n  \"categorical = enjoy useful logical job adult; \n   usevar = enjoy useful logical job adult;\n    \n   classes = c(4); \n    \n   auxiliary =   ! list all potential covariates and distals here\n   female  mothed      ! covariate\n   math_irt;      ! distal math test score in 12th grade \",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 0;\n    optseed = 568405;\",\n  \n  SAVEDATA = \n   \"File=3step_savedata.dat;\n    Save=cprob;\",\n  \n  OUTPUT = \"residual tech11 tech14\",\n  \n  PLOT = \n    \"type = plot3; \n    series = enjoy-adult(*);\",\n  \n  usevariables = colnames(lsay_data),\n  rdata = lsay_data)\n\nstep1_fit <- mplusModeler(step1,\n                            dataout=here(\"three_step\", \"manual_3step\", \"Step1.dat\"),\n                            modelout=here(\"three_step\", \"manual_3step\", \"one.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\nsource(here(\"functions\", \"plot_lca.R\"))\noutput_lsay <- readModels(here(\"three_step\", \"manual_3step\",\"one.out\"))\n\nplot_lca(model_name = output_lsay)"},{"path":"including-auxiliary-variables.html","id":"step-2---determine-measurement-error","chapter":"5 Including Auxiliary Variables","heading":"5.3.2.2 Step 2 - Determine Measurement Error","text":"Extract logits classification probabilities likely latent classExtract saved dataset part mplusObject “step1_fit”Rename column savedata named “C” change “N”","code":"\nlogit_cprobs <- as.data.frame(output_lsay[[\"class_counts\"]]\n                                       [[\"logitProbs.mostLikely\"]])\nsavedata <- as.data.frame(output_lsay[[\"savedata\"]])\ncolnames(savedata)[colnames(savedata)==\"C\"] <- \"N\""},{"path":"including-auxiliary-variables.html","id":"step-3---lca-auxiliary-variable-model-with-2-covariates-and-1-distal-outcome","chapter":"5 Including Auxiliary Variables","heading":"5.3.2.3 Step 3 - LCA Auxiliary Variable Model with 2 covariates and 1 distal outcome","text":"","code":""},{"path":"including-auxiliary-variables.html","id":"wald-test-table","chapter":"5 Including Auxiliary Variables","heading":"5.3.2.3.1 Wald Test Table","text":"testing relation latent class variable distal outcome (mathirt)Save figure","code":"\nmodelParams <- readModels(here(\"three_step\", \"manual_3step\", \"three.out\"))\n\n# Extract information as data frame\nwald <- as.data.frame(modelParams[[\"summaries\"]]) %>%\n  dplyr::select(WaldChiSq_Value:WaldChiSq_PValue) %>% \n  mutate(WaldChiSq_DF = paste0(\"(\", WaldChiSq_DF, \")\")) %>% \n  unite(wald_test, WaldChiSq_Value, WaldChiSq_DF, sep = \" \") %>% \n  rename(pval = WaldChiSq_PValue) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001))))\n\n# Create table\n\nwald_table <- wald %>% \n  gt() %>%\n    tab_header(\n    title = \"Wald Test Distal Means (Math IRT Scores)\") %>%\n    cols_label(\n      wald_test = md(\"Wald Test (*df*)\"), \n      pval = md(\"*p*-value\")) %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\")\n\nwald_table\ngtsave(wald_table, here(\"figures\",\"wald_table.docx\"))"},{"path":"including-auxiliary-variables.html","id":"table-of-pairwise-distal-outcome-differences","chapter":"5 Including Auxiliary Variables","heading":"5.3.2.3.2 Table of Pairwise Distal Outcome Differences","text":"","code":"\nmodelParams <- readModels(here(\"three_step\", \"manual_3step\", \"three.out\"))\n\n# Extract information as data frame\ndiff <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n  filter(grepl(\"DIFF\", param)) %>% \n  dplyr::select(param:pval) %>% \n  mutate(se = paste0(\"(\", format(round(se,2), nsmall =2), \")\")) %>% \n  unite(estimate, est, se, sep = \" \") %>% \n  mutate(param = str_remove(param, \"DIFF\"),\n         param = as.numeric(param)) %>% \n  separate(param, into = paste0(\"Group\", 1:2), sep = 1) %>% \n  mutate(class = paste0(\"Class \", Group1, \" vs \", Group2)) %>% \n  dplyr::select(class, estimate, pval) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001))))\n\n# Create table\n\ndiff %>% \n  gt() %>%\n    tab_header(\n    title = \"Distal Outcome Differences\") %>%\n    cols_label(\n      class = \"Class\",\n      estimate = md(\"Mean (*se*)\"),\n      pval = md(\"*p*-value\")) %>% \n    sub_missing(1:3,\n              missing_text = \"\") %>%\n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\")"},{"path":"including-auxiliary-variables.html","id":"plot-distal-outcome-means","chapter":"5 Including Auxiliary Variables","heading":"5.3.2.3.3 Plot Distal Outcome Means","text":"","code":"\nmodelParams <- readModels(here(\"three_step\", \"manual_3step\", \"three.out\"))\n\n# Extract class size \nc_size <- as.data.frame(modelParams[[\"class_counts\"]][[\"modelEstimated\"]][[\"proportion\"]]) %>% \n  rename(\"cs\" = 1) %>% \n  mutate(cs = round(cs*100, 2))\n\nc_size_val <- paste0(\"C\", 1:nrow(c_size), glue(\" ({c_size[1:nrow(c_size),]}%)\"))\n\n\n# Extract information as data frame\nestimates <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n  filter(paramHeader == \"Intercepts\") %>%\n  dplyr::select(param, est, se) %>% \n  filter(param == \"MATH_IRT\") %>% \n  mutate(across(c(est, se), as.numeric)) %>% \n  mutate(LatentClass = c_size_val)\n\n# Add labels (NOTE: You must change the labels to match the significance testing!!) \n#value_labels <- paste0(estimates$est, c(\"a\",\" bc\",\" abd\",\" cd\"))\n\nestimates$LatentClass <- fct_inorder(estimates$LatentClass)\n\n# Plot bar graphs\nestimates %>%\n  ggplot(aes(x=LatentClass, y = est, fill = LatentClass)) +\n  geom_col(position = \"dodge\", stat = \"identity\", color = \"black\") +\n  geom_errorbar(aes(ymin=est-se, ymax=est+se),\n                size=.3,    # Thinner lines\n                width=.2,\n                position=position_dodge(.9)) +\n  geom_text(aes(label = est), \n            family = \"serif\", size = 4,\n            position=position_dodge(.9),\n            vjust = 8) +  \n # scale_fill_grey(start = .4, end = .7) + # Remove for colorful bars\n  labs(y=\"Math Scores\", x=\"\") +\n  theme_cowplot() +\n  theme(text = element_text(family = \"serif\", size = 15),\n        axis.text.x = element_text(size=15),\n        legend.position=\"none\")\n\n# Save plot\nggsave(here(\"figures\",\"ManualDistal_Plot.jpeg\"),              \n       dpi=300, width=10, height = 7, units=\"in\") "},{"path":"including-auxiliary-variables.html","id":"covariates-relations","chapter":"5 Including Auxiliary Variables","heading":"5.3.2.3.4 Covariates Relations","text":"","code":"\nmodelParams <- readModels(here(\"three_step\", \"manual_3step\", \"three.out\"))\n\n# Extract information as data frame\ncov <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n   filter(str_detect(paramHeader, \"^C#\\\\d+\\\\.ON$\")) %>% \n  mutate(param = str_replace(param, \"FEMALE\", \"Gender\")) %>% # Change this to your own covariates\n  mutate(param = str_replace(param, \"MOTHED\", \"Mother's Education\")) %>%\n  mutate(est = format(round(est, 3), nsmall = 3),\n         se = round(se, 2),\n         pval = round(pval, 3)) %>% \n  mutate(latent_class = str_replace(paramHeader, \"^C#(\\\\d+)\\\\.ON$\", \"Class \\\\1\")) %>% \n  dplyr::select(param, est, se, pval, latent_class) %>% \n  mutate(se = paste0(\"(\", format(round(se,2), nsmall =2), \")\")) %>% \n  unite(logit, est, se, sep = \" \") %>% \n  dplyr::select(param, logit, pval, latent_class) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001)))) \n\nor <- as.data.frame(modelParams[[\"parameters\"]][[\"odds\"]]) %>%\n  filter(str_detect(paramHeader, \"^C#\\\\d+\\\\.ON$\")) %>% \n  mutate(param = str_replace(param, \"FEMALE\", \"Gender\")) %>% # Change this to your own covariates\n  mutate(param = str_replace(param, \"MOTHED\", \"Mother's Education\")) %>%\n  mutate(est = format(round(est, 3), nsmall = 3)) %>% \n  mutate(latent_class = str_replace(paramHeader, \"^C#(\\\\d+)\\\\.ON$\", \"Class \\\\1\")) %>% \n  mutate(CI = paste0(\"[\", format(round(lower_2.5ci, 3), nsmall = 3), \", \", format(round(upper_2.5ci, 3), nsmall = 3), \"]\")) %>% \n  dplyr::select(param, est, CI, latent_class) %>% \n  rename(or = est)\n  \ncombined <- or %>% \n  full_join(cov) %>% \n  dplyr::select(param, latent_class, logit, pval, or, CI)\n\n\n# Create table\n\ncombined %>% \n  gt(groupname_col = \"latent_class\", rowname_col = \"param\") %>%\n  tab_header(\n    title = \"Predictors of Class Membership\") %>%\n  cols_label(\n    logit = md(\"Logit (*se*)\"),\n    or = md(\"Odds Ratio\"),\n    CI = md(\"95% CI\"),\n    pval = md(\"*p*-value\")) %>% \n  sub_missing(1:3,\n              missing_text = \"\") %>%\n  sub_values(values = c(\"999.000\"), replacement = \"-\") %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\") %>%   \n  tab_footnote(\n    footnote = \"Reference Class: 4\",\n    locations = cells_title(groups = \"title\")\n  )"},{"path":"including-auxiliary-variables.html","id":"distal-outcome-regressed-on-the-covariate","chapter":"5 Including Auxiliary Variables","heading":"5.3.2.3.5 Distal outcome regressed on the covariate","text":"relation distal outcome (Math IRT Scores) covariate (Gender)?","code":"\nmodelParams <- readModels(here(\"three_step\", \"manual_3step\", \"three.out\"))\n\n# Extract information as data frame\ndonx <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n  filter(param %in% c(\"FEMALE\", \"MOTHED\")) %>% \n  mutate(param = str_replace(param, \"FEMALE\", \"Gender\")) %>% \n  mutate(param = str_replace(param, \"MOTHED\", \"Mother's Education\")) %>%\n  mutate(LatentClass = sub(\"^\",\"Class \", LatentClass)) %>%  \n  dplyr::select(!paramHeader) %>% \n  mutate(se = paste0(\"(\", format(round(se,2), nsmall =2), \")\")) %>% \n    unite(estimate, est, se, sep = \" \") %>% \n  dplyr::select(param, estimate, pval) %>% \n  distinct(param, .keep_all=TRUE) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001))))\n\n\n# Create table\n\ndonx %>% \n  gt(groupname_col = \"LatentClass\", rowname_col = \"param\") %>%\n  tab_header(\n    title = \"Gender Predicting Math Scores\") %>%\n  cols_label(\n    estimate = md(\"Estimate (*se*)\"),\n    pval = md(\"*p*-value\")) %>% \n  sub_missing(1:3,\n              missing_text = \"\") %>%\n  sub_values(values = c(\"999.000\"), replacement = \"-\") %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\")"},{"path":"including-auxiliary-variables.html","id":"step-3---lca-auxiliary-variable-model-with-1-covariate","chapter":"5 Including Auxiliary Variables","heading":"5.3.2.4 Step 3 - LCA Auxiliary Variable Model with 1 covariate","text":"","code":""},{"path":"including-auxiliary-variables.html","id":"covariates-relations-single-covariate","chapter":"5 Including Auxiliary Variables","heading":"5.3.2.4.1 Covariates Relations (single covariate)","text":"","code":"\nmodelParams <- readModels(here(\"three_step\", \"manual_3step\", \"three.out\"))\n\n# Extract information as data frame\ncov <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n   filter(str_detect(paramHeader, \"^C#\\\\d+\\\\.ON$\")) %>% \n#  mutate(param = str_replace(param, \"FEMALE\", \"Gender\")) %>% # Change this to your own covariates\n  mutate(param = str_replace(param, \"MOTHED\", \"Mother's Education\")) %>%\n  mutate(est = format(round(est, 3), nsmall = 3),\n         se = round(se, 2),\n         pval = round(pval, 3)) %>% \n  mutate(latent_class = str_replace(paramHeader, \"^C#(\\\\d+)\\\\.ON$\", \"Class \\\\1\")) %>% \n  dplyr::select(param, est, se, pval, latent_class) %>% \n  mutate(se = paste0(\"(\", format(round(se,2), nsmall =2), \")\")) %>% \n  unite(logit, est, se, sep = \" \") %>% \n  dplyr::select(param, logit, pval, latent_class) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001)))) \n\nor <- as.data.frame(modelParams[[\"parameters\"]][[\"odds\"]]) %>%\n  filter(str_detect(paramHeader, \"^C#\\\\d+\\\\.ON$\")) %>% \n # mutate(param = str_replace(param, \"FEMALE\", \"Gender\")) %>% # Change this to your own covariates\n  mutate(param = str_replace(param, \"MOTHED\", \"Mother's Education\")) %>%\n  mutate(est = format(round(est, 3), nsmall = 3)) %>% \n  mutate(latent_class = str_replace(paramHeader, \"^C#(\\\\d+)\\\\.ON$\", \"Class \\\\1\")) %>% \n  mutate(CI = paste0(\"[\", format(round(lower_2.5ci, 3), nsmall = 3), \", \", format(round(upper_2.5ci, 3), nsmall = 3), \"]\")) %>% \n  dplyr::select(param, est, CI, latent_class) %>% \n  rename(or = est)\n  \ncombined <- or %>% \n  full_join(cov) %>% \n  dplyr::select(param, latent_class, logit, pval, or, CI)\n\n\n# Create table\n\ncombined %>% \n  gt(groupname_col = \"latent_class\", rowname_col = \"param\") %>%\n  tab_header(\n    title = \"Covariate Results: Mother's Education on Class\") %>%\n  cols_label(\n    logit = md(\"Logit (*se*)\"),\n    or = md(\"Odds Ratio\"),\n    CI = md(\"95% CI\"),\n    pval = md(\"*p*-value\")) %>% \n  sub_missing(1:3,\n              missing_text = \"\") %>%\n  sub_values(values = c(\"999.000\"), replacement = \"-\") %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\") %>%   \n  tab_footnote(\n    footnote = \"Reference Class: 4\",\n    locations = cells_title(groups = \"title\")\n  )"},{"path":"enumeration-3.html","id":"enumeration-3","chapter":"6 Enumeration","heading":"6 Enumeration","text":"Example: PISA Student DataThe first example closely follows vignette used demonstrate tidyLPA package (Rosenberg, 2019).model utilizes PISA data collected U.S. 2015. learn data see .access 2015 US PISA data & documentation R use following code:Variables:broad_interest\ncomposite measure students’ self reported broad interest\ncomposite measure students’ self reported broad interestenjoyment\ncomposite measure students’ self reported enjoyment\ncomposite measure students’ self reported enjoymentinstrumental_mot\ncomposite measure students’ self reported instrumental motivation\ncomposite measure students’ self reported instrumental motivationself_efficacy\ncomposite measure students’ self reported self efficacy\ncomposite measure students’ self reported self efficacy","code":"\n#devtools::install_github(\"jrosen48/pisaUSA15\")\n#library(pisaUSA15)"},{"path":"enumeration-3.html","id":"latent-profile-models","chapter":"6 Enumeration","heading":"6.1 Latent Profile Models","text":"Latent Profile Analysis (LPA) statistical modeling approach estimating distinct profiles variables.\nsocial sciences educational research, profiles represent, example, different youth experience dimensions engaged (.e., cognitively, behaviorally, affectively) time.\nNote LPA works best continuous variables (, cases, ordinal variables), appropriate dichotomous (binary) variables.Many analysts carried LPA using latent variable modeling approach.\napproach, different parameters - means, variances, covariances - freely estimated across profiles, fixed across profiles, constrained zero.\nMPlus software commonly used estimate models (see ) using expectation-maximization (EM) algorithm obtain maximum likelihood estimates parameters.Different models (whether parameters estimated) can specified estimated.\nMPlus widely-used (powerful), costly, closed-source, can difficult use, particularly respect interpreting using output specified models part reproducible workflow.","code":""},{"path":"enumeration-3.html","id":"terminology-for-specifying-variance-covariance-matrix","chapter":"6 Enumeration","heading":"6.2 Terminology for specifying variance-covariance matrix","text":"code used estimate LPA models walkthrough tidyLPA package.\nTidyLPA(source) R package designed estimate latent profile models using tidy framework.\ncan interface Mplus via MplusAutomation package, enabling estimation latent profile models different variance-covariance structures.model 1 Profile-invariant / Diagonal: Equal variances, covariances fixed 0model 2 Profile-varying / Diagonal: Free variances covariances fixed 0model 3 Profile-invariant / Non-Diagonal: Equal variances equal covariances\nNote: alternative Model 3 freely estimating covariances\nNote: alternative Model 3 freely estimating covariancesmodel 4 Free variances, equal covariancesmodel 5 Equal variances, free covariancesmodel 6 Profile Varying / Non-Diagonal: Free variances free covariances","code":""},{"path":"enumeration-3.html","id":"model-1","chapter":"6 Enumeration","heading":"6.2.1 Model 1","text":"Profile-invariant/diagonal:Equal Variances: Variances fixed equality across profiles (.e., variances constrained equal profile).Equal Variances: Variances fixed equality across profiles (.e., variances constrained equal profile).Covariances fixed zero (.e., -diagonal cells matrix zero).Covariances fixed zero (.e., -diagonal cells matrix zero).parsimonious model restricted.\\[\n\\begin{pmatrix}\n\\sigma^2_1 & 0 & 0 \\\\\n0 & \\sigma^2_2 & 0 \\\\\n0 & 0 & \\sigma^2_3 \\\\\n\\end{pmatrix}\n\\]","code":""},{"path":"enumeration-3.html","id":"model-2","chapter":"6 Enumeration","heading":"6.2.2 Model 2","text":"Profile-varying/diagonal:Free variances: Variances parameters freely estimated across profiles (.e., variances vary profile).Free variances: Variances parameters freely estimated across profiles (.e., variances vary profile).Covariances fixed zero (.e., -diagonal cells matrix zero).Covariances fixed zero (.e., -diagonal cells matrix zero).model flexible less parsimonious model 1.\\[\n\\begin{pmatrix}\n\\sigma^2_{1p} & 0 & 0 \\\\\n0 & \\sigma^2_{2p} & 0 \\\\\n0 & 0 & \\sigma^2_{3p} \\\\\n\\end{pmatrix}\n\\]","code":""},{"path":"enumeration-3.html","id":"model-3","chapter":"6 Enumeration","heading":"6.2.3 Model 3","text":"Profile-invariant/ non-diagonal unrestricted:Equal variances: Variances fixed equality across profile.\n(.e., variances constrained profile).Equal variances: Variances fixed equality across profile.\n(.e., variances constrained profile).Equal Covariances: covariances now estimated constrained equal.\nalternative Model 3 freely estimating covariances (Model 5 ).\nEqual Covariances: covariances now estimated constrained equal.alternative Model 3 freely estimating covariances (Model 5 ).\\[\n\\begin{pmatrix}\n\\sigma^2_1 & \\sigma_{12} & \\sigma_{13} \\\\\n\\sigma_{12} & \\sigma^2_2 & \\sigma_{23} \\\\\n\\sigma_{13} & \\sigma_{23} & \\sigma^2_3 \\\\\n\\end{pmatrix}\n\\]","code":""},{"path":"enumeration-3.html","id":"model-4","chapter":"6 Enumeration","heading":"6.2.4 Model 4","text":"Varying means, varying variances, equal covariances:Free variances: Variances parameters freely estimated across profiles (.e., variances vary profile).Free variances: Variances parameters freely estimated across profiles (.e., variances vary profile).Equal Covariances: Covariances constrained equal.Equal Covariances: Covariances constrained equal.model also considered extension Model 3.\\[\n\\begin{pmatrix}\n\\sigma^2_{1p} & \\sigma_{12} & \\sigma_{13} \\\\\n\\sigma_{12} & \\sigma^2_{2p} & \\sigma_{23} \\\\\n\\sigma_{13} & \\sigma_{23} & \\sigma^2_{3p} \\\\\n\\end{pmatrix}\n\\]","code":""},{"path":"enumeration-3.html","id":"model-5","chapter":"6 Enumeration","heading":"6.2.5 Model 5","text":"Varying means, equal variances, varying covariances:Equal variances: Variances fixed equality across profiles.\n(.e., variances constrained profile).Equal variances: Variances fixed equality across profiles.\n(.e., variances constrained profile).Free Covariances: Covariances now freely estimated across profiles.Free Covariances: Covariances now freely estimated across profiles.model also considered extension Model 3.\\[\n\\begin{pmatrix}\n\\sigma^2_{1} & \\sigma_{12p} & \\sigma_{13p} \\\\\n\\sigma_{12p} & \\sigma^2_{2} & \\sigma_{23p} \\\\\n\\sigma_{13p} & \\sigma_{23p} & \\sigma^2_{3} \\\\\n\\end{pmatrix}\n\\]","code":""},{"path":"enumeration-3.html","id":"model-6","chapter":"6 Enumeration","heading":"6.2.6 Model 6","text":"Profile-varying / Non-diagonal:Free variances: Variances parameters freely estimated across profiles (.e., variances vary profile).Free variances: Variances parameters freely estimated across profiles (.e., variances vary profile).Free Covariances: Covariances now freely estimated across profiles.Free Covariances: Covariances now freely estimated across profiles.complex unrestricted model.\nalso least parsimoniousNote: unrestricted model also sometimes known Model 4.\\[\n\\begin{pmatrix}\n\\sigma^2_{1p} & \\sigma_{12p} & \\sigma_{13p} \\\\\n\\sigma_{12p} & \\sigma^2_{2p} & \\sigma_{23p} \\\\\n\\sigma_{13p} & \\sigma_{23p} & \\sigma^2_{3p} \\\\\n\\end{pmatrix}\n\\]","code":""},{"path":"enumeration-3.html","id":"load-packages-1","chapter":"6 Enumeration","heading":"6.3 Load packages","text":"","code":"\nlibrary(naniar)\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(glue)\nlibrary(MplusAutomation)\nlibrary(here)\nlibrary(janitor)\nlibrary(gt)\nlibrary(tidyLPA)\nlibrary(pisaUSA15)\nlibrary(cowplot)\nlibrary(filesstrings)\nlibrary(patchwork)\nlibrary(RcppAlgos)"},{"path":"enumeration-3.html","id":"prepare-data-2","chapter":"6 Enumeration","heading":"6.4 Prepare Data","text":"","code":"\n\npisa <- pisaUSA15[1:500,] %>%\n  dplyr::select(broad_interest, enjoyment, instrumental_mot, self_efficacy)"},{"path":"enumeration-3.html","id":"descriptive-statistics-2","chapter":"6 Enumeration","heading":"6.5 Descriptive Statistics","text":"Quick SummaryMean TableHistograms","code":"\nsummary(pisa)\n#>  broad_interest    enjoyment    instrumental_mot\n#>  Min.   :1.000   Min.   :1.00   Min.   :1.000   \n#>  1st Qu.:2.200   1st Qu.:2.40   1st Qu.:1.750   \n#>  Median :2.800   Median :3.00   Median :2.000   \n#>  Mean   :2.666   Mean   :2.82   Mean   :2.129   \n#>  3rd Qu.:3.200   3rd Qu.:3.00   3rd Qu.:2.500   \n#>  Max.   :5.000   Max.   :4.00   Max.   :4.000   \n#>  NA's   :23      NA's   :14     NA's   :21      \n#>  self_efficacy  \n#>  Min.   :1.000  \n#>  1st Qu.:1.750  \n#>  Median :2.000  \n#>  Mean   :2.125  \n#>  3rd Qu.:2.500  \n#>  Max.   :4.000  \n#>  NA's   :23\nds <- pisa %>% \n  pivot_longer(broad_interest:self_efficacy, names_to = \"variable\") %>% \n  group_by(variable) %>% \n  summarise(mean = mean(value, na.rm = TRUE),\n            sd = sd(value, na.rm = TRUE)) \n\nds %>% \n  gt () %>% \n  tab_header(title = md(\"**Descriptive Summary**\")) %>%\n  cols_label(\n    variable = \"Variable\",\n    mean = md(\"M\"),\n    sd = md(\"SD\")\n  ) %>%\n  fmt_number(c(2:3),\n             decimals = 2) %>% \n  cols_align(\n    align = \"center\",\n    columns = mean\n  ) \ndata_long <- pisa %>%\n  pivot_longer(broad_interest:self_efficacy, names_to = \"variable\")\n\nggplot(data_long, aes(x = value)) +\n  geom_histogram(binwidth = .3, fill = \"#69b3a2\", color = \"black\") +\n  facet_wrap(~ variable, scales = \"free_x\") +\n  labs(title = \"Histograms of Variables\", x = \"Value\", y = \"Frequency\") +\n  theme_cowplot()"},{"path":"enumeration-3.html","id":"enumeration-4","chapter":"6 Enumeration","heading":"6.6 Enumeration","text":"","code":""},{"path":"enumeration-3.html","id":"tidylpa","chapter":"6 Enumeration","heading":"6.6.1 tidyLPA","text":"Enumerate using estimate_profiles():Estimate models profiles \\(K = 1:5\\)Model 4 continuous indicatorsDefault variance-covariance specifications (model 1)Change variances covariances indicate model want specify, example, estimating six models.","code":"\n\n# Run LPA models \nlpa_fit <- pisa %>% \n    estimate_profiles(1:5,\n                      package = \"MplusAutomation\",\n                      ANALYSIS = \"starts = 500 100;\",\n                      OUTPUT = \"sampstat residual tech11 tech14\",\n                      variances = c(\"equal\", \"varying\", \"equal\", \"varying\", \"equal\", \"varying\"),\n                      covariances = c(\"zero\", \"zero\", \"equal\", \"equal\", \"varying\", \"varying\"),\n                      keepfiles = TRUE)\n\n# Compare fit statistics\nget_fit(lpa_fit)\n\n\n# Move files to folder \nfiles <- list.files(here(), pattern = \"^model\")\nmove_files(files, here(\"lpa\", \"tidyLPA\"), overwrite = TRUE)"},{"path":"enumeration-3.html","id":"mplus","chapter":"6 Enumeration","heading":"6.6.2 Mplus","text":"Alternative method estimate_profiles(): Run enumeration using mplusObject methodYou can change model specification LPA using syntax provided lecture.","code":""},{"path":"enumeration-3.html","id":"model-1-1","chapter":"6 Enumeration","heading":"6.6.2.1 Model 1","text":"estimating LPA Mplus, default variance/covariance specification restricted model (Model 1).\ndon’t specify anything .","code":"\n\nlpa_k14  <- lapply(1:5, function(k) {\n  lpa_enum  <- mplusObject(\n      \n    TITLE = glue(\"Profile {k}\"), \n  \n    VARIABLE = glue(\n    \"usevar = broad_interest-self_efficacy;\n     classes = c({k}); \"),\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 500 100;\",\n  \n  OUTPUT = \"sampstat svalues residual tech11 tech14;\",\n  \n  usevariables = colnames(pisa),\n  rdata = pisa)\n\nlpa_enum_fit <- mplusModeler(lpa_enum, \n                dataout=glue(here(\"lpa\", \"enum_lpa\", \"lpa_pisa\")),\n                modelout=glue(here(\"lpa\", \"enum_lpa\", \"c{k}_lpa_m1.inp\")) ,\n                check=TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"enumeration-3.html","id":"model-2-1","chapter":"6 Enumeration","heading":"6.6.2.2 Model 2","text":", addition loop adds variance/covariance specifications class-specific statement.\nprofile-varying/diagonal specification, must specify variances freely estimated:broad_interest-self_efficacy;reference, Mplus syntax different specifications:Fixed covariance zero (DEFAULT):broad_interest enjoyment@0;Free covariance:broad_interest enjoyment;Equal covariances:%c#1%broad_interest enjoyment (1);%c#2%broad_interest enjoyment (1);Equal variance (DEFAULT):%c#1%broad_interest (1);%c#2%broad_interest (1);Free variance:mth_scor-bio_scor;can also open tidyLPA .inp files see specifications.","code":"\nlpa_m2_k14  <- lapply(1:5, function(k){ \n  \n  # This MODEL section changes the model specification\n  MODEL <- paste(sapply(1:k, function(i) {\n    glue(\"\n    %c#{i}%\n    broad_interest-self_efficacy;      ! variances are freely estimated\n    \")\n  }), collapse = \"\\n\")\n  \n  lpa_enum_m2  <- mplusObject(\n    TITLE = glue(\"Profile {k} - Model 2\"),\n    \n    VARIABLE = glue(\n      \"usevar = broad_interest-self_efficacy;\n     classes = c({k});\"),\n    \n    ANALYSIS = \n      \"estimator = mlr; \n    type = mixture;\n    starts = 500 100;\",\n    \n    MODEL = MODEL,\n    \n    \n    OUTPUT = \"sampstat svalues residual tech11 tech14;\",\n    \n    usevariables = colnames(pisa),\n    rdata = pisa)\n  \n  lpa_m2_fit <- mplusModeler(lpa_enum_m2,\n                             dataout = here(\"lpa\", \"enum_lpa\", \"lpa_pisa\"),\n                             modelout = glue(here(\"lpa\", \"enum_lpa\",\"c{k}_lpa_m2.inp\")),\n                             check = TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"enumeration-3.html","id":"table-of-fit-2","chapter":"6 Enumeration","heading":"6.7 Table of Fit","text":"Evaluate model specification separately using fit indices.examining outputs (increasing random starts necessary), models excluded analysis:Model 2:Profile 4Profile 4Profile 5Profile 5Model 3:Profile 5Model 4:Profile 4Profile 4Profile 5Profile 5Model 5:Profile 3Profile 3Profile 4Profile 4Profile 5Profile 5Model 6:Profile 4Profile 4Profile 5Profile 5","code":"\nsource(here(\"functions\",\"enum_table_lpa.R\"))\n\n# Read in model\noutput_enum <- readModels(here(\"lpa\", \"tidyLPA\"), quiet = TRUE)\n\n# Preview with numbered rows\nenum_fit(output_enum)\n#> # A tibble: 29 × 12\n#>      row Title     Parameters     LL   BIC  aBIC  CAIC   AWE\n#>    <int> <chr>          <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl>\n#>  1     1 Model 1 …          8 -2089. 4227. 4201. 4235. 4300.\n#>  2     2 Model 1 …         13 -1997. 4074. 4032. 4087. 4193.\n#>  3     3 Model 1 …         18 -1953. 4017. 3960. 4035. 4183.\n#>  4     4 Model 1 …         23 -1889. 3921. 3848. 3944. 4133.\n#>  5     5 Model 1 …         28 -1871. 3915. 3826. 3943. 4172.\n#>  6     6 Model 2 …          8 -2089. 4227. 4201. 4235. 4300.\n#>  7     7 Model 2 …         17 -1989. 4083. 4029. 4100. 4239.\n#>  8     8 Model 2 …         26 -1878. 3917. 3834. 3943. 4156.\n#>  9     9 Model 2 …         35 -1851. 3919. 3808. 3954. 4241.\n#> 10    10 Model 2 …         44 -1825. 3923. 3783. 3967. 4327.\n#> # ℹ 19 more rows\n#> # ℹ 4 more variables: BLRT_PValue <dbl>,\n#> #   T11_VLMR_PValue <dbl>, BF <dbl>, cmPk <dbl>\n\n\nselect_models <-LatexSummaryTable(output_enum,                                 \n                keepCols=c(\"Title\", \"Parameters\", \"LL\", \"BIC\", \"aBIC\",\n                           \"BLRT_PValue\", \"T11_VLMR_PValue\",\"Observations\")) %>% \n  slice( # Remove the models that we don't want to consider!!!! Because we looked at every single output, we know which models did not converge, thus we exclude them.\n    # Model 2\n    -9, -10, \n    # Model 3\n    -15,\n    # Model 4\n    -19, -20,\n    # Model 5\n    -23, -24, -25,\n    # Model 6\n    -29, -30\n  )\n\n# Check to make sure that the rows of the models we don't want are removed\n#View(select_models)\n\nenum_table(select_models, 1:5, 6:8, 9:12, 13:15, 16:17, 18:20)"},{"path":"enumeration-3.html","id":"information-criteria-plot-2","chapter":"6 Enumeration","heading":"6.8 Information Criteria Plot","text":"Look “elbow” help profile selectionBased fit indices, choosing following candidate models:Model 1: 2 ProfileModel 2: 3 ProfileModel 3: 4 ProfileModel 4: 3 ProfileModel 5: 2 ProfileModel 6: 2 Profile","code":"\nsource(here(\"functions\",\"ic_plot_lpa.R\"))\nic_plot(select_models)"},{"path":"enumeration-3.html","id":"compare-models","chapter":"6 Enumeration","heading":"6.9 Compare models","text":"","code":""},{"path":"enumeration-3.html","id":"correct-model-probability-cmpk-recalculation","chapter":"6 Enumeration","heading":"6.9.1 Correct Model Probability (cmpK) recalculation","text":"Take candidate models recalculate approximate correct model probabilities (Masyn, 2013)","code":"\n# CmpK recalculation:\nenum_fit1 <- enum_fit(output_enum)\n\nstage2_cmpk <- enum_fit1 %>% \n  slice(2, 8, 14, 18, 22, 27) %>% \n  mutate(SIC = -.5 * BIC,\n         expSIC = exp(SIC - max(SIC)),\n         cmPk = expSIC / sum(expSIC),\n         BF = exp(SIC - lead(SIC))) %>% \n  select(Title, Parameters, BIC:AWE, cmPk, BF)\n\n \n# Format Fit Table\nstage2_cmpk %>%\n  gt() %>% \n  tab_options(column_labels.font.weight = \"bold\") %>%\n  fmt_number(\n    7,\n    decimals = 2,\n    drop_trailing_zeros = TRUE,\n    suffixing = TRUE\n  ) %>%\n  fmt_number(c(3:6),\n             decimals = 2) %>% \n    fmt_number(8,decimals = 2,\n             drop_trailing_zeros=TRUE,\n             suffixing = TRUE) %>% \n  fmt(8, fns = function(x) \n    ifelse(x>100, \">100\",\n           scales::number(x, accuracy = .1))) %>% \n  tab_style(\n    style = list(\n      cell_text(weight = \"bold\")\n      ),\n    locations = list(cells_body(\n     columns = BIC,\n     row = BIC == min(BIC[1:nrow(stage2_cmpk)]) \n    ),\n    cells_body(\n     columns = aBIC,\n     row = aBIC == min(aBIC[1:nrow(stage2_cmpk)])\n    ),\n    cells_body(\n     columns = CAIC,\n     row = CAIC == min(CAIC[1:nrow(stage2_cmpk)])\n    ),\n    cells_body(\n     columns = AWE,\n     row = AWE == min(AWE[1:nrow(stage2_cmpk)])\n    ),\n    cells_body(\n     columns = cmPk,\n     row =  cmPk == max(cmPk[1:nrow(stage2_cmpk)])\n     ),\n    cells_body(\n     columns = BF, \n     row =  BF > 10)\n  )\n)"},{"path":"enumeration-3.html","id":"compare-loglikelihood","chapter":"6 Enumeration","heading":"6.9.2 Compare loglikelihood","text":"can also compare models using nested model testing directly MplusAutomation.\nNote can compare across models profiles must stay ., Model 1 (restricted, fewer parameters) nested Model 2.\nchi-square difference test, assuming nested models, shows significant improvement fit Model 2 Model 1, despite added parameters.","code":"\n# MplusAutomation Method using `compareModels` \n\ncompareModels(output_enum[[\"model_2_class_3.out\"]],\n  output_enum[[\"model_4_class_3.out\"]], diffTest = TRUE)\n#> \n#> ==============\n#> \n#> Mplus model comparison\n#> ----------------------\n#> \n#> ------\n#> Model 1:  C:\\Users\\dnajiarch\\Box\\lca-bookdown\\lpa\\tidyLPA/model_2_class_3.out \n#> Model 2:  C:\\Users\\dnajiarch\\Box\\lca-bookdown\\lpa\\tidyLPA/model_4_class_3.out \n#> ------\n#> \n#> Model Summary Comparison\n#> ------------------------\n#> \n#>              m1                     m2                    \n#> Title        model 2 with 3 classes model 4 with 3 classes\n#> Observations 488                    488                   \n#> Estimator    MLR                    MLR                   \n#> Parameters   26                     32                    \n#> LL           -1877.965              -1859.466             \n#> AIC          3807.929               3782.932              \n#> BIC          3916.877               3917.022              \n#> \n#>   MLR Chi-Square Difference Test for Nested Models Based on Loglikelihood\n#>   -----------------------------------------------------------------------\n#> \n#>   Difference Test Scaling Correction:  1.177167 \n#>   Chi-square difference:  31.4297 \n#>   Diff degrees of freedom:  6 \n#>   P-value:  0 \n#> \n#>   Note: The chi-square difference test assumes that these models are nested.\n#>   It is up to you to verify this assumption.\n#> \n#>   MLR Chi-Square Difference test for nested models\n#>   --------------------------------------------\n#> \n#>   Difference Test Scaling Correction:  \n#>   Chi-square difference:  \n#>   Diff degrees of freedom:  \n#>   P-value:  \n#> \n#> Note: The chi-square difference test assumes that these models are nested.\n#>   It is up to you to verify this assumption.\n#> \n#> =========\n#> \n#> Model parameter comparison\n#> --------------------------\n#>   Parameters present in both models\n#> =========\n#> \n#>   Approximately equal in both models (param. est. diff <= 1e-04)\n#>   ----------------------------------------------\n#>  paramHeader     param LatentClass m1_est m2_est . m1_se\n#>        Means ENJOYMENT           1  2.968  2.968 | 0.012\n#>  m2_se . m1_est_se m2_est_se . m1_pval m2_pval\n#>  0.021 |   238.242   143.432 |       0       0\n#> \n#> \n#>   Parameter estimates that differ between models (param. est. diff > 1e-04)\n#>   ----------------------------------------------\n#>    paramHeader      param                  LatentClass\n#>  BROAD_IN.WITH  ENJOYMENT                            1\n#>  BROAD_IN.WITH  ENJOYMENT                            2\n#>  BROAD_IN.WITH  ENJOYMENT                            3\n#>  BROAD_IN.WITH INSTRUMENT                            1\n#>  BROAD_IN.WITH INSTRUMENT                            2\n#>  BROAD_IN.WITH INSTRUMENT                            3\n#>  BROAD_IN.WITH SELF_EFFIC                            1\n#>  BROAD_IN.WITH SELF_EFFIC                            2\n#>  BROAD_IN.WITH SELF_EFFIC                            3\n#>  ENJOYMEN.WITH INSTRUMENT                            1\n#>  ENJOYMEN.WITH INSTRUMENT                            2\n#>  ENJOYMEN.WITH INSTRUMENT                            3\n#>  ENJOYMEN.WITH SELF_EFFIC                            1\n#>  ENJOYMEN.WITH SELF_EFFIC                            2\n#>  ENJOYMEN.WITH SELF_EFFIC                            3\n#>  INSTRUME.WITH SELF_EFFIC                            1\n#>  INSTRUME.WITH SELF_EFFIC                            2\n#>  INSTRUME.WITH SELF_EFFIC                            3\n#>          Means BROAD_INTE                            1\n#>          Means BROAD_INTE                            2\n#>          Means BROAD_INTE                            3\n#>          Means       C1#1 Categorical.Latent.Variables\n#>          Means       C1#2 Categorical.Latent.Variables\n#>          Means  ENJOYMENT                            2\n#>          Means  ENJOYMENT                            3\n#>          Means INSTRUMENT                            1\n#>          Means INSTRUMENT                            2\n#>          Means INSTRUMENT                            3\n#>          Means SELF_EFFIC                            1\n#>          Means SELF_EFFIC                            2\n#>          Means SELF_EFFIC                            3\n#>      Variances BROAD_INTE                            1\n#>      Variances BROAD_INTE                            2\n#>      Variances BROAD_INTE                            3\n#>      Variances  ENJOYMENT                            1\n#>      Variances  ENJOYMENT                            2\n#>      Variances  ENJOYMENT                            3\n#>      Variances INSTRUMENT                            1\n#>      Variances INSTRUMENT                            2\n#>      Variances INSTRUMENT                            3\n#>      Variances SELF_EFFIC                            1\n#>      Variances SELF_EFFIC                            2\n#>      Variances SELF_EFFIC                            3\n#>  m1_est m2_est . m1_se m2_se . m1_est_se m2_est_se .\n#>   0.000  0.010 | 0.000 0.008 |   999.000     1.300 |\n#>   0.000  0.010 | 0.000 0.008 |   999.000     1.300 |\n#>   0.000  0.010 | 0.000 0.008 |   999.000     1.300 |\n#>   0.000 -0.013 | 0.000 0.024 |   999.000    -0.530 |\n#>   0.000 -0.013 | 0.000 0.024 |   999.000    -0.530 |\n#>   0.000 -0.013 | 0.000 0.024 |   999.000    -0.530 |\n#>   0.000 -0.029 | 0.000 0.023 |   999.000    -1.290 |\n#>   0.000 -0.029 | 0.000 0.023 |   999.000    -1.290 |\n#>   0.000 -0.029 | 0.000 0.023 |   999.000    -1.290 |\n#>   0.000 -0.034 | 0.000 0.015 |   999.000    -2.252 |\n#>   0.000 -0.034 | 0.000 0.015 |   999.000    -2.252 |\n#>   0.000 -0.034 | 0.000 0.015 |   999.000    -2.252 |\n#>   0.000 -0.029 | 0.000 0.008 |   999.000    -3.702 |\n#>   0.000 -0.029 | 0.000 0.008 |   999.000    -3.702 |\n#>   0.000 -0.029 | 0.000 0.008 |   999.000    -3.702 |\n#>   0.000  0.056 | 0.000 0.020 |   999.000     2.777 |\n#>   0.000  0.056 | 0.000 0.020 |   999.000     2.777 |\n#>   0.000  0.056 | 0.000 0.020 |   999.000     2.777 |\n#>   2.912  2.926 | 0.049 0.060 |    59.366    49.094 |\n#>   3.205  3.264 | 0.086 0.126 |    37.165    25.960 |\n#>   2.257  2.159 | 0.082 0.114 |    27.478    18.918 |\n#>  -0.285  0.143 | 0.188 0.341 |    -1.517     0.418 |\n#>  -0.891 -1.085 | 0.183 0.271 |    -4.877    -4.013 |\n#>   3.808  3.948 | 0.048 0.016 |    80.133   251.321 |\n#>   2.305  2.271 | 0.072 0.149 |    32.038    15.214 |\n#>   2.042  1.991 | 0.082 0.051 |    24.923    39.148 |\n#>   1.735  1.801 | 0.092 0.110 |    18.769    16.401 |\n#>   2.357  2.400 | 0.068 0.093 |    34.505    25.743 |\n#>   2.072  2.086 | 0.057 0.044 |    36.269    47.523 |\n#>   1.724  1.758 | 0.067 0.076 |    25.718    23.035 |\n#>   2.330  2.294 | 0.049 0.074 |    47.308    30.997 |\n#>   0.262  0.279 | 0.056 0.053 |     4.671     5.292 |\n#>   0.405  0.384 | 0.104 0.222 |     3.887     1.729 |\n#>   0.594  0.578 | 0.083 0.099 |     7.189     5.844 |\n#>   0.010  0.051 | 0.003 0.019 |     3.283     2.630 |\n#>   0.060  0.011 | 0.016 0.004 |     3.701     3.089 |\n#>   0.397  0.448 | 0.044 0.070 |     9.011     6.373 |\n#>   0.358  0.312 | 0.119 0.061 |     3.008     5.143 |\n#>   0.636  0.797 | 0.134 0.178 |     4.738     4.479 |\n#>   0.560  0.654 | 0.069 0.069 |     8.137     9.507 |\n#>   0.298  0.328 | 0.038 0.032 |     7.771    10.250 |\n#>   0.309  0.377 | 0.048 0.067 |     6.484     5.626 |\n#>   0.435  0.449 | 0.045 0.054 |     9.689     8.370 |\n#>  m1_pval m2_pval\n#>  999.000   0.194\n#>  999.000   0.194\n#>  999.000   0.194\n#>  999.000   0.596\n#>  999.000   0.596\n#>  999.000   0.596\n#>  999.000   0.197\n#>  999.000   0.197\n#>  999.000   0.197\n#>  999.000   0.024\n#>  999.000   0.024\n#>  999.000   0.024\n#>  999.000   0.000\n#>  999.000   0.000\n#>  999.000   0.000\n#>  999.000   0.005\n#>  999.000   0.005\n#>  999.000   0.005\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.129   0.676\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.084\n#>    0.000   0.000\n#>    0.001   0.009\n#>    0.000   0.002\n#>    0.000   0.000\n#>    0.003   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#>    0.000   0.000\n#> \n#> \n#>   P-values that differ between models (p-value diff > 1e-04)\n#>   -----------------------------------\n#>    paramHeader      param                  LatentClass\n#>  BROAD_IN.WITH  ENJOYMENT                            1\n#>  BROAD_IN.WITH  ENJOYMENT                            2\n#>  BROAD_IN.WITH  ENJOYMENT                            3\n#>  BROAD_IN.WITH INSTRUMENT                            1\n#>  BROAD_IN.WITH INSTRUMENT                            2\n#>  BROAD_IN.WITH INSTRUMENT                            3\n#>  BROAD_IN.WITH SELF_EFFIC                            1\n#>  BROAD_IN.WITH SELF_EFFIC                            2\n#>  BROAD_IN.WITH SELF_EFFIC                            3\n#>  ENJOYMEN.WITH INSTRUMENT                            1\n#>  ENJOYMEN.WITH INSTRUMENT                            2\n#>  ENJOYMEN.WITH INSTRUMENT                            3\n#>  ENJOYMEN.WITH SELF_EFFIC                            1\n#>  ENJOYMEN.WITH SELF_EFFIC                            2\n#>  ENJOYMEN.WITH SELF_EFFIC                            3\n#>  INSTRUME.WITH SELF_EFFIC                            1\n#>  INSTRUME.WITH SELF_EFFIC                            2\n#>  INSTRUME.WITH SELF_EFFIC                            3\n#>          Means       C1#1 Categorical.Latent.Variables\n#>      Variances BROAD_INTE                            2\n#>      Variances  ENJOYMENT                            1\n#>      Variances  ENJOYMENT                            2\n#>      Variances INSTRUMENT                            1\n#>  m1_est m2_est . m1_se m2_se . m1_est_se m2_est_se .\n#>   0.000  0.010 | 0.000 0.008 |   999.000     1.300 |\n#>   0.000  0.010 | 0.000 0.008 |   999.000     1.300 |\n#>   0.000  0.010 | 0.000 0.008 |   999.000     1.300 |\n#>   0.000 -0.013 | 0.000 0.024 |   999.000    -0.530 |\n#>   0.000 -0.013 | 0.000 0.024 |   999.000    -0.530 |\n#>   0.000 -0.013 | 0.000 0.024 |   999.000    -0.530 |\n#>   0.000 -0.029 | 0.000 0.023 |   999.000    -1.290 |\n#>   0.000 -0.029 | 0.000 0.023 |   999.000    -1.290 |\n#>   0.000 -0.029 | 0.000 0.023 |   999.000    -1.290 |\n#>   0.000 -0.034 | 0.000 0.015 |   999.000    -2.252 |\n#>   0.000 -0.034 | 0.000 0.015 |   999.000    -2.252 |\n#>   0.000 -0.034 | 0.000 0.015 |   999.000    -2.252 |\n#>   0.000 -0.029 | 0.000 0.008 |   999.000    -3.702 |\n#>   0.000 -0.029 | 0.000 0.008 |   999.000    -3.702 |\n#>   0.000 -0.029 | 0.000 0.008 |   999.000    -3.702 |\n#>   0.000  0.056 | 0.000 0.020 |   999.000     2.777 |\n#>   0.000  0.056 | 0.000 0.020 |   999.000     2.777 |\n#>   0.000  0.056 | 0.000 0.020 |   999.000     2.777 |\n#>  -0.285  0.143 | 0.188 0.341 |    -1.517     0.418 |\n#>   0.405  0.384 | 0.104 0.222 |     3.887     1.729 |\n#>   0.010  0.051 | 0.003 0.019 |     3.283     2.630 |\n#>   0.060  0.011 | 0.016 0.004 |     3.701     3.089 |\n#>   0.358  0.312 | 0.119 0.061 |     3.008     5.143 |\n#>  m1_pval m2_pval\n#>  999.000   0.194\n#>  999.000   0.194\n#>  999.000   0.194\n#>  999.000   0.596\n#>  999.000   0.596\n#>  999.000   0.596\n#>  999.000   0.197\n#>  999.000   0.197\n#>  999.000   0.197\n#>  999.000   0.024\n#>  999.000   0.024\n#>  999.000   0.024\n#>  999.000   0.000\n#>  999.000   0.000\n#>  999.000   0.000\n#>  999.000   0.005\n#>  999.000   0.005\n#>  999.000   0.005\n#>    0.129   0.676\n#>    0.000   0.084\n#>    0.001   0.009\n#>    0.000   0.002\n#>    0.003   0.000\n#> \n#> \n#>   Parameters unique to model 1: 0\n#>   -----------------------------\n#> \n#>   None\n#> \n#> \n#>   Parameters unique to model 2: 0\n#>   -----------------------------\n#> \n#>  None\n#> \n#> \n#> =============="},{"path":"enumeration-3.html","id":"plot-comparison","chapter":"6 Enumeration","heading":"6.9.3 Plot comparison","text":"can also plot comparisons look error bars.NOTE: plotMixtures() function used plotting LPA models (.e., means & variances)","code":"\na <- plotMixtures(output_enum$model_2_class_3.out,\n  ci = 0.95, bw = FALSE) \n\nb <- plotMixtures(output_enum$model_4_class_3.out,\n  ci = 0.95, bw = FALSE) \n\na + labs(title = \"Model 2\") +\n    theme(plot.title = element_text(size = 12)) +\nb + labs(title = \"Model 4\") +\n    theme(plot.title = element_text(size = 12))"},{"path":"enumeration-3.html","id":"visualization","chapter":"6 Enumeration","heading":"6.10 Visualization","text":"","code":""},{"path":"enumeration-3.html","id":"latent-profile-plot","chapter":"6 Enumeration","heading":"6.10.1 Latent Profile Plot","text":"Save figure","code":"\nsource(here(\"functions\", \"plot_lpa.R\"))\n\nplot_lpa(model_name = output_enum$model_3_class_4.out)\nggsave(here(\"figures\", \"model3_profile4.png\"), dpi = \"retina\", bg = \"white\", height=5, width=8, units=\"in\")"},{"path":"enumeration-3.html","id":"plots-means-and-variances","chapter":"6 Enumeration","heading":"6.10.2 Plots Means and Variances","text":"","code":"\nplotMixtures(output_enum$model_3_class_4.out, ci = 0.95, bw = FALSE) + \n  labs(title = \"Model 3: Equal Variances, Equal Covariances\")"},{"path":"enumeration-3.html","id":"model-evaluation","chapter":"6 Enumeration","heading":"6.11 Model Evaluation","text":"","code":""},{"path":"enumeration-3.html","id":"classifications-diagnostics-table","chapter":"6 Enumeration","heading":"6.11.1 Classifications Diagnostics Table","text":"Use Mplus calculate k-class confidence intervals (Note: Change syntax make chosen *k*-class model):Create table","code":"\nclassification  <- mplusObject(\n  \n  TITLE = \"LPA - Calculated k-Class 95% CI\",\n  \n  VARIABLE = \n  \"usevar =  broad_interest-self_efficacy;\n   classes = c1(4);\",\n  \n  ANALYSIS = \n   \"estimator = ml; \n    type = mixture;    \n    starts = 0; \n    processors = 10;\n    optseed = 468036; ! This seed is taken from chosen model output\n    bootstrap = 1000;\",\n  \n  MODEL =\n    \" \n    ! This is copied and pasted from the chosen model input\n  %c1#1%\n  broad_interest (vbroad_interest);\n  enjoyment (venjoyment);\n  instrumental_mot (vinstrumental_mot);\n  self_efficacy (vself_efficacy);\n\n  broad_interest WITH enjoyment (broad_interestWenjoyment);\n  broad_interest WITH instrumental_mot (broad_interestWinstrumental_mot);\n  broad_interest WITH self_efficacy (broad_interestWself_efficacy);\n  enjoyment WITH instrumental_mot (enjoymentWinstrumental_mot);\n  enjoyment WITH self_efficacy (enjoymentWself_efficacy);\n  instrumental_mot WITH self_efficacy (instrumental_motWself_efficacy);\n\n  %c1#2%\n  broad_interest (vbroad_interest);\n  enjoyment (venjoyment);\n  instrumental_mot (vinstrumental_mot);\n  self_efficacy (vself_efficacy);\n\n  broad_interest WITH enjoyment (broad_interestWenjoyment);\n  broad_interest WITH instrumental_mot (broad_interestWinstrumental_mot);\n  broad_interest WITH self_efficacy (broad_interestWself_efficacy);\n  enjoyment WITH instrumental_mot (enjoymentWinstrumental_mot);\n  enjoyment WITH self_efficacy (enjoymentWself_efficacy);\n  instrumental_mot WITH self_efficacy (instrumental_motWself_efficacy);\n\n  %c1#3%\n  broad_interest (vbroad_interest);\n  enjoyment (venjoyment);\n  instrumental_mot (vinstrumental_mot);\n  self_efficacy (vself_efficacy);\n\n  broad_interest WITH enjoyment (broad_interestWenjoyment);\n  broad_interest WITH instrumental_mot (broad_interestWinstrumental_mot);\n  broad_interest WITH self_efficacy (broad_interestWself_efficacy);\n  enjoyment WITH instrumental_mot (enjoymentWinstrumental_mot);\n  enjoyment WITH self_efficacy (enjoymentWself_efficacy);\n  instrumental_mot WITH self_efficacy (instrumental_motWself_efficacy);\n\n  %c1#4%\n  broad_interest (vbroad_interest);\n  enjoyment (venjoyment);\n  instrumental_mot (vinstrumental_mot);\n  self_efficacy (vself_efficacy);\n\n  broad_interest WITH enjoyment (broad_interestWenjoyment);\n  broad_interest WITH instrumental_mot (broad_interestWinstrumental_mot);\n  broad_interest WITH self_efficacy (broad_interestWself_efficacy);\n  enjoyment WITH instrumental_mot (enjoymentWinstrumental_mot);\n  enjoyment WITH self_efficacy (enjoymentWself_efficacy);\n  instrumental_mot WITH self_efficacy (instrumental_motWself_efficacy);\n  \n  \n  !CHANGE THIS SECTION TO YOUR CHOSEN k-CLASS MODEL\n    \n  %OVERALL%\n  [C1#1](c1);\n  [C1#2](c2);\n  [C1#3](c3);\n\n  Model Constraint:\n  New(p1 p2 p3 p4);\n  \n  p1 = exp(c1)/(1+exp(c1)+exp(c2)+exp(c3));\n  p2 = exp(c2)/(1+exp(c1)+exp(c2)+exp(c3));\n  p3 = exp(c3)/(1+exp(c1)+exp(c2)+exp(c3));  \n  p4 = 1/(1+exp(c1)+exp(c2)+exp(c3));\",\n\n  \n  OUTPUT = \"cinterval(bcbootstrap)\",\n  \n  usevariables = colnames(pisa),\n  rdata = pisa)\n\nclassification_fit <- mplusModeler(classification,\n                dataout=here(\"lpa\", \"mplus\", \"class.dat\"),\n                modelout=here(\"lpa\", \"mplus\", \"class.inp\") ,\n                check=TRUE, run = TRUE, hashfilename = FALSE)\nsource(here(\"functions\", \"diagnostics_table.R\"))\n\nclass_output <- readModels(here(\"mplus\", \"class.out\"))\n\ndiagnostics_table(class_output)"},{"path":"enumeration-3.html","id":"profile-homogeneity","chapter":"6 Enumeration","heading":"6.11.2 Profile Homogeneity","text":"Profile homogeneity model-estimated within-profile variances indicator m across k-profiles comparing total overall sample variance:\\[ \\frac{\\theta_{mk}}{\\theta_{m}} \\]","code":"\n# Change this to look at your chosen LPA model\n\nchosen_model <- output_enum$model_3_class_4.out\n\n\n# Extract overall and profile-specifc variances\noverall_variances <- data.frame(chosen_model$sampstat$univariate.sample.statistics) %>% \n  select(Variance) %>% \n  rownames_to_column(var = \"item\") %>% \n  clean_names() %>% \n  mutate(item = str_sub(item, 1, 8))\n\nprofile_variances <- data.frame(chosen_model$parameters$unstandardized) %>% \n  filter(paramHeader == \"Variances\") %>% \n  select(param, est, LatentClass) %>% \n  rename(item = param,\n         variance = est,\n         k = LatentClass) %>% \n  mutate(k = paste0(\"Profile \", k)) %>% \n  mutate(item = str_sub(item, 1, 8))\n\n  \n# Evaluate the ratio\nhomogeneity <- profile_variances %>%\n  left_join(overall_variances, by = \"item\") %>%\n  mutate(homogeneity_ratio = round((variance.x / variance.y),2))\n\n# Create a gt table\nhomogeneity %>%\n  select(homogeneity_ratio, k, item) %>% \n  pivot_wider(\n    names_from = k, \n    values_from = homogeneity_ratio\n  ) %>%\n  gt() %>%\n  cols_label(\n    item = \"Item\"\n  ) %>%\n  fmt_number(\n    columns = everything(),\n    decimals = 3\n  ) %>%\n  tab_header(\n    title = \"Profile Homogeneity Table\"\n  ) %>%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  )"},{"path":"enumeration-3.html","id":"profile-separation","chapter":"6 Enumeration","heading":"6.11.3 Profile Separation","text":"can evaluate degree profile separation assessing actual distance profile-specific means.\nquantify profile separation Profile j Profile k respect particular item m, compute standard mean difference:\\[\\hat{d}_{mjk}= \\frac{{\\hat{\\alpha_{mj}}-\\hat{\\alpha_{mk}}}}{\\sigma_{mjk}}\\] Pooled variance:\\[\\hat{\\sigma}_{mj k} = \\sqrt{\\frac{(\\hat{\\pi}_j)(n)(\\hat{\\theta}_{mj}) + (\\hat{\\pi}_k)(n)(\\hat{\\theta}_{mk})}{(\\hat{\\pi}_j + \\hat{\\pi}_k) n}}\\]","code":"\n# Change this to look at your chosen LPA model\nchosen_model <- output_enum$model_3_class_4.out\n\n\n# Profile-specific means\nprofile_means <- data.frame(chosen_model$parameters$unstandardized) %>% \n  filter(paramHeader == \"Means\") %>%\n  filter(!str_detect(param, \"#\")) %>% \n  select(param, est, LatentClass) %>% \n  rename(item = param,\n         means = est,\n         k = LatentClass) %>% \n  mutate(k = paste0(\"Profile \", k)) %>% \n  mutate(item = str_sub(item, 1, 8))\n\n# Relative profile sizes\nprofile_sizes <- data.frame(chosen_model$class_counts$modelEstimated) %>% \n  rename(size = proportion,\n         k = class) %>% \n  mutate(k = paste0(\"Profile \", k)) %>% \n  select(-count)\n\n# Sample size\nn <- chosen_model$summaries$Observations\n\n# Profile-specific variance\nprofile_variances <- data.frame(chosen_model$parameters$unstandardized) %>% \n  filter(paramHeader == \"Variances\") %>% \n  select(param, est, LatentClass) %>% \n  rename(item = param,\n         variance = est,\n         k = LatentClass) %>% \n  mutate(k = paste0(\"Profile \", k)) %>% \n  mutate(item = str_sub(item, 1, 8))\n\n# Combine profile variances with profile sizes\nvariance_with_sizes <- profile_variances %>%\n  left_join(profile_sizes, by = \"k\")\n\n# Create the combinations\nprofile_combinations <- data.frame(comboGrid(k1 = unique(profile_sizes$k), k2 = unique(profile_sizes$k))) %>%\n  filter(k1 != k2) %>%  \n  arrange(k1, k2)\n\n# Calculate pooled variance for each item across the profiles\ncombined_results <- profile_combinations %>%\n  rowwise() %>%\n  do({\n    pair <- .\n    k1 <- pair$k1\n    k2 <- pair$k2\n    \n    # Filter for the two profiles (variance)\n    data_k1_var <- variance_with_sizes %>% filter(k == k1)\n    data_k2_var <- variance_with_sizes %>% filter(k == k2)\n    \n    # Filter for the two profiles (means)\n    data_k1_mean <- profile_means %>% filter(k == k1)\n    data_k2_mean <- profile_means %>% filter(k == k2)\n    \n    # Combine variance data for the two profiles\n    variance_data <- data_k1_var %>%\n      inner_join(data_k2_var, by = \"item\", suffix = c(\"_k1\", \"_k2\")) %>%\n      mutate(\n        pooled_variance = sqrt(\n          ((size_k1 * n * variance_k1) + (size_k2 * n * variance_k2)) / ((size_k1 + size_k2) * n)\n        ),\n        comparison = paste(k1, \"vs\", k2)\n      ) %>%\n      select(item, pooled_variance, comparison)\n    \n    # Combine mean data for the two profilees\n    mean_data <- data_k1_mean %>%\n      inner_join(data_k2_mean, by = \"item\", suffix = c(\"_k1\", \"_k2\")) %>%\n      mutate(\n        mean_diff = means_k1 - means_k2,\n        comparison = paste(k1, \"vs\", k2)\n      ) %>%\n      select(item, mean_diff, comparison)\n    \n    # Combine both variance and mean differences data\n    combined_data <- variance_data %>%\n      left_join(mean_data, by = c(\"item\", \"comparison\")) %>% \n      mutate(\n         mean_diff_by_pooled_variance = mean_diff / pooled_variance\n      )\n    \n    combined_data\n  }) %>%\n  bind_rows()\n\n# Create a gt table\ncombined_results %>%\n  select(mean_diff_by_pooled_variance, comparison ,item) %>% \n  pivot_wider(\n    names_from = comparison, \n    values_from = mean_diff_by_pooled_variance\n  ) %>%\n  gt() %>%\n  cols_label(\n    item = \"Item\"\n  ) %>%\n  fmt_number(\n    columns = everything(),\n    decimals = 3\n  ) %>%\n  tab_header(\n    title = \"Profile Separation Table\"\n  ) %>%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  )"},{"path":"moderation.html","id":"moderation","chapter":"7 Moderation","heading":"7 Moderation","text":"Example: Longitudinal Study American YouthData source: : See documentation ","code":""},{"path":"moderation.html","id":"load-packages-2","chapter":"7 Moderation","heading":"7.1 Load Packages","text":"","code":"\nlibrary(MplusAutomation)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(glue)\nlibrary(gt)\nlibrary(cowplot)\nlibrary(kableExtra)\nlibrary(psych)\nlibrary(float)\nlibrary(janitor)"},{"path":"moderation.html","id":"moderation-path-diagram","chapter":"7 Moderation","heading":"7.2 Moderation Path Diagram","text":"Read LSAY dataset","code":"\ndata <- read_csv(here(\"data\",\"lsay_subset.csv\")) %>% \n  clean_names() %>%   # make variable names lowercase\n  mutate(female = recode(gender, `1` = 0, `2` = 1)) # relabel values from 1,2 to 0,1"},{"path":"moderation.html","id":"descriptive-statistics-3","chapter":"7 Moderation","heading":"7.3 Descriptive Statistics","text":"","code":""},{"path":"moderation.html","id":"descriptive-statistics-using-r","chapter":"7 Moderation","heading":"7.3.1 Descriptive Statistics using R:","text":"Quick view relevant variables:Proportion indicators using R:","code":"\ndata %>% \n  select(enjoy, useful, logical, job, adult, female, math_irt) %>% \n  describe()\n# Set up data to find proportions of binary indicators\nds <- data %>% \n  pivot_longer(c(enjoy, useful, logical, job, adult), names_to = \"Variable\") \n\n# Create table of variables and counts\ntab <- table(ds$Variable, ds$value)\n\n# Find proportions and round to 3 decimal places\nprop <- prop.table(tab, margin = 1) %>% \n  round(3)\n\n# Combine everything to one table \ndframe <- data.frame(Variables=rownames(tab), Proportion=prop[,2], Count=tab[,2])\n#remove row names\nrow.names(dframe) <- NULL\ngt(dframe) %>% \ntab_header(title = md(\"**LCA Indicator Proportions**\"), subtitle = md(\"&nbsp;\")) %>%\ntab_options(column_labels.font.weight = \"bold\", row_group.font.weight = \"bold\") "},{"path":"moderation.html","id":"descriptive-statistics-using-mplusautomation","chapter":"7 Moderation","heading":"7.3.2 Descriptive Statistics using MplusAutomation:","text":"View .file:, view descriptive statistics using get_sampstat():","code":"\nbasic_mplus  <- mplusObject(\n  TITLE = \"LSAL Descriptive Statistics;\",\n  \n  VARIABLE =\n    \"usevar = enjoy, useful, logical, job, adult, female, math_irt;\n    categorical = enjoy, useful, logical, job, adult, female;\",\n\n  ANALYSIS = \"TYPE=basic;\",\n  \n  OUTPUT = \"sampstat;\",  \n  \n  usevariables = colnames(data),\n  rdata = data)\n\nbasic_mplus_fit <- mplusModeler(basic_mplus, \n                            dataout = here(\"moderation\", \"LSAL_data.dat\"),\n                            modelout = here(\"moderation\",\"basic.inp\"),\n                            check = TRUE, run = TRUE, hashfilename = FALSE)\n# Using MplusAutomation\nMplusAutomation::get_sampstat(basic_mplus_fit)\n\n# Using base R\nsummary(data)"},{"path":"moderation.html","id":"enumeration-5","chapter":"7 Moderation","heading":"7.4 Enumeration","text":"code uses mplusObject function MplusAutomation package saves model runs mplus_enum folder.IMPORTANT: moving forward, make sure examine output document ensure models estimated normally. example, last model (6-class models) produce reliable output excluded.","code":"\n\nlca_enum_6  <- lapply(1:6, function(k) {\n  lca_enum  <- mplusObject(\n    \n    TITLE = glue(\"{k}-Class\"), \n    \n    VARIABLE = glue(\n      \"categorical = enjoy, useful, logical, job, adult; \n     usevar = enjoy, useful, logical, job, adult;\n     classes = c({k});\"),\n    \n    ANALYSIS = \n      \"estimator = mlr; \n    type = mixture;\n    processors = 12;\n    starts = 500 100;\",\n    \n    OUTPUT = \"sampstat residual tech11 tech14;\",\n\n    usevariables = colnames(data),\n    rdata = data)\n  \n  lca_enum_fit <- mplusModeler(lca_enum, \n                               dataout=glue(here(\"moderation\",\"enum\", \"LSAY_data.dat\")),\n                               modelout=glue(here(\"moderation\",\"enum\", \"c{k}_lsal.inp\")) ,\n                               check=TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"moderation.html","id":"table-of-fit-3","chapter":"7 Moderation","heading":"7.4.1 Table of Fit","text":"First, extract data:","code":"\nsource(here(\"functions\", \"extract_mplus_info.R\"))\n\n# Define the directory where all of the .out files are located.\noutput_dir <- here(\"moderation\",\"enum\")\n\n# Get all .out files\noutput_files <- list.files(output_dir, pattern = \"\\\\.out$\", full.names = TRUE)\n\n# Process all .out files into one dataframe\nfinal_data <- map_dfr(output_files, extract_mplus_info_extended)\n\n# Extract Sample_Size from final_data\nsample_size <- unique(final_data$Sample_Size)"},{"path":"moderation.html","id":"examine-mplus-warnings-1","chapter":"7 Moderation","heading":"7.4.1.1 Examine Mplus Warnings","text":"","code":"\nsource(here(\"functions\", \"extract_warnings.R\"))\n\nwarnings_table <- extract_warnings(final_data)\nwarnings_table\n\n# Save the warnings table\n#gtsave(warnings_table, here(\"figures\", \"warnings_table.png\"))"},{"path":"moderation.html","id":"examine-mplus-errors-1","chapter":"7 Moderation","heading":"7.4.1.2 Examine Mplus Errors","text":"","code":"\nsource(here(\"functions\", \"error_visualization.R\"))\n\n# Process errors\nerror_table_data <- process_error_data(final_data)\nerror_table_data\n\n# Save the errors table\n#gtsave(error_table, here(\"figures\", \"error_table.png\"))"},{"path":"moderation.html","id":"examine-convergence-and-loglikelihood-replications-1","chapter":"7 Moderation","heading":"7.4.1.3 Examine Convergence and Loglikelihood Replications","text":"N = 2675Random StartsFinal starting value sets convergingLL ReplicationSmallest ClassModelBest LLnparInitialFinalf%f%f%1-Class-8,150.3515500100100100%100100.0%2,675100.0%2-Class-7,191.87811500100100100%100100.0%80330.0%3-Class-7,124.921175001006868%6494.1%37213.9%4-Class-7,095.123235001003535%3085.7%2629.8%5-Class-7,091.946295001003434%617.6%30611.4%6-Class-7,090.886355001004141%922.0%1385.2%","code":"\nsource(here(\"functions\", \"summary_table.R\"))\n\n# Print Table with Superheader & Heatmap\nsummary_table <- create_flextable(final_data, sample_size)\nsummary_table\n\n# Save the flextable as a PNG image\n#invisible(save_as_image(summary_table, path = here(\"figures\", \"housekeeping.png\")))"},{"path":"moderation.html","id":"final-fit-table","chapter":"7 Moderation","heading":"7.4.1.4 Final Fit Table","text":"First, extract data:Save table:","code":"\nsource(here(\"functions\", \"enum_table_lca.R\"))\n\noutput_enum <- readModels(here(\"moderation\", \"enum\"), quiet = TRUE)\n\nfit_table <- fit_table_lca(output_enum, final_data)\nfit_table\ngtsave(fit_table, here(\"figures\", \"fit_table_lca.png\"))"},{"path":"moderation.html","id":"information-criteria-plot-3","chapter":"7 Moderation","heading":"7.4.2 Information Criteria Plot","text":"Save figure:","code":"\nsource(here(\"functions\", \"ic_plot_lca.R\"))\nic_plot(output_enum)\nggsave(here(\"figures\", \"info_criteria_moderation.png\"),  dpi = \"retina\", bg = \"white\", height=5, width=7, units=\"in\")"},{"path":"moderation.html","id":"compare-class-solutions-1","chapter":"7 Moderation","heading":"7.4.3 Compare Class Solutions","text":"Compare probability plots \\(K = 1:5\\) class solutionsSave figure:","code":"\nmodel_results <- data.frame()\n\nfor (i in 1:length(output_enum)) {\n  temp <- output_enum[[i]]$parameters$probability.scale %>%\n    mutate(model = paste0(i, \"-Class Model\"))\n  \n  model_results <- rbind(model_results, temp)\n}\n\ncompare_plot <-\n  model_results %>%\n  filter(category == 2) %>%\n  dplyr::select(est, model, LatentClass, param) %>%\n  filter(model != \"6-Class Model\") #Remove from plot\n\ncompare_plot$param <- fct_inorder(compare_plot$param)\n\nggplot(\n  compare_plot,\n  aes(\n    x = param,\n    y = est,\n    color = LatentClass,\n    shape = LatentClass,\n    group = LatentClass,\n    lty = LatentClass\n  )\n) +\n  geom_point() +\n  geom_line() +\n  scale_colour_viridis_d() +\n  facet_wrap(~ model, ncol = 2) +\n  labs(title = \"Math Attitude Items\", x = \" \", y = \"Probability\") +\n  theme_minimal() +\n  theme(panel.grid.major.y = element_blank(),\n        axis.text.x = element_text(angle = -45, hjust = -.1))                            \nggsave(here(\"figures\", \"compare_kclass_plot_mod.png\"), dpi = \"retina\", bg = \"white\", height=5, width=7, units=\"in\")"},{"path":"moderation.html","id":"class-probability-plot-2","chapter":"7 Moderation","heading":"7.4.4 4-Class Probability Plot","text":"Use plot_lca function provided folder plot item probability plot. function requires one argument:\n- model_name: name Mplus readModels object (e.g., output_lsal$c4_lsal.)Save figure:","code":"\nsource(here(\"functions\",\"plot_lca.R\"))\n\nplot_lca(model_name = output_enum$c4_lsal.out)\nggsave(here(\"figures\", \"probability_plot_mod.png\"), dpi = \"retina\", bg = \"white\", height=5, width=7, units=\"in\")"},{"path":"moderation.html","id":"lca-moderation---ml-three-step","chapter":"7 Moderation","heading":"7.5 LCA Moderation - ML Three-Step","text":"","code":""},{"path":"moderation.html","id":"step-1---estimate-unconditional-model-w-auxiliary-specification","chapter":"7 Moderation","heading":"7.5.1 Step 1 - Estimate Unconditional Model w/ Auxiliary Specification","text":"Note: Ensure classes shift step (.g., Class 1 enumeration run now Class 4). Evaluate output compare class counts proportions latent classes. Using OPTSEED function ensures replication best loglikelihood value run.selecting latent class model, add class labels item probability plot using plot_lca_labels function. function requires three arguments:model_name: Mplus readModels object (e.g., output_lsal$c4_lsal.)item_labels: item labels x-axis (e.g.,c(“Enjoy”,“Useful”,“Logical”,“Job”,“Adult”))class_labels: class labels (e.g., c(“Pro-Science w/ Elevated Utility Value”, “Ambivalent w/ Minimal Utility Value”, “Ambivalent w/ Elevated Utility Value”, “Anti-Science w/ Minimal Utility Value”))Note: Use \\n add return label lengthy.","code":"\n\nstep1  <- mplusObject(\n  TITLE = \"Step 1 - Unconditional Model w/ Auxiliary Specification\", \n  VARIABLE = \"categorical = enjoy, useful, logical, job, adult;\n  usevar =  enjoy, useful, logical, job, adult;\n  classes = c(4);\n  AUXILIARY = female math_irt;\",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 0;\n    OPTSEED = 813779;\",\n  \n  SAVEDATA = \n   \"File=savedata.dat;\n    Save=cprob;\n    format=free;\",\n  \n  OUTPUT = \"sampstat residual tech11 tech14\",\n  \n  usevariables = colnames(data),\n  rdata = data)\n\nstep1_fit <- mplusModeler(step1,\n                            dataout=here(\"moderation\", \"three_step\", \"new.dat\"),\n                            modelout=here(\"moderation\", \"three_step\", \"one.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\nsource(here(\"functions\",\"plot_lca_labels.R\"))\n\n# Read in output from step 1.\noutput_one <- readModels(here(\"moderation\",\"three_step\",\"one.out\"))\n\n# Plot Title\ntitle <- \"LCA Probability Plot - LSAL\"\n\n#Identify item and class labels (Make sure they are in the order presented in the plot above)\nitem_labels <-  c(\n  \"I Enjoy \\nScience\",\n  \"Science is Useful \\nin Everyday Problems\",\n  \"Science Helps \\nLogical Thinking\",\n  \"Need Science for \\na Good Job\",\n  \"Will Use Science \\nOften as an Adult\"\n)\n\nclass_labels <- c(\n\n  \"Ambivalent w/ \\nElevated Utility Value\",\n  \"Anti-Science w/ \\nMinimal Utility Value\",\n  \"Ambivalent w/ \\nMinimal Utility Value\",\n  \"Pro-Science w/ \\nElevated Utility Value\"\n)\n\n# Plot LCA plot\nplot_lca_labels(model_name = output_one, item_labels, class_labels, title)\n\n# Save\nggsave(here(\"figures\", \"final_probability_plot_mod.png\"), dpi = \"retina\", bg = \"white\", height=7, width=10, units=\"in\")"},{"path":"moderation.html","id":"step-2---determine-measurement-error-1","chapter":"7 Moderation","heading":"7.5.2 Step 2 - Determine Measurement Error","text":"Extract logits classification probabilities likely latent class:Extract saved dataset step one:Rename column savedata named “C” change “N”:","code":"\nlogit_cprobs <- as.data.frame(output_one[[\"class_counts\"]]\n                                       [[\"logitProbs.mostLikely\"]])\nsavedata <- as.data.frame(output_one[[\"savedata\"]])\ncolnames(savedata)[colnames(savedata)==\"C\"] <- \"N\""},{"path":"moderation.html","id":"step-3---add-auxiliary-variables","chapter":"7 Moderation","heading":"7.5.3 Step 3 - Add Auxiliary Variables","text":"Build moderation model:","code":"\nstep3mod  <- mplusObject(\n  TITLE = \"LCA Moderation\", \n  \n  VARIABLE = \n \"nominal=N;\n  usevar = n;\n\n  classes = c(4);\n  \n  usevar = female math_irt;\" ,\n  \n  ANALYSIS = \n \"estimator = mlr; \n  type = mixture; \n  starts = 0;\",\n \n  MODEL =\n  glue(\n \"!DISTAL = math_irt, COVARIATE = female, MODERATOR = C\n \n  %OVERALL%\n  math_irt on female;\n  math_irt;\n\n     %C#1%\n  [n#1@{logit_cprobs[1,1]}];\n  [n#2@{logit_cprobs[1,2]}];\n  [n#3@{logit_cprobs[1,3]}];\n  \n  math_irt on female(s1);  ! conditional slope (class 1)\n  [math_irt](m1);          ! conditional distal mean\n  math_irt;                ! conditional distal variance (freely estimated)\n\n  %C#2%\n  [n#1@{logit_cprobs[2,1]}];\n  [n#2@{logit_cprobs[2,2]}];\n  [n#3@{logit_cprobs[2,3]}];\n  \n  math_irt on female(s2);\n  [math_irt](m2);\n  math_irt;\n  \n  %C#3%\n  [n#1@{logit_cprobs[3,1]}];\n  [n#2@{logit_cprobs[3,2]}];\n  [n#3@{logit_cprobs[3,3]}];\n  \n  math_irt on female(s3);\n  [math_irt](m3);\n  math_irt;\n\n  %C#4%\n  [n#1@{logit_cprobs[4,1]}];\n  [n#2@{logit_cprobs[4,2]}];\n  [n#3@{logit_cprobs[4,3]}];\n  \n  math_irt on female(s4);\n  [math_irt](m4);\n  math_irt; \"),\n  \n  MODELCONSTRAINT = \n   \"New (\n   diff12 diff13 diff23\n   diff14 diff24 diff34\n   slope12 slope13 slope23 \n    slope14 slope24 slope34);\n    \n    diff12 = m1-m2;  ! test distal outcome differences\n    diff13 = m1-m3;\n    diff23 = m2-m3;\n    diff14 = m1-m4;\n    diff24 = m2-m4;\n    diff34 = m3-m4;\n  \n    slope12 = s1-s2;  ! test pairwise slope differences\n    slope13 = s1-s3;\n    slope23 = s2-s3;\n    slope14 = s1-s4;\n    slope24 = s2-s4;\n    slope34 = s3-s4;\",\n  \n  MODELTEST = \" ! can run only a single Omnibus test per model \n    s1=s2;\n    s2=s3;\n    s3=s4;\",\n \n  usevariables = colnames(savedata), \n  rdata = savedata)\n\nstep3mod_fit <- mplusModeler(step3mod,\n               dataout=here(\"moderation\", \"three_step\", \"mod.dat\"), \n               modelout=here(\"moderation\", \"three_step\", \"three.inp\"), \n               check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"moderation.html","id":"wald-test-table-1","chapter":"7 Moderation","heading":"7.5.3.1 Wald Test Table","text":"","code":"\nmodelParams <- readModels(here(\"moderation\", \"three_step\", \"three.out\"))\n\n# Extract information as data frame\nwald <- as.data.frame(modelParams[[\"summaries\"]]) %>%\n  dplyr::select(WaldChiSq_Value:WaldChiSq_PValue) %>% \n  mutate(WaldChiSq_DF = paste0(\"(\", WaldChiSq_DF, \")\")) %>% \n  unite(wald_test, WaldChiSq_Value, WaldChiSq_DF, sep = \" \") %>% \n  rename(pval = WaldChiSq_PValue) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\".001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001))))\n\n# Create table\n\nwald %>% \n  gt() %>%\n    tab_header(\n    title = \"Wald Test of Paramter Constraints (Slope)\") %>%\n    cols_label(\n      wald_test = md(\"Wald Test (*df*)\"),\n      pval = md(\"*p*-value\")) %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\")"},{"path":"moderation.html","id":"table-of-slope-and-intercept-values-across-classes","chapter":"7 Moderation","heading":"7.5.3.2 Table of Slope and Intercept Values Across Classes","text":"","code":"\nmodelParams <- readModels(here(\"moderation\", \"three_step\", \"three.out\"))\n\n# Change these to how the variables are written in Mplus\nx <- \"FEMALE\"\ny <- \"MATH_IRT\"\n\n# Extract information as data frame\nvalues <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n  filter(param %in% c(x, y),\n         paramHeader != \"Residual.Variances\") %>% \n  mutate(param = str_replace(param, pattern = x, replacement = \"Slope\"),\n         param = str_replace(param, pattern = y, replacement = \"Intercept\")) %>% \n  mutate(LatentClass = sub(\"^\",\"Class \", LatentClass)) %>%  \n  dplyr::select(!paramHeader) %>% \n  mutate(se = paste0(\"(\", format(round(se,2), nsmall =2), \")\")) %>% \n    unite(estimate, est, se, sep = \" \") %>% \n  select(!est_se) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001))))\n\n\n# Create table\n\nvalues %>% \n  gt(groupname_col = \"LatentClass\", rowname_col = \"param\") %>%\n  tab_header(\n    title = \"Slope and Intercept Values Across Science Attitudes Classes\") %>%\n  cols_label(\n    estimate = md(\"Estimate (*se*)\"),\n    pval = md(\"*p*-value\")) %>% \n  sub_values(values = \"999.000\", replacement = \"-\") %>% \n  sub_missing(1:3,\n              missing_text = \"\") %>%\n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\")"},{"path":"moderation.html","id":"table-of-distal-outcome-differences","chapter":"7 Moderation","heading":"7.5.3.3 Table of Distal Outcome Differences","text":"","code":"\nmodelParams <- readModels(here(\"moderation\", \"three_step\", \"three.out\"))\n\n# Extract information as data frame\ndiff <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n  filter(grepl(\"DIFF\", param)) %>% \n  dplyr::select(param:pval) %>% \n  mutate(se = paste0(\"(\", format(round(se,2), nsmall =2), \")\")) %>% \n  unite(estimate, est, se, sep = \" \") %>% \n  mutate(param = str_remove(param, \"DIFF\"),\n         param = as.numeric(param)) %>% \n  separate(param, into = paste0(\"Group\", 1:2), sep = 1) %>% \n  mutate(class = paste0(\"Class \", Group1, \" vs \", Group2)) %>% \n  select(class, estimate, pval)\n\n\n# Create table\n\ndiff %>% \n  gt() %>%\n    tab_header(\n    title = \"Distal Outcome Differences\") %>%\n    cols_label(\n      class = \"Class\",\n      estimate = md(\"Mean (*se*)\"),\n      pval = md(\"*p*-value\")) %>% \n    sub_missing(1:3,\n              missing_text = \"\") %>%\n  fmt(3,\n    fns = function(x)\n      ifelse(x<0.05, paste0(scales::number(x, accuracy = .001), \"*\"),\n             scales::number(x, accuracy = .001))\n  ) %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\")"},{"path":"moderation.html","id":"table-of-slope-differences","chapter":"7 Moderation","heading":"7.5.3.4 Table of Slope Differences","text":"","code":"\nmodelParams <- readModels(here(\"moderation\", \"three_step\", \"three.out\"))\n\n# Extract information as data frame\ndiff2 <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n  filter(grepl(\"SLOPE\", param)) %>% \n  dplyr::select(param:pval) %>% \n  mutate(se = paste0(\"(\", format(round(se,2), nsmall =2), \")\")) %>% \n  unite(estimate, est, se, sep = \" \") %>% \n  mutate(param = str_remove(param, \"SLOPE\"),\n         param = as.numeric(param)) %>% \n  separate(param, into = paste0(\"Group\", 1:2), sep = 1) %>% \n  mutate(class = paste0(\"Class \", Group1, \" vs \", Group2)) %>% \n  select(class, estimate, pval) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001))))\n\n\n# Create table\n\ndiff2 %>% \n  gt() %>%\n    tab_header(\n    title = \"Slope Differences\") %>%\n    cols_label(\n      class = \"Class\",\n      estimate = md(\"Mean (*se*)\"),\n      pval = md(\"*p*-value\")) %>% \n    sub_missing(1:3,\n              missing_text = \"\") %>%\n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\")"},{"path":"moderation.html","id":"table-of-covariates","chapter":"7 Moderation","heading":"7.5.3.5 Table of Covariates","text":"","code":"\nmodelParams <- readModels(here(\"moderation\", \"three_step\", \"three.out\"))\n\nx <- \"FEMALE\"\nrename <- \"Gender\"\n\n# Extract information as data frame\ncov <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n  filter(param %in% c(x)) %>% \n  mutate(param = str_replace(param, x, rename)) %>% \n  mutate(LatentClass = sub(\"^\",\"Class \", LatentClass)) %>%  \n  dplyr::select(!paramHeader) %>% \n  mutate(se = paste0(\"(\", format(round(se,2), nsmall =2), \")\")) %>% \n    unite(estimate, est, se, sep = \" \") %>% \n  select(param, estimate, pval) %>% \n  distinct(param, .keep_all=TRUE)  %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001))))\n\n\n# Create table\n\ncov %>% \n  gt(groupname_col = \"LatentClass\", rowname_col = \"param\") %>%\n  tab_header(\n    title = \"Relations Between the Covariates and Distal Outcome\") %>%\n  cols_label(\n    estimate = md(\"Estimate (*se*)\"),\n    pval = md(\"*p*-value\")) %>% \n  sub_missing(1:3,\n              missing_text = \"\") %>%\n  sub_values(values = c(999.000), replacement = \"-\") %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\")"},{"path":"moderation.html","id":"plot-distal-outcome","chapter":"7 Moderation","heading":"7.5.3.6 Plot Distal Outcome","text":"","code":"\nmodelParams <- readModels(here(\"moderation\", \"three_step\", \"three.out\"))\n\ny <- \"MATH_IRT\"\n\n# Extract class size \nc_size <- as.data.frame(modelParams[[\"class_counts\"]][[\"modelEstimated\"]][[\"proportion\"]]) %>% \n  rename(\"cs\" = 1) %>% \n  mutate(cs = round(cs*100, 2))\n\n# Keep this code if you want a generic label for the classes\n#c_size_val <- paste0(\"C\", 1:nrow(c_size), glue(\" ({c_size[1:nrow(c_size),]}%)\"))\n\n# Otherwise use this:\nc_size_val <- paste0(class_labels, glue(\" ({c_size[1:nrow(c_size),]}%)\"))\n\n\n# Extract information as data frame\nestimates <- as.data.frame(modelParams[[\"parameters\"]][[\"unstandardized\"]]) %>%\n  filter(paramHeader == \"Intercepts\") %>%\n  dplyr::select(param, est, se) %>% \n  filter(param == y) %>% \n  mutate(across(c(est, se), as.numeric)) %>% \n  mutate(LatentClass = c_size_val)\n\n# Plot bar graphs\nestimates %>%\n  ggplot(aes(x=LatentClass, y = est, fill = LatentClass)) +\n  geom_col(position = \"dodge\", stat = \"identity\", color = \"black\") +\n  geom_errorbar(aes(ymin=est-se, ymax=est+se),\n                size=.3,    # Thinner lines\n                width=.2,\n                position=position_dodge(.9)) +\n  geom_text(aes(label = est), \n            family = \"serif\", size = 4,\n            position=position_dodge(.9),\n            vjust = 8) +  \n  #scale_fill_grey(start = .4, end = .7) +\n  labs(y=\"Math Score\", x=\"\") +\n  theme_cowplot() +\n  theme(text = element_text(family = \"serif\", size = 12),\n        axis.text.x = element_text(size=12),\n        legend.position=\"none\") +\n  coord_cartesian(ylim=c(0,80),  # Change ylim based on distal outcome range\n                  expand = FALSE) \n\n\n# Save plot\nggsave(here(\"figures\",\"distal_plot_mod.jpeg\"), dpi = \"retina\", bg = \"white\", width=10, height = 7, units=\"in\") "},{"path":"moderation.html","id":"plot-interaction","chapter":"7 Moderation","heading":"7.5.4 Plot Interaction","text":"","code":""},{"path":"moderation.html","id":"bar-plot","chapter":"7 Moderation","heading":"7.5.4.1 Bar plot","text":"","code":"\nmodelParams <- readModels(here(\"moderation\", \"three_step\", \"three.out\"))\n\nx <- \"FEMALE\"\n\n# Extract information as data frame\ndesc <- as.data.frame(modelParams$sampstat$univariate.sample.statistics) %>% \n  rownames_to_column(\"Variables\")\n\n# Select min amd max values of covariate\nxmin <- desc %>% \n  filter(Variables == x) %>% \n  dplyr::select(Minimum) %>% \n  as.numeric()\nxmax <- desc %>% \n  filter(Variables == x) %>% \n  dplyr::select(Maximum) %>% \n  as.numeric()\n\n# Add slope and intercept, Min and Max values \nline <- as.data.frame(modelParams$parameters$unstandardized) %>% \n  filter(str_detect(paramHeader, 'ON|Inter')) %>% \n  unite(\"param\", paramHeader:param, remove = TRUE) %>% \n  mutate(param = replace(param,agrep(\".ON\",param),\"slope\"),\n         param = replace(param,agrep(\"Inter\", param), \"intercept\"),\n         LatentClass = factor(LatentClass, labels = c_size_val)) %>% \n  dplyr::select(param, est, LatentClass) %>% \n  pivot_wider(names_from = param, values_from = est) %>%  \n  add_column(x_max = xmax,\n         x_min = xmin)\n\n\n# Add column with y values\ndata <- line %>% \n  mutate(y_min = (slope*xmin) + intercept,\n         y_max = (slope*xmax) + intercept) %>% \n  dplyr::select(-slope, -intercept) %>% \n  pivot_longer(-LatentClass, \n               names_to = c(\"xvalues\", \"yvalues\"), \n               names_sep=\"_\" ) %>% \n  pivot_wider(names_from = xvalues, values_from = value) %>% \n  dplyr::select(-yvalues) %>% \n  mutate(x = case_when(\n           x == 1 ~ \"Female\", ## Change these names\n           x == 0 ~ \"Male\")) ## Change these names\n  \n# Plot bar graphs\ndata %>%\n  ggplot(aes(x=factor(x), y = y, fill = LatentClass)) +\n  geom_col(position = \"dodge\", stat = \"identity\", color = \"black\") +\n  geom_text(aes(label = y), \n            family = \"serif\", size = 4,\n            position=position_dodge(.9),\n            vjust = -.5) +  \n  #scale_fill_grey(start = .4, end = .7) +\n  labs(y=\"Math Score\", x=\"\") +\n  theme_cowplot() +\n  theme(text = element_text(family = \"serif\", size = 12),\n        axis.text.x = element_text(size=12)) +\n  coord_cartesian(ylim=c(0,80),  # Change ylim based on distal outcome range\n                  expand = FALSE) \n\n# Save plot\nggsave(here(\"figures\",\"interaction_mod.jpeg\"), dpi = \"retina\", bg = \"white\", width=10, height = 7, units=\"in\") "},{"path":"moderation.html","id":"line-plot","chapter":"7 Moderation","heading":"7.5.4.2 Line plot","text":"can visualize slopes.’s also important report slope coefficients. ones assume significant based plots?","code":"\n\nx <- \"FEMALE\"\n\n# Minimum and Maximum Values\ndesc <- as.data.frame(modelParams$sampstat$univariate.sample.statistics) %>% \n  rownames_to_column(\"Variables\")\n\n\n# Select min amd max values of covariate\nxmin <- desc %>% \n  filter(Variables == x) %>% \n  dplyr::select(Minimum) %>% \n  as.numeric()\nxmax <- desc %>% \n  filter(Variables == x) %>% \n  dplyr::select(Maximum) %>% \n  as.numeric()\n\n# Add slope and intercept, Min and Max values \nline <- as.data.frame(modelParams$parameters$unstandardized) %>% \n  filter(str_detect(paramHeader, 'ON|Inter')) %>% \n  unite(\"param\", paramHeader:param, remove = TRUE) %>% \n  mutate(param = replace(param,agrep(\".ON\",param),\"slope\"),\n         param = replace(param,agrep(\"Inter\", param), \"intercept\"),\n         LatentClass = factor(LatentClass, labels = c_size_val)) %>% \n  dplyr::select(param, est, LatentClass) %>% \n  pivot_wider(names_from = param, values_from = est) %>%  \n  add_column(x_max = xmax,\n         x_min = xmin)\n\n\n# Add column with y values\ndata <- line %>% \n  mutate(y_min = (slope*xmin) + intercept,\n         y_max = (slope*xmax) + intercept) %>% \n  dplyr::select(-slope, -intercept) %>% \n  pivot_longer(-LatentClass, \n               names_to = c(\"xvalues\", \"yvalues\"), \n               names_sep=\"_\" ) %>% \n  pivot_wider(names_from = xvalues, values_from = value) %>% \n  dplyr::select(-yvalues) %>% \n  mutate(x = case_when(\n           x == 1 ~ \"Female\",\n           x == 0 ~ \"Male\"))\n  \n# Plot \ndata %>%\n  ggplot(aes(\n    x = factor(x),\n    y = y,\n    color = LatentClass,\n    group = LatentClass,\n    lty = LatentClass,\n    shape = LatentClass\n  )) +\n  geom_point(size = 4) +\n  geom_line(aes(group = LatentClass), size = 1) +\n  labs(x = \"\",\n       y = \"Math Score\") +\n  #scale_colour_grey(start = .3, end = .6) +\n  theme_cowplot() +\n  theme(\n    text = element_text(family = \"serif\", size = 12),\n    axis.text.x = element_text(size = 12),\n    legend.text = element_text(family = \"serif\", size = 10),\n    legend.position = \"top\",\n    legend.title = element_blank()\n  ) \n\n# Save\nggsave(here(\"figures\",\"slope_plot_mod.jpeg\"),  dpi = \"retina\", bg = \"white\", width=10, height = 7, units=\"in\")   "},{"path":"latent-transition-analysis.html","id":"latent-transition-analysis","chapter":"8 Latent Transition Analysis","heading":"8 Latent Transition Analysis","text":"Data Source: data used illustrate analyses include elementary school student Science Attitude survey items collected 7th 10th grades Longitudinal Study American Youth (LSAY; Miller, 2015).install package {rhdf5}Load packagesRead LSAY data file, lsay_new.csv.","code":"\nif (!requireNamespace(\"BiocManager\", quietly = TRUE)) \n  install.packages(\"BiocManager\")\n\n#BiocManager::install(\"rhdf5\")\nlibrary(MplusAutomation)\nlibrary(rhdf5)\nlibrary(tidyverse)       \nlibrary(here)            \nlibrary(glue)            \nlibrary(janitor)            \nlibrary(gt) \nlibrary(reshape2)\nlibrary(cowplot)\nlibrary(ggrepel)\nlibrary(haven)\nlibrary(modelsummary)\nlibrary(corrplot)\nlibrary(DiagrammeR)\nlibrary(filesstrings)\nlibrary(PNWColors)\n\nlsay_data <- read_csv(here(\"data\",\"lsay_lta.csv\"), na = c(\"9999\")) %>% \n    mutate(across(everything(), as.numeric))"},{"path":"latent-transition-analysis.html","id":"descriptive-statistics-4","chapter":"8 Latent Transition Analysis","heading":"8.1 Descriptive Statistics","text":"","code":""},{"path":"latent-transition-analysis.html","id":"data-summary","chapter":"8 Latent Transition Analysis","heading":"8.1.1 Data Summary","text":"","code":"\ndata <- lsay_data\n\nselect_data <- data %>% \n select(female, minority, ab39m:ga33l)\n\nf <- All(select_data) ~ Mean + SD + Min + Median + Max + Histogram\ndatasummary(f, data, output=\"markdown\")"},{"path":"latent-transition-analysis.html","id":"correlation-table","chapter":"8 Latent Transition Analysis","heading":"8.1.2 Correlation Table","text":"","code":"\nselect_data %>% \n  datasummary_correlation(output = \"markdown\")"},{"path":"latent-transition-analysis.html","id":"correlation-plot","chapter":"8 Latent Transition Analysis","heading":"8.1.3 Correlation Plot","text":"","code":"\nf_cor <- data %>% \n select(female, minority, ab39m:ga33l) %>% \n  cor(use = \"pairwise.complete.obs\")\n\ncorrplot(f_cor, \n         method = \"circle\",\n         type = \"upper\", \n         tl.col=\"black\", \n         tl.srt=45)"},{"path":"latent-transition-analysis.html","id":"enumeration-6","chapter":"8 Latent Transition Analysis","heading":"8.2 Enumeration","text":"","code":""},{"path":"latent-transition-analysis.html","id":"enumerate-time-point-1-7th-grade","chapter":"8 Latent Transition Analysis","heading":"8.2.1 Enumerate Time Point 1 (7th grade)","text":"NEXT STEP Check output (.) files check convergence warnings syntax errors.","code":"\n# NOTE CHANGE: '1:6' indicates the number of k-class models to estimate\n# User can change this number to fit research context\n# In this example, the code loops or iterates over values 1 through 6 ( '{k}' )\n#\nt1_enum_k_16  <- lapply(1:6, function(k) { \n  enum_t1  <- mplusObject(                 \n    \n# The 'glue' function inserts R code within a string or \"quoted green text\" using the syntax {---}\n#\n    TITLE = glue(\"Class-{k}_Time1\"), \n  \n    VARIABLE = glue( \n    \"!!! NOTE CHANGE: List of the five 7th grade science attitude indicators !!!\n     categorical = ab39m-ab39x; \n          usevar = ab39m-ab39x;\n     \n     classes = c({k});\"),\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    !!! NOTE CHANGE: The intial and final start values. Reduce to speed up estimation time. !!!\n    starts = 500 100;           \n    processors=10;\",\n  \n  OUTPUT = \"sampstat residual tech11 tech14;\",\n  \n  PLOT = \n    \"type = plot3; \n    series = ab39m-ab39x(*);\",\n  \n  usevariables = colnames(lsay_data),\n  rdata = lsay_data)\n\n# NOTE CHANGE: Fix to match appropriate sub-folder name\n# See after `here` function (e.g., \"enum_LCA_time1\")\nenum_t1_fit <- mplusModeler(enum_t1,\n                 dataout=here(\"lta\",\"enum_t1\",\"t1.dat\"), \n                 modelout=glue(here(\"lta\",\"enum_t1\",\"c{k}_lca_enum_time1.inp\")),\n                 check=TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"latent-transition-analysis.html","id":"enumerate-time-point-2-10th-grade","chapter":"8 Latent Transition Analysis","heading":"8.2.2 Enumerate Time Point 2 (10th grade)","text":"","code":"\n\nt2_enum_k_16  <- lapply(1:6, function(k) { \n  enum_t2  <- mplusObject(                 \n      \n    TITLE = glue(\"Class-{k}_Time2\"), \n  \n    VARIABLE = \n  glue( \n    \"!!! NOTE CHANGE: List of the five 10th grade science attitude indicators !!!\n     categorical = ga33a-ga33l; \n          usevar = ga33a-ga33l;\n    \n     classes = c({k});\"),\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 500 100;\n    processors=10;\",\n  \n  OUTPUT = \"sampstat residual tech11 tech14;\",\n  \n  PLOT = \n    \"type = plot3; \n    series = ga33a-ga33l(*);\",\n  \n  usevariables = colnames(lsay_data),\n  rdata = lsay_data)\n\nenum_t2_fit <- mplusModeler(enum_t2, \n                 dataout=here(\"lta\",\"enum_t2\",\"t2.dat\"),\n                 modelout=glue(here(\"lta\",\"enum_t2\",\"c{k}_lca_enum_time2.inp\")),\n                 check=TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"latent-transition-analysis.html","id":"create-model-fit-summary-table","chapter":"8 Latent Transition Analysis","heading":"8.2.3 Create Model Fit Summary Table","text":"Read models enumeration tableExtract model fit data","code":"\noutput_enum_t1 <- readModels(here(\"lta\",\"enum_t1\"), quiet = TRUE)\noutput_enum_t2 <- readModels(here(\"lta\",\"enum_t2\"), quiet = TRUE)\n\nenum_extract1 <- LatexSummaryTable(output_enum_t1,                                 \n                keepCols=c(\"Title\", \"Parameters\", \"LL\", \"BIC\", \"aBIC\",\n                           \"BLRT_PValue\", \"T11_VLMR_PValue\",\"Observations\"))   \n\nenum_extract2 <- LatexSummaryTable(output_enum_t2,                                 \n                keepCols=c(\"Title\", \"Parameters\", \"LL\", \"BIC\", \"aBIC\",\n                           \"BLRT_PValue\", \"T11_VLMR_PValue\",\"Observations\")) "},{"path":"latent-transition-analysis.html","id":"calculate-indices-derived-from-the-log-likelihood-ll","chapter":"8 Latent Transition Analysis","heading":"8.2.4 Calculate Indices Derived from the Log Likelihood (LL)","text":"","code":"\n                           \nallFit1 <- enum_extract1 %>% \n  mutate(aBIC = -2*LL+Parameters*log((Observations+2)/24)) %>% \n  mutate(CAIC = -2*LL+Parameters*(log(Observations)+1)) %>% \n  mutate(AWE = -2*LL+2*Parameters*(log(Observations)+1.5)) %>%\n  mutate(SIC = -.5*BIC) %>% \n  mutate(expSIC = exp(SIC - max(SIC))) %>% \n  mutate(BF = exp(SIC-lead(SIC))) %>% \n  mutate(cmPk = expSIC/sum(expSIC)) %>% \n  select(1:5,9:10,6:7,13,14) %>% \n  arrange(Parameters)\n\nallFit2 <- enum_extract2 %>% \n  mutate(aBIC = -2*LL+Parameters*log((Observations+2)/24)) %>% \n  mutate(CAIC = -2*LL+Parameters*(log(Observations)+1)) %>% \n  mutate(AWE = -2*LL+2*Parameters*(log(Observations)+1.5)) %>%\n  mutate(SIC = -.5*BIC) %>% \n  mutate(expSIC = exp(SIC - max(SIC))) %>% \n  mutate(BF = exp(SIC-lead(SIC))) %>% \n  mutate(cmPk = expSIC/sum(expSIC)) %>% \n  select(1:5,9:10,6:7,13,14) %>% \n  arrange(Parameters)\n\nallFit <- full_join(allFit1,allFit2)"},{"path":"latent-transition-analysis.html","id":"format-fit-table","chapter":"8 Latent Transition Analysis","heading":"8.3 Format Fit Table","text":"","code":"\nrows_m1 <- 1:6\nrows_m2 <- 7:12\n\n\nallFit %>% \n  mutate(Title = str_remove(Title, \"_Time*\")) %>% \n  gt() %>%\n  tab_header(\n    title = md(\"**Model Fit Summary Table**\")) %>% \n  cols_label(\n    Title = \"Classes\",\n    Parameters = md(\"Par\"),\n    LL = md(\"*LL*\"),\n    T11_VLMR_PValue = \"VLMR\",\n    BLRT_PValue = \"BLRT\",\n    BF = md(\"BF\"),\n    cmPk = md(\"*cmP_k*\")) %>%\n  tab_footnote(\n    footnote = md(\n    \"*Note.* Par = Parameters; *LL* = model log likelihood;\n      BIC = Bayesian information criterion;\n      aBIC = sample size adjusted BIC; CAIC = consistent Akaike information criterion;\n      AWE = approximate weight of evidence criterion;\n      BLRT = bootstrapped likelihood ratio test p-value;\n      VLMR = Vuong-Lo-Mendell-Rubin adjusted likelihood ratio test p-value;\n      cmPk = approximate correct model probability.\"), \n    locations = cells_title()) %>% \n  tab_options(column_labels.font.weight = \"bold\") %>% \n  fmt_number(10,decimals = 2,\n             drop_trailing_zeros=TRUE,\n             suffixing = TRUE) %>% \n  fmt_number(c(3:9,11), \n             decimals = 2) %>% \n  fmt_missing(1:11,\n              missing_text = \"--\") %>% \n  fmt(c(8:9,11),\n    fns = function(x) \n    ifelse(x<0.001, \"<.001\",\n           scales::number(x, accuracy = 0.01))) %>%\n  fmt(10, fns = function(x) \n    ifelse(x>100, \">100\",\n           scales::number(x, accuracy = .1))) %>%\n  tab_row_group(\n    group = \"Time-1\",\n    rows = 1:6) %>%\n  tab_row_group(\n    group = \"Time-2\",\n    rows = 7:12) %>% \n  row_group_order(\n      groups = c(\"Time-1\",\"Time-2\")\n      ) %>% \n   tab_style(\n    style = list(\n      cell_text(weight = \"bold\")\n      ),\n    locations = list(cells_body(\n     columns = BIC,\n     row = BIC == min(BIC[rows_m1]) # Model 1\n    ),\n    cells_body(\n     columns = aBIC,\n     row = aBIC == min(aBIC[rows_m1])\n    ),\n    cells_body(\n     columns = CAIC,\n     row = CAIC == min(CAIC[rows_m1])\n    ),\n    cells_body(\n     columns = AWE,\n     row = AWE == min(AWE[rows_m1])\n    ),\n    cells_body(\n     columns = cmPk,\n     row =  cmPk == max(cmPk[rows_m1])\n     ),   \n    cells_body(\n     columns = BIC,\n     row = BIC == min(BIC[rows_m2]) # Model 2\n    ),\n    cells_body(\n     columns = aBIC,\n     row = aBIC == min(aBIC[rows_m2])\n    ),\n    cells_body(\n     columns = CAIC,\n     row = CAIC == min(CAIC[rows_m2])\n    ),\n    cells_body(\n     columns = AWE,\n     row = AWE == min(AWE[rows_m2])\n    ),\n    cells_body(\n     columns = cmPk,\n     row =  cmPk == max(cmPk[rows_m2])\n     ),  \n    cells_body(\n     columns = BF, \n     row =  BF > 10),\n    cells_body(\n     columns =  BLRT_PValue,\n     row =  ifelse(BLRT_PValue < .05 & lead(BLRT_PValue) > .05, BLRT_PValue < .05, NA)),\n    cells_body(\n     columns =  T11_VLMR_PValue,\n     row =  ifelse(T11_VLMR_PValue < .05 & lead(T11_VLMR_PValue) > .05, T11_VLMR_PValue < .05, NA))\n  )\n)"},{"path":"latent-transition-analysis.html","id":"compare-time-1-time-2-lca-plots","chapter":"8 Latent Transition Analysis","heading":"8.4 Compare Time 1 & Time 2 LCA Plots","text":"Read models plotting (4-class models)","code":"\nmodel_t1_c4 <- output_enum_t1$c4_lca_enum_time1.out\nmodel_t2_c4 <- output_enum_t2$c4_lca_enum_time2.out"},{"path":"latent-transition-analysis.html","id":"create-a-function-plot_lca_function-that-requires-5-arguments","chapter":"8 Latent Transition Analysis","heading":"8.4.1 Create a function plot_lca_function that requires 5 arguments:","text":"model_name: name Mplus model object (e.g., model_t1_c4)item_num: number items LCA measurement model (e.g., 5)class_num: number classes (k) LCA model (e.g., 4)item_labels: item labels x-axis (e.g., c(\"Enjoy\",\"Useful\",\"Logical\",\"Job\",\"Adult\"))plot_title: include title plot (e.g., \"Time 1 LCA Conditional Item Probability Plot\")","code":"\n\nplot_lca_function <- function(model_name,item_num,class_num,item_labels,plot_title){\n\nmplus_model <- as.data.frame(model_name$gh5$means_and_variances_data$estimated_probs$values)\nplot_t1 <- mplus_model[seq(2, 2*item_num, 2),]\n\nc_size <- as.data.frame(model_name$class_counts$modelEstimated$proportion)\ncolnames(c_size) <- paste0(\"cs\")\nc_size <- c_size %>% mutate(cs = round(cs*100, 2))\ncolnames(plot_t1) <- paste0(\"C\", 1:class_num, glue(\" ({c_size[1:class_num,]}%)\"))\n\nplot_t1 <- cbind(Var = paste0(\"U\", 1:item_num), plot_t1)\nplot_t1$Var <- factor(plot_t1$Var,\n               labels = item_labels)\nplot_t1$Var <- fct_inorder(plot_t1$Var)\npd_long_t1 <- melt(plot_t1, id.vars = \"Var\") \n\np <- pd_long_t1 %>%\n  ggplot(aes(x = as.integer(Var), y = value,\n  shape = variable, colour = variable, lty = variable)) +\n  geom_point(size = 4) + geom_line() + \n  scale_x_continuous(\"\", breaks = 1:5, labels = plot_t1$Var) + \n  scale_colour_grey() + \n  labs(title = plot_title, y = \"Probability\") +\n  theme_cowplot() +\n  theme(legend.title = element_blank(), \n        legend.position = \"top\")\n\np\nreturn(p)\n}"},{"path":"latent-transition-analysis.html","id":"time-1-lca---conditional-item-probability-plot","chapter":"8 Latent Transition Analysis","heading":"8.4.2 Time 1 LCA - Conditional Item Probability Plot","text":"","code":"\n\nplot_lca_function(\n  model_name = model_t1_c4, \n  item_num = 5,\n  class_num = 4,\n  item_labels = c(\"Enjoy\",\"Useful\",\"Logical\",\"Job\",\"Adult\"),\n  plot_title = \"Time 1 LCA Conditional Item Probability Plot\"\n  )\nggsave(here(\"figures\", \"t1_c4_lca_plot.png\"), dpi=300, height=5, width=7, units=\"in\")"},{"path":"latent-transition-analysis.html","id":"time-2-lca---conditional-item-probability-plot","chapter":"8 Latent Transition Analysis","heading":"8.4.3 Time 2 LCA - Conditional Item Probability Plot","text":"","code":"\nplot_lca_function(\n  model_name = model_t2_c4,\n  item_num = 5,         \n  class_num = 4,\n  item_labels = c(\"Enjoy\",\"Useful\",\"Logical\",\"Job\",\"Adult\"),\n  plot_title = \"Time 2 LCA Conditional Item Probability Plot\"\n  )\nggsave(here(\"figures\", \"t2_c4_lca_plot.png\"), dpi=300, height=5, width=7, units=\"in\")"},{"path":"latent-transition-analysis.html","id":"estimate-latent-transition-analysis-lta-model","chapter":"8 Latent Transition Analysis","heading":"8.5 Estimate Latent Transition Analysis (LTA) Model","text":"","code":""},{"path":"latent-transition-analysis.html","id":"estimate-invariant-lta-model","chapter":"8 Latent Transition Analysis","heading":"8.5.1 Estimate Invariant LTA Model","text":"","code":"\n\nlta_inv <- mplusObject(\n  \n  TITLE = \n    \"Invariant LTA\", \n  \n  VARIABLE = \n     \"usev = ab39m ab39t ab39u ab39w ab39x  ! 7th grade indicators\n             ga33a ga33h ga33i ga33k ga33l; ! 10th grade indicators\n      \n      categorical = ab39m-ab39x ga33a-ga33l;\n\n      classes = c1(4) c2(4);\",\n    \n  ANALYSIS = \n     \"estimator = mlr;\n      type = mixture;\n      starts = 500 100;\n      processors=10;\",\n\n  MODEL = \n     \"%overall%\n      c2 on c1;\n\n      MODEL c1: \n      %c1#1%\n      [AB39M$1-AB39X$1] (1-5);  !!! labels that are repeated will constrain parameters to equality !!!\n      %c1#2%\n      [AB39M$1-AB39X$1] (6-10);\n      %c1#3%\n      [AB39M$1-AB39X$1] (11-15);\n      %c1#4%\n      [AB39M$1-AB39X$1] (16-20);\n\n      MODEL c2:\n      %c2#1%\n      [GA33A$1-GA33L$1] (1-5);\n      %c2#2%\n      [GA33A$1-GA33L$1] (6-10);\n      %c2#3%\n      [GA33A$1-GA33L$1] (11-15);\n      %c2#4%\n      [GA33A$1-GA33L$1] (16-20);\",\n   \n  SAVEDATA = \n   \"file = LTA_Inv_CPROBS.dat;\n    save = cprob;\n    missflag = 9999;\",\n\n  OUTPUT = \"tech1 tech15 svalues;\",\n  \n  usevariables = colnames(lsay_data),\n  rdata = lsay_data)\n\nlta_inv_fit <- mplusModeler(lta_inv,\n                 dataout=here(\"lta\",\"lta_model\",\"lta.dat\"),\n                 modelout=here(\"lta\",\"lta_model\",\"4-class-invariant.inp\"),\n                 check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"latent-transition-analysis.html","id":"estimate-non-invariant-estimated-lta-model","chapter":"8 Latent Transition Analysis","heading":"8.5.2 Estimate Non-Invariant Estimated LTA Model","text":"","code":"\n\nlta_non_inv <- mplusObject(\n  \n  TITLE = \n    \"Non-Invariant LTA\", \n  \n  VARIABLE = \n     \"usev = ab39m ab39t ab39u ab39w ab39x  ! 7th grade indicators\n             ga33a ga33h ga33i ga33k ga33l; ! 10th grade indicators\n      \n      categorical = ab39m-ab39x ga33a-ga33l;\n\n      classes = c1(4) c2(4);\",\n    \n  ANALYSIS = \n     \"estimator = mlr;\n      type = mixture;\n      starts = 500 100;\n      processors=10;\",\n\n  MODEL = \n     \"%overall%\n      c2 on c1; !!! estimate all multinomial logistic regressions !!!\n      \n      !!! The above syntax can also be written as: !!!\n               ! c2#1 on c1#1 c1#2 c1#3; !  \n               ! c2#2 on c1#1 c1#2 c1#3; !\n               ! c2#3 on c1#1 c1#2 c1#3; !\n\n      MODEL c1: !!! the following syntax will allow item thresholds to be estimated for each class (e.g. noninvariance) !!!\n      \n      %c1#1%\n      [AB39M$1-AB39X$1]; \n      %c1#2%\n      [AB39M$1-AB39X$1];\n      %c1#3%\n      [AB39M$1-AB39X$1];\n      %c1#4%\n      [AB39M$1-AB39X$1];\n\n      MODEL c2:\n      %c2#1%\n      [GA33A$1-GA33L$1];\n      %c2#2%\n      [GA33A$1-GA33L$1];\n      %c2#3%\n      [GA33A$1-GA33L$1];\n      %c2#4%\n      [GA33A$1-GA33L$1];\",\n\n  OUTPUT = \"tech1 tech15 svalues;\",\n  \n  usevariables = colnames(lsay_data),\n  rdata = lsay_data)\n\nlta_non_inv_fit <- mplusModeler(lta_non_inv,\n                     dataout=here(\"lta\",\"lta_model\",\"lta.dat\"),\n                     modelout=here(\"lta\",\"lta_model\",\"4-class-non-invariant.inp\"),\n                     check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"latent-transition-analysis.html","id":"conduct-sattorra-bentler-adjusted-log-likelihood-ratio-difference-testing","chapter":"8 Latent Transition Analysis","heading":"8.5.3 Conduct Sattorra-Bentler adjusted Log Likelihood Ratio Difference Testing","text":"non-invariant (comparison): model parameters.non-invariant (comparison): model parameters.invariant (nested): model less parameters.invariant (nested): model less parameters.RESULT: Log Likelihood \\(\\chi^2\\) difference test comparing invariant non-invariant LTA models , \\(\\chi^2 (20) = 21.542, p = .624\\).ReferenceRead invariance model extract parameters (intercepts multinomial regression coefficients)Manual method calculate transition probabilities:Although possible extract transition probabilities directly output following code illustrates parameters used calculate transition. useful conducting advanced LTA model specifications making specific constraints within transition matrices, testing equivalence specific transition probabilities.","code":"\n\n# *0 = null or nested model & *1 = comparison  or parent model\n\nlta_models <- readModels(here(\"lta\",\"lta_model\"), quiet = TRUE)\n\n# Log Likelihood Values\nL0 <- lta_models[[\"X4.class.invariant.out\"]][[\"summaries\"]][[\"LL\"]]\nL1 <- lta_models[[\"X4.class.non.invariant.out\"]][[\"summaries\"]][[\"LL\"]] \n\n# LRT equation\nlr <- -2*(L0-L1) \n\n# Parameters\np0 <- lta_models[[\"X4.class.invariant.out\"]][[\"summaries\"]][[\"Parameters\"]] \np1 <- lta_models[[\"X4.class.non.invariant.out\"]][[\"summaries\"]][[\"Parameters\"]]\n\n# Scaling Correction Factors\nc0 <- lta_models[[\"X4.class.invariant.out\"]][[\"summaries\"]][[\"LLCorrectionFactor\"]]\nc1 <- lta_models[[\"X4.class.non.invariant.out\"]][[\"summaries\"]][[\"LLCorrectionFactor\"]]\n\n# Difference Test Scaling correction\ncd <- ((p0*c0)-(p1*c1))/(p0-p1)\n\n# Chi-square difference test(TRd)\nTRd <- (lr)/(cd)\n\n# Degrees of freedom\ndf <- abs(p0 - p1)\n\n\n# Significance test\n(p_diff <- pchisq(TRd, df, lower.tail=FALSE))\n#> [1] 0.6245173\n\nlta_inv1 <- readModels(here(\"lta\",\"lta_model\",\"4-Class-Invariant.out\" ), quiet = TRUE)\n\npar <- as_tibble(lta_inv1[[\"parameters\"]][[\"unstandardized\"]]) %>% \n  select(1:3) %>% \n  filter(grepl('ON|Means', paramHeader)) %>% \n  mutate(est = as.numeric(est))\n# Name each parameter individually to make the subsequent calculations more readable\na1 <- unlist(par[13,3]); a2 <- unlist(par[14,3]); a3 <- unlist(par[15,3]); b11 <- unlist(par[1,3]);\nb21 <- unlist(par[4,3]); b31 <- unlist(par[7,3]); b12 <- unlist(par[2,3]); b22 <- unlist(par[5,3]);\nb32 <- unlist(par[8,3]); b13 <- unlist(par[3,3]); b23 <- unlist(par[6,3]); b33 <- unlist(par[9,3])\n\n# Calculate transition probabilities from the logit parameters\nt11 <- exp(a1+b11)/(exp(a1+b11)+exp(a2+b21)+exp(a3+b31)+exp(0))\nt12 <- exp(a2+b21)/(exp(a1+b11)+exp(a2+b21)+exp(a3+b31)+exp(0))\nt13 <- exp(a3+b31)/(exp(a1+b11)+exp(a2+b21)+exp(a3+b31)+exp(0))\nt14 <- 1 - (t11 + t12 + t13)\n\nt21 <- exp(a1+b12)/(exp(a1+b12)+exp(a2+b22)+exp(a3+b32)+exp(0))\nt22 <- exp(a2+b22)/(exp(a1+b12)+exp(a2+b22)+exp(a3+b32)+exp(0))\nt23 <- exp(a3+b32)/(exp(a1+b12)+exp(a2+b22)+exp(a3+b32)+exp(0))\nt24 <- 1 - (t21 + t22 + t23)\n\nt31 <- exp(a1+b13)/(exp(a1+b13)+exp(a2+b23)+exp(a3+b33)+exp(0))\nt32 <- exp(a2+b23)/(exp(a1+b13)+exp(a2+b23)+exp(a3+b33)+exp(0))\nt33 <- exp(a3+b33)/(exp(a1+b13)+exp(a2+b23)+exp(a3+b33)+exp(0))\nt34 <- 1 - (t31 + t32 + t33)\n\nt41 <- exp(a1)/(exp(a1)+exp(a2)+exp(a3)+exp(0))\nt42 <- exp(a2)/(exp(a1)+exp(a2)+exp(a3)+exp(0))\nt43 <- exp(a3)/(exp(a1)+exp(a2)+exp(a3)+exp(0))\nt44 <- 1 - (t41 + t42 + t43)"},{"path":"latent-transition-analysis.html","id":"create-transition-table","chapter":"8 Latent Transition Analysis","heading":"8.5.4 Create Transition Table","text":"","code":"\n\nt_matrix <- tibble(\n  \"Time1\" = c(\"C1=Anti-Science\",\"C1=Amb. w/ Elevated\",\"C1=Amb. w/ Minimal\",\"C1=Pro-Science\"),\n  \"C2=Anti-Science\" = c(t11,t21,t31,t41),\n  \"C2=Amb. w/ Elevated\" = c(t12,t22,t32,t42),\n  \"C2=Amb. w/ Minimal\" = c(t13,t23,t33,t43),\n  \"C2=Pro-Science\" = c(t14,t24,t34,t44))\n\nt_matrix %>% \n  gt(rowname_col = \"Time1\") %>%\n  tab_stubhead(label = \"7th grade\") %>% \n  tab_header(\n    title = md(\"**Student transitions from 7th grade (rows) to 10th grade (columns)**\"),\n    subtitle = md(\"&nbsp;\")) %>% \n  fmt_number(2:5,decimals = 2) %>% \n  tab_spanner(label = \"10th grade\",columns = 2:5) %>% \n  tab_footnote(\n    footnote = md(\n    \"*Note.* Transition matrix values are the identical to Table 5, however Table 5 \n    has the values rearranged by class for interpretation purposes. Classes may be arranged\n    directly through Mplus syntax using start values.\"), \n    locations = cells_title())"},{"path":"latent-transition-analysis.html","id":"adding-covariates","chapter":"8 Latent Transition Analysis","heading":"8.6 Adding Covariates","text":"use ML three-step method estimate LTA models predictors distal outcomes). Estimate unconditional model latent variable predictors included auxiliary option least one models.Covariatessci_issues7: Interest science issues (1 = interested, 2 = Moderately Interested, 3 = interested)sci_irt7: 7th Grade Science IRT Score (Continuous)female: Gender (0 = Male, 1 = Female)","code":""},{"path":"latent-transition-analysis.html","id":"step-1---estimate-unconditional-model-w-auxiliary-specification-1","chapter":"8 Latent Transition Analysis","heading":"8.6.1 Step 1 - Estimate Unconditional Model w/ Auxiliary Specification","text":"7th Grade10th GradePlot Time 1Plot Time 2","code":"\n\nstep1  <- mplusObject(\n  TITLE = \"Step 1 - T1\", \n  VARIABLE = \n  \"usevar = ab39m ab39t ab39u ab39w ab39x;\n  categorical = ab39m ab39t ab39u ab39w ab39x;\n    \n   classes = c(4); \n    \n   auxiliary = sci_issues7 sci_irt7 female;\n  \n   idvariable = casenum;\",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 0;\n    optseed = 534483;\",\n  \n  SAVEDATA = \n   \"File=3step_t1.dat;\n    Save=cprob;\",\n  \n  OUTPUT = \"residual tech11 tech14 svalues\",\n  \n  PLOT = \n    \"type = plot3; \n    series = ab39m-ab39x(*);\",\n  \n  usevariables = colnames(lsay_data),\n  rdata = lsay_data)\n\nstep1_fit <- mplusModeler(step1,\n                            dataout=here(\"lta\",\"cov_model\",\"t1.dat\"),\n                            modelout=here(\"lta\",\"cov_model\",\"one_T1.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\n\nstep1  <- mplusObject(\n  TITLE = \"Step 1 - T1\", \n  VARIABLE = \n  \"usevar =  ga33a ga33h ga33i ga33k ga33l;\n  categorical =  ga33a ga33h ga33i ga33k ga33l;\n    \n   classes = c(4); \n    \n   !auxiliary = sci_issues7 sci_irt7 female;\n  \n   idvariable = casenum;\",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 0;\n    optseed = 392418;\",\n  \n  SAVEDATA = \n   \"File=3step_t2.dat;\n    Save=cprob;\",\n  \n  OUTPUT = \"residual tech11 tech14 svalues\",\n\n  PLOT = \n    \"type = plot3; \n    series = ga33a-ga33l(*);\",\n  \n  usevariables = colnames(lsay_data),\n  rdata = lsay_data)\n\nstep1_fit <- mplusModeler(step1,\n                            dataout=here(\"lta\",\"cov_model\",\"t2.dat\"),\n                            modelout=here(\"lta\",\"cov_model\",\"one_T2.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\n\noutput_T1 <- readModels(here(\"lta\",\"cov_model\",\"one_T1.out\"))\n\nplot_lca_function(\n  model_name = output_T1, \n  item_num = 5,\n  class_num = 4,\n  item_labels = c(\"Enjoy\",\"Useful\",\"Logical\",\"Job\",\"Adult\"),\n  plot_title = \"Time 1 LCA Conditional Item Probability Plot\"\n  )\n\noutput_T2 <- readModels(here(\"lta\",\"cov_model\",\"one_T2.out\"))\n\nplot_lca_function(\n  model_name = output_T2, \n  item_num = 5,\n  class_num = 4,\n  item_labels = c(\"Enjoy\",\"Useful\",\"Logical\",\"Job\",\"Adult\"),\n  plot_title = \"Time 2 LCA Conditional Item Probability Plot\"\n  )"},{"path":"latent-transition-analysis.html","id":"step-2---determine-measurement-error-2","chapter":"8 Latent Transition Analysis","heading":"8.6.2 Step 2 - Determine Measurement Error","text":"Extract logits classification probabilities likely latent class:Extract saved dataset:Rename column savedata named “C” change “N”","code":"\n\nlogit_cprobs_T1 <- as.data.frame(output_T1[[\"class_counts\"]]\n                                       [[\"logitProbs.mostLikely\"]])\n\nlogit_cprobs_T2 <- as.data.frame(output_T2[[\"class_counts\"]]\n                                       [[\"logitProbs.mostLikely\"]])\n\nsavedata_T1 <- as.data.frame(output_T1[[\"savedata\"]])\nsavedata_T2 <- as.data.frame(output_T2[[\"savedata\"]])\n\ncolnames(savedata_T1)[colnames(savedata_T1)==\"C\"] <- \"N_T1\"\ncolnames(savedata_T2)[colnames(savedata_T2)==\"C\"] <- \"N_T2\"\n\nsavedata <- savedata_T1 %>% \n  full_join(savedata_T2, by = \"CASENUM\")"},{"path":"latent-transition-analysis.html","id":"step-3---add-auxiliary-variables-1","chapter":"8 Latent Transition Analysis","heading":"8.6.3 Step 3 - Add Auxiliary Variables","text":"","code":"\nstep3  <- mplusObject(\n  TITLE = \"ML Three Step LTA Model\", \n  \n  VARIABLE = \n \"nominal=N_T1 N_T2;\n  usevar = N_T1 N_T2 SCI_IRT7 FEMALE;\n  classes = c1(4) c2(4);\" ,\n  \n  ANALYSIS = \n \"estimator = mlr; \n  type = mixture; \n  starts = 0;\",\n \n  MODEL =\n  glue(\n \" %OVERALL%\n      \n   c2 on c1; \n   c1 c2 on  SCI_IRT7 FEMALE;\n\n  MODEL c1:\n  \n  %c1#1%\n  [N_T1#1@{logit_cprobs_T1[1,1]}];\n  [N_T1#2@{logit_cprobs_T1[1,2]}];\n  [N_T1#3@{logit_cprobs_T1[1,3]}];\n\n  %c1#2%\n  [N_T1#1@{logit_cprobs_T1[2,1]}];\n  [N_T1#2@{logit_cprobs_T1[2,2]}];\n  [N_T1#3@{logit_cprobs_T1[2,3]}];\n  \n  %c1#3%\n  [N_T1#1@{logit_cprobs_T1[3,1]}]; \n  [N_T1#2@{logit_cprobs_T1[3,2]}];\n  [N_T1#3@{logit_cprobs_T1[3,3]}];\n  \n  %c1#4%\n  [N_T1#1@{logit_cprobs_T1[4,1]}]; \n  [N_T1#2@{logit_cprobs_T1[4,2]}];\n  [N_T1#3@{logit_cprobs_T1[4,3]}];\n\n \n  MODEL c2:\n  \n  %c2#1%\n  [N_T2#1@{logit_cprobs_T2[1,1]}]; \n  [N_T2#2@{logit_cprobs_T2[1,2]}];\n  [N_T2#3@{logit_cprobs_T2[1,3]}];\n\n  %c2#2%\n  [N_T2#1@{logit_cprobs_T2[2,1]}];\n  [N_T2#2@{logit_cprobs_T2[2,2]}];\n  [N_T2#3@{logit_cprobs_T2[2,3]}];\n\n  \n  %c2#3%\n  [N_T2#1@{logit_cprobs_T2[3,1]}];\n  [N_T2#2@{logit_cprobs_T2[3,2]}];\n  [N_T2#3@{logit_cprobs_T2[3,3]}];\n \n   %c2#4%\n  [N_T2#1@{logit_cprobs_T2[4,1]}];\n  [N_T2#2@{logit_cprobs_T2[4,2]}];\n  [N_T2#3@{logit_cprobs_T2[4,3]}];\"),\n \n \n  OUTPUT = \"tech15;\",\n  \n  usevariables = colnames(savedata), \n  rdata = savedata)\n\nstep3_fit <- mplusModeler(step3,\n               dataout=here(\"lta\",\"cov_model\",\"three.dat\"), \n               modelout=here(\"lta\",\"cov_model\",\"three.inp\"), \n               check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"latent-transition-analysis.html","id":"lta-transition-plot","chapter":"8 Latent Transition Analysis","heading":"8.6.3.1 LTA Transition Plot","text":"code adapted source code plotLTA function found NOTE: function found plot_transitions_function.R specific model 2 time-points 4-classes & must updated accommodate models.Table:","code":"\nsource(here(\"functions\",\"plot_transitions_function.R\"))\n\nlta_model <- readModels(here(\"lta\",\"cov_model\",\"three.out\"))\n\nplot_transitions_function(\n  model_name = lta_model,\n  color_pallete = pnw_palette(\"Bay\", n=4, type = \"discrete\"),\n  facet_labels =c(\n    `1` = \"Transitions to 10th Grade from the Pro-Science w/ Elevated Utility Class\",\n    `2` = \"Transitions to 10th Grade from the Ambivalent w/ Elevated Utility Class\",\n    `3` = \"Transitions to 10th Grade from the Ambivalent w/ Minimal Utility Class\",\n    `4` = \"Transitions to 10th Grade from the Anti-Science w/ Minimal Utility Class\"),\n  timepoint_labels = c('1' = \"7th Grade\", '2' = \"10th Grade\"),\n  class_labels = c(\n    \"Pro-Science\",\n    \"Amb. / Elev. Utility\",\n    \"Amb. / Min. Utility\",\n    \"Anti-Science\")\n  )\nlta_prob <- as.data.frame(lta_model$class_counts$transitionProbs$probability)\n\n\nt_matrix <- tibble(\n  \"7th Grade\" = c(\"Pro-Science\",\"Amb. / Elev. Utility\",\"Amb. / Min. Utility\",\"Anti-Science\"),\n  \"Pro-Science\" = c(lta_prob[1,1],lta_prob[2,1],lta_prob[3,1],lta_prob[4,1]),\n  \"Amb. / Elev. Utility\" = c(lta_prob[5,1],lta_prob[6,1],lta_prob[7,1],lta_prob[8,1]),\n  \"Amb. / Min. Utility\" = c(lta_prob[9,1],lta_prob[10,1],lta_prob[11,1],lta_prob[12,1]),\n  \"Anti-Science\" = c(lta_prob[13,1],lta_prob[14,1],lta_prob[15,1],lta_prob[16,1]))\n\nt_matrix %>% \n  gt(rowname_col = \"7th Grade\") %>%\n  tab_stubhead(label = \"7th Grade\") %>% \n  tab_header(\n    title = md(\"**Transition Probabilities**\")) %>% \n  fmt_number(2:5,decimals = 2) %>% \n  tab_spanner(label = \"10th Grade\",columns = 2:5)#%>% \n  #gtsave(\"matrix.docx\")"},{"path":"latent-transition-analysis.html","id":"covariate-table","chapter":"8 Latent Transition Analysis","heading":"8.6.3.2 Covariate Table","text":"","code":"\n# REFERENCE CLASS 4\ncov <- as.data.frame(lta_model[[\"parameters\"]][[\"unstandardized\"]]) %>%\n  filter(param %in% c(\"SCI_IRT7\", \"FEMALE\")) %>% \n  mutate(param = case_when(\n            param == \"SCI_IRT7\" ~ \"Science IRT Score\",\n            param == \"FEMALE\" ~ \"Gender\"),\n    se = paste0(\"(\", format(round(se,2), nsmall =2), \")\")) %>% \n  separate(paramHeader, into = c(\"Time\", \"Class\"), sep = \"#\") %>% \n  mutate(Class = case_when(\n            Class == \"1.ON\" ~ \"Pro-Science\",\n            Class == \"2.ON\" ~ \"Amb. / Elev. Utility\",\n            Class == \"3.ON\" ~ \"Amb. / Min. Utility\"),\n         Time = case_when(\n            Time == \"C1\" ~ \"7th Grade (T1)\",\n            Time == \"C2\" ~ \"10th Grade (T2)\",\n         )\n         ) %>% \n  unite(estimate, est, se, sep = \" \") %>% \n  select(Time:pval, -est_se) %>% \n  mutate(pval = ifelse(pval<0.001, paste0(\"<.001*\"),\n                       ifelse(pval<0.05, paste0(scales::number(pval, accuracy = .001), \"*\"),\n                              scales::number(pval, accuracy = .001))))\n\n\n# Create table\n\ncov_m1 <- cov %>% \n  group_by(param, Class) %>% \n  gt() %>% \n  tab_header(\n    title = \"Relations Between the Covariates and Latent Class\") %>%\n  tab_footnote(\n    footnote = md(\n      \"Reference Group: Anti-Science\"\n    ),\nlocations = cells_title()\n  ) %>% \n  cols_label(\n    param = md(\"Covariate\"),\n    estimate = md(\"Estimate (*se*)\"),\n    pval = md(\"*p*-value\")) %>% \n  sub_missing(1:3,\n              missing_text = \"\") %>%\n  sub_values(values = c(999.000), replacement = \"-\") %>% \n  cols_align(align = \"center\") %>% \n  opt_align_table_header(align = \"left\") %>% \n  gt::tab_options(table.font.names = \"serif\") \n\ncov_m1"},{"path":"latent-transition-analysis.html","id":"manually-calculate-transition-probabilities-by-covariate","chapter":"8 Latent Transition Analysis","heading":"8.6.3.3 Manually calculate transition probabilities by covariate","text":"Read invariance model extract parameters (intercepts multinomial regression coefficients)Manual method calculate transition probabilities covariate:Create table","code":"\nstep3  <- mplusObject(\n  TITLE = \"LTA (invariant)\", \n  \n  VARIABLE = \n \"usevar = ab39m ab39t ab39u ab39w ab39x  ! 7th grade indicators\n               ga33a ga33h ga33i ga33k ga33l FEMALE;\n  categorical = ab39m-ab39x ga33a-ga33l;\n  classes = c1(4) c2(4);\" ,\n  \n  ANALYSIS = \n \"estimator = mlr; \n  type = mixture; \n  starts = 500 100;\n processors = 10;\",\n \n  MODEL =\n  \"%overall%\n\n    c2 c1 on FEMALE;\n\n      c2#1 on c1#1 (b11);\n      c2#2 on c1#1 (b21);\n      c2#3 on c1#1 (b31);\n    \n      c2#1 on c1#2 (b12);\n      c2#2 on c1#2 (b22);\n      c2#3 on c1#2 (b32);    \n\n      c2#1 on c1#3 (b13);\n      c2#2 on c1#3 (b23);\n      c2#3 on c1#3 (b33);\n\n      [c2#1] (a1);\n      [c2#2] (a2);\n      [c2#3] (a3);\n\n     c2#1 ON female (b212);\n     c2#2 ON female (b222);\n     c2#3 ON female (b232);\n     c1#1 ON female (b112);\n     c1#2 ON female (b122);\n     c1#3 ON female (b132);\n\n      MODEL c1: \n      %c1#1%\n      [AB39M$1-AB39X$1] (1-5);  !!! labels that are repeated will constrain parameters to equality !!!\n      %c1#2%\n      [AB39M$1-AB39X$1] (6-10);\n      %c1#3%\n      [AB39M$1-AB39X$1] (11-15);\n      %c1#4%\n      [AB39M$1-AB39X$1] (16-20);\n\n      MODEL c2:\n      %c2#1%\n      [GA33A$1-GA33L$1] (1-5);\n      %c2#2%\n      [GA33A$1-GA33L$1] (6-10);\n      %c2#3%\n      [GA33A$1-GA33L$1] (11-15);\n      %c2#4%\n      [GA33A$1-GA33L$1] (16-20);\",\n \n \n  OUTPUT = \"tech1 tech15 svalues;\",\n \n  MODELCONSTRAINT = \"  ! Compute joint and marginal probabilities:\n        New(\n        t11 t12 t13 t14\n        t21 t22 t23 t24\n        t31 t32 t33 t34\n        t41 t42 t43 t44\n\n        t11B t12B t13B t14B\n        t21B t22B t23B t24B\n        t31B t32B t33B t34B\n        t41B t42B t43B t44B\n        \n        diff_11_22 x\n        \n       );\n\n        t11 = exp(a1 +b11)/(exp(a1+b11)+exp(a2+b21)+exp(a3+b31)+exp(0));\n        t12 = exp(a2 +b21)/(exp(a1+b11)+exp(a2+b21)+exp(a3+b31)+exp(0));\n        t13 = exp(a3 +b31)/(exp(a1+b11)+exp(a2+b21)+exp(a3+b31)+exp(0));\n        t14 = 1 - (t11+t12+t13);\n\n        t21 = exp(a1 +b12)/(exp(a1+b12)+exp(a2+b22)+exp(a3+b32)+exp(0));\n        t22 = exp(a2 +b22)/(exp(a1+b12)+exp(a2+b22)+exp(a3+b32)+exp(0));\n        t23 = exp(a3 +b32)/(exp(a1+b12)+exp(a2+b22)+exp(a3+b32)+exp(0));\n        t24 = 1 - (t21+t22+t23);\n\n        t31 = exp(a1 +b13)/(exp(a1+b13)+exp(a2+b23)+exp(a3+b33)+exp(0));\n        t32 = exp(a2 +b23)/(exp(a1+b13)+exp(a2+b23)+exp(a3+b33)+exp(0));\n        t33 = exp(a3 +b33)/(exp(a1+b13)+exp(a2+b23)+exp(a3+b33)+exp(0));\n        t34 = 1 - (t31+t32+t33);\n\n        t41 = exp(a1)/(exp(a1)+exp(a2)+exp(a3)+exp(0));\n        t42 = exp(a2)/(exp(a1)+exp(a2)+exp(a3)+exp(0));\n        t43 = exp(a3)/(exp(a1)+exp(a2)+exp(a3)+exp(0));\n        t44 = 1 - (t41+t42+t43);\n        \n        \n     !c1#3 ON female (b132);\n\n x= 1  ; ! x=1 is female, x=0 males\n\nt11B = exp(a1 +b11+b212*x)/(exp(a1+b11+b212*x)+exp(a2+b21+b222*x)+exp(a3+b31+b232*x)\n+exp(0));\nt12B = exp(a2 +b21+b222*x)/(exp(a1+b11+b212*x)+exp(a2+b21+b222*x)+exp(a3+b31+b232*x)\n+exp(0));\nt13B = exp(a3 +b31+b232*x)/(exp(a1+b11+b212*x)+exp(a2+b21+b222*x)+exp(a3+b31+b232*x)\n+exp(0));\nt14B = 1 - (t11B+t12B+t13B);\n\nt21B = exp(a1 +b12+b212*x)/(exp(a1+b12+b212*x)+exp(a2+b22+b222*x)+exp(a3+b32+b232*x)\n+exp(0));\nt22B = exp(a2 +b22+b222*x)/(exp(a1+b12+b212*x)+exp(a2+b22+b222*x)+exp(a3+b32+b232*x)\n+exp(0));\nt23B = exp(a3 +b32+b232*x)/(exp(a1+b12+b212*x)+exp(a2+b22+b222*x)+exp(a3+b32+b232*x)\n+exp(0));\nt24B = 1 - (t21B+t22B+t23B);\n\nt31B = exp(a1 +b13+b212*x)/(exp(a1+b13+b212*x)+exp(a2+b23+b222*x)+exp(a3+b33+b232*x)\n+exp(0));\nt32B = exp(a2 +b23+b222*x)/(exp(a1+b13+b212*x)+exp(a2+b23+b222*x)+exp(a3+b33+b232*x)\n+exp(0));\nt33B = exp(a3 +b33+b232*x)/(exp(a1+b13+b212*x)+exp(a2+b23+b222*x)+exp(a3+b33+b232*x)\n+exp(0));\nt34B = 1 - (t31B+t32B+t33B);\n\nt41B = exp(a1+b212*x)/(exp(a1+b212*x)+exp(a2+b222*x)+exp(a3+b232*x)+exp(0));\nt42B = exp(a2+b222*x)/(exp(a1+b212*x)+exp(a2+b222*x)+exp(a3+b232*x)+exp(0));\nt43B = exp(a3+b232*x)/(exp(a1+b212*x)+exp(a2+b222*x)+exp(a3+b232*x)+exp(0));\nt44B = 1 - (t41B+t42B+t43B);\n\n        diff_11_22= t11-t11B;\",\n  \n  usevariables = colnames(savedata), \n  rdata = savedata)\n\nstep3_fit <- mplusModeler(step3,\n               dataout=here(\"lta\",\"cov_model\",\"calc_tran.dat\"), \n               modelout=here(\"lta\",\"cov_model\",\"calc_tran.inp\"), \n               check=TRUE, run = TRUE, hashfilename = FALSE)\n\nlta_inv1 <- readModels(here(\"lta\",\"cov_model\",\"calc_tran.out\" ), quiet = TRUE)\n\npar <- as_tibble(lta_inv1[[\"parameters\"]][[\"unstandardized\"]]) %>% \n  select(1:3) %>% \n  filter(grepl('ON|Means|Intercept', paramHeader)) %>% \n  mutate(est = as.numeric(est),\n         label = c(\"b11\", \"b12\", \"b13\", \"b21\", \"b22\", \"b23\", \"b31\", \"b32\", \"b33\", \"b212\", \"b222\", \"b232\", \"b112\", \"b122\", \"b132\", \"a11\", \"a21\", \"a31\", \"a12\", \"a22\", \"a32\"))\n# Name each parameter individually to make the subsequent calculations more readable\na1 <- unlist(par[19,3]); \na2 <- unlist(par[20,3]); \na3 <- unlist(par[21,3]);\n\nb11 <- unlist(par[1,3]);\nb21 <- unlist(par[4,3]); \nb31 <- unlist(par[7,3]); \n\nb12 <- unlist(par[2,3]); \nb22 <- unlist(par[5,3]);\nb32 <- unlist(par[8,3]); \n\nb13 <- unlist(par[3,3]); \nb23 <- unlist(par[6,3]); \nb33 <- unlist(par[9,3]);\n\nb212 <- unlist(par[10,3]);\nb222 <- unlist(par[11,3]);\nb232 <- unlist(par[12,3]);\n\nb112 <- unlist(par[13,3]);\nb122 <- unlist(par[14,3]);\nb132 <- unlist(par[15,3]);\n\nx <- 0 # x=1 is female, x=0 males\n\n# Calculate transition probabilities from the logit parameters\nt11B <- exp(a1 + b11 + b212*x) / (exp(a1 + b11 + b212*x) + exp(a2 + b21 + b222*x) + exp(a3 + b31 + b232*x) + exp(0))\nt12B <- exp(a2 + b21 + b222*x) / (exp(a1 + b11 + b212*x) + exp(a2 + b21 + b222*x) + exp(a3 + b31 + b232*x) + exp(0))\nt13B <- exp(a3 + b31 + b232*x) / (exp(a1 + b11 + b212*x) + exp(a2 + b21 + b222*x) + exp(a3 + b31 + b232*x) + exp(0))\nt14B <- 1 - (t11B + t12B + t13B)\n\nt21B <- exp(a1 + b12 + b212*x) / (exp(a1 + b12 + b212*x) + exp(a2 + b22 + b222*x) + exp(a3 + b32 + b232*x) + exp(0))\nt22B <- exp(a2 + b22 + b222*x) / (exp(a1 + b12 + b212*x) + exp(a2 + b22 + b222*x) + exp(a3 + b32 + b232*x) + exp(0))\nt23B <- exp(a3 + b32 + b232*x) / (exp(a1 + b12 + b212*x) + exp(a2 + b22 + b222*x) + exp(a3 + b32 + b232*x) + exp(0))\nt24B <- 1 - (t21B + t22B + t23B)\n\nt31B <- exp(a1 + b13 + b212*x) / (exp(a1 + b13 + b212*x) + exp(a2 + b23 + b222*x) + exp(a3 + b33 + b232*x) + exp(0))\nt32B <- exp(a2 + b23 + b222*x) / (exp(a1 + b13 + b212*x) + exp(a2 + b23 + b222*x) + exp(a3 + b33 + b232*x) + exp(0))\nt33B <- exp(a3 + b33 + b232*x) / (exp(a1 + b13 + b212*x) + exp(a2 + b23 + b222*x) + exp(a3 + b33 + b232*x) + exp(0))\nt34B <- 1 - (t31B + t32B + t33B)\n\nt41B <- exp(a1 + b212*x) / (exp(a1 + b212*x) + exp(a2 + b222*x) + exp(a3 + b232*x) + exp(0))\nt42B <- exp(a2 + b222*x) / (exp(a1 + b212*x) + exp(a2 + b222*x) + exp(a3 + b232*x) + exp(0))\nt43B <- exp(a3 + b232*x) / (exp(a1 + b212*x) + exp(a2 + b222*x) + exp(a3 + b232*x) + exp(0))\nt44B <- 1 - (t41B + t42B + t43B)\n\nx <- 1 # x=1 is female, x=0 males\n\n# Calculate transition probabilities from the logit parameters\nt11 <- exp(a1 + b11 + b212*x) / (exp(a1 + b11 + b212*x) + exp(a2 + b21 + b222*x) + exp(a3 + b31 + b232*x) + exp(0))\nt12 <- exp(a2 + b21 + b222*x) / (exp(a1 + b11 + b212*x) + exp(a2 + b21 + b222*x) + exp(a3 + b31 + b232*x) + exp(0))\nt13 <- exp(a3 + b31 + b232*x) / (exp(a1 + b11 + b212*x) + exp(a2 + b21 + b222*x) + exp(a3 + b31 + b232*x) + exp(0))\nt14 <- 1 - (t11 + t12 + t13)\n\nt21 <- exp(a1 + b12 + b212*x) / (exp(a1 + b12 + b212*x) + exp(a2 + b22 + b222*x) + exp(a3 + b32 + b232*x) + exp(0))\nt22 <- exp(a2 + b22 + b222*x) / (exp(a1 + b12 + b212*x) + exp(a2 + b22 + b222*x) + exp(a3 + b32 + b232*x) + exp(0))\nt23 <- exp(a3 + b32 + b232*x) / (exp(a1 + b12 + b212*x) + exp(a2 + b22 + b222*x) + exp(a3 + b32 + b232*x) + exp(0))\nt24 <- 1 - (t21 + t22 + t23)\n\nt31 <- exp(a1 + b13 + b212*x) / (exp(a1 + b13 + b212*x) + exp(a2 + b23 + b222*x) + exp(a3 + b33 + b232*x) + exp(0))\nt32 <- exp(a2 + b23 + b222*x) / (exp(a1 + b13 + b212*x) + exp(a2 + b23 + b222*x) + exp(a3 + b33 + b232*x) + exp(0))\nt33 <- exp(a3 + b33 + b232*x) / (exp(a1 + b13 + b212*x) + exp(a2 + b23 + b222*x) + exp(a3 + b33 + b232*x) + exp(0))\nt34 <- 1 - (t31 + t32 + t33)\n\nt41 <- exp(a1 + b212*x) / (exp(a1 + b212*x) + exp(a2 + b222*x) + exp(a3 + b232*x) + exp(0))\nt42 <- exp(a2 + b222*x) / (exp(a1 + b212*x) + exp(a2 + b222*x) + exp(a3 + b232*x) + exp(0))\nt43 <- exp(a3 + b232*x) / (exp(a1 + b212*x) + exp(a2 + b222*x) + exp(a3 + b232*x) + exp(0))\nt44 <- 1 - (t41 + t42 + t43)"},{"path":"latent-transition-analysis.html","id":"create-transition-table-1","chapter":"8 Latent Transition Analysis","heading":"8.6.4 Create Transition Table","text":"","code":"\n\nt_matrix <- tibble(\n  \"Time1\" = c(\"C1=Anti-Science\",\"C1=Amb. w/ Elevated\",\"C1=Amb. w/ Minimal\",\"C1=Pro-Science\"),\n  \"C2=Anti-Science\" = c(t11,t21,t31,t41),\n  \"C2=Amb. w/ Elevated\" = c(t12,t22,t32,t42),\n  \"C2=Amb. w/ Minimal\" = c(t13,t23,t33,t43),\n  \"C2=Pro-Science\" = c(t14,t24,t34,t44))\n\nt_matrix %>% \n  gt(rowname_col = \"Time1\") %>%\n  tab_stubhead(label = \"7th grade\") %>% \n  tab_header(\n    title = md(\"**FEMALES: Student transitions from 7th grade (rows) to 10th grade (columns)**\")) %>% \n  fmt_number(2:5,decimals = 3) %>% \n  tab_spanner(label = \"10th grade\",columns = 2:5) %>% \n  tab_footnote(\n    footnote = md(\n    \"*Note.* Transition matrix values are the identical to Table 5, however Table 5 \n    has the values rearranged by class for interpretation purposes. Classes may be arranged\n    directly through Mplus syntax using start values.\"), \n    locations = cells_title())\n\n\n\nt_matrix <- tibble(\n  \"Time1\" = c(\"C1=Anti-Science\",\"C1=Amb. w/ Elevated\",\"C1=Amb. w/ Minimal\",\"C1=Pro-Science\"),\n  \"C2=Anti-Science\" = c(t11B,t21B,t31B,t41B),\n  \"C2=Amb. w/ Elevated\" = c(t12B,t22B,t32B,t42B),\n  \"C2=Amb. w/ Minimal\" = c(t13B,t23B,t33B,t43B),\n  \"C2=Pro-Science\" = c(t14B,t24B,t34B,t44B))\n\nt_matrix %>% \n  gt(rowname_col = \"Time1\") %>%\n  tab_stubhead(label = \"7th grade\") %>% \n  tab_header(\n    title = md(\"**MALES: Student transitions from 7th grade (rows) to 10th grade (columns)**\")) %>% \n  fmt_number(2:5,decimals = 3) %>% \n  tab_spanner(label = \"10th grade\",columns = 2:5) %>% \n  tab_footnote(\n    footnote = md(\n    \"*Note.* Transition matrix values are the identical to Table 5, however Table 5 \n    has the values rearranged by class for interpretation purposes. Classes may be arranged\n    directly through Mplus syntax using start values.\"), \n    locations = cells_title())"},{"path":"joint-occurrence.html","id":"joint-occurrence","chapter":"9 Joint Occurrence","heading":"9 Joint Occurrence","text":"Example: Longitudinal Study American YouthData source: : See documentation ","code":""},{"path":"joint-occurrence.html","id":"load-packages-3","chapter":"9 Joint Occurrence","heading":"9.1 Load Packages","text":"","code":"\nlibrary(MplusAutomation)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(glue)\nlibrary(gt)\nlibrary(cowplot)\nlibrary(kableExtra)\nlibrary(psych)\nlibrary(float)\nlibrary(janitor)\nlibrary(ggalluvial)\nlibrary(DiagrammeR)\nlibrary(modelsummary)\nlibrary(corrplot)\nlibrary(ggrepel)"},{"path":"joint-occurrence.html","id":"path-diagram","chapter":"9 Joint Occurrence","heading":"9.2 Path Diagram","text":"Read LSAY dataset","code":"\ngrViz(\"\ndigraph model {\n    graph [layout=dot, overlap=true]\n\n    node [shape=box]\n    math_enjoy    [label=\\\"Math: Enjoy\\\"]    \n    math_useful    [label=\\\"Math: Useful\\\"]\n    math_logical   [label=\\\"Math: Logical\\\"]\n    math_job       [label=\\\"Math: Job\\\"]\n    math_adult     [label=\\\"Math: Adult\\\"]\n    science_enjoy [label=\\\"Science: Enjoy\\\"]\n    science_useful [label=\\\"Science: Useful\\\"]\n    science_logical[label=\\\"Science: Logical\\\"]\n    science_job    [label=\\\"Science: Job\\\"]\n    science_adult  [label=\\\"Science: Adult\\\"]\n\n    node [shape=circle]\n    C_math [label=<C<SUB>Math<\/SUB>>];\n    C_sci [label=<C<SUB>Science<\/SUB>>];\n\n    edge []\n    C_math -> {math_enjoy math_useful math_logical math_job math_adult}\n    C_sci -> {science_enjoy science_useful science_logical science_job science_adult}\n    C_math -> C_sci\n\n    {rank = same; C_math; C_sci;}\n}\n\")\ndata <- read_csv(here(\"data\", \"lsay_joint_occurrence.csv\")) %>% \n  rename(\n    math_enjoy   = KA46A, # Renaming the variables\n    math_useful  = KA46H,\n    math_logical = KA46I,\n    math_job     = KA46K,\n    math_adult   = KA46L,\n    sci_enjoy   = KA47A,\n    sci_useful  = KA47H,\n    sci_logical = KA47I,\n    sci_job     = KA47K,\n    sci_adult   = KA47L\n  ) %>% \n  clean_names() %>% # Making variables lower-case\n  mutate(across(\n    .cols = math_enjoy:sci_adult, # Dichtomizing the variables\n    .fns = ~ case_when(\n      . %in% c(1, 2) ~ 1,\n      . %in% c(3, 4, 5) ~ 0,\n      TRUE ~ NA_real_\n    )\n  ))"},{"path":"joint-occurrence.html","id":"descriptive-statistics-5","chapter":"9 Joint Occurrence","heading":"9.3 Descriptive Statistics","text":"","code":""},{"path":"joint-occurrence.html","id":"descriptive-statistics-using-r-1","chapter":"9 Joint Occurrence","heading":"9.3.1 Descriptive Statistics using R:","text":"Quick view relevant variables:Proportion indicators using R:Data summary:Correlation table:Correlation plot:","code":"\ndata %>%\n  select(\n    math_enjoy, math_useful, math_logical, math_job, math_adult,\n    sci_enjoy, sci_useful, sci_logical, sci_job, sci_adult\n  ) %>%\n  psych::describe()\n# Set up data to find proportions of binary indicators\nds <- data %>% \n  pivot_longer(c(math_enjoy:sci_adult), names_to = \"Variable\") \n\n# Create table of variables and counts\ntab <- table(ds$Variable, ds$value)\n\n# Find proportions and round to 3 decimal places\nprop <- prop.table(tab, margin = 1) %>% \n  round(3)\n\n# Combine everything to one table \ndframe <- data.frame(Variables=rownames(tab), Proportion=prop[,2], Count=tab[,2])\n#remove row names\nrow.names(dframe) <- NULL\n\n# Create table\ngt(dframe) %>% \ntab_header(title = md(\"**LCA Indicator Proportions**\"), subtitle = md(\"&nbsp;\")) %>%\ntab_options(column_labels.font.weight = \"bold\", row_group.font.weight = \"bold\") %>% \ntab_row_group(group = \"Math\", rows = 1:5) %>% \ntab_row_group(group = \"Science\", rows = 6:10) %>%\nrow_group_order(groups = c(\"Math\",\"Science\")) %>% \ntab_options(column_labels.font.weight = \"bold\", row_group.font.weight = \"bold\") \nselect_data <- data %>% \n select(math_enjoy:sci_adult)\n\nf <- All(select_data) ~ Mean + SD + Min + Median + Max + Histogram\ndatasummary(f, data, output=\"markdown\")\nselect_data %>% \n  datasummary_correlation(output = \"markdown\")\nf_cor <- data %>% \n select(math_enjoy:sci_adult) %>% \n  cor(use = \"pairwise.complete.obs\")\n\ncorrplot(f_cor, \n         method = \"circle\",\n         type = \"upper\", \n         tl.col=\"black\", \n         tl.srt=45)"},{"path":"joint-occurrence.html","id":"descriptive-statistics-using-mplusautomation-1","chapter":"9 Joint Occurrence","heading":"9.3.2 Descriptive Statistics using MplusAutomation:","text":"View output (goes detail) see brief view descriptive statistics using get_sampstat():","code":"\nbasic_mplus  <- mplusObject(\n  TITLE = \"Descriptive Statistics;\",\n  \n  VARIABLE =\n    \"usevar = math_enjoy-sci_adult;\n    categorical = math_enjoy-sci_adult;\",\n\n  ANALYSIS = \"TYPE=basic;\",\n  \n  OUTPUT = \"sampstat;\",  \n  \n  usevariables = colnames(data),\n  rdata = data)\n\nbasic_mplus_fit <- mplusModeler(basic_mplus, \n                            dataout = here(\"joint_occurrence\", \"data.dat\"),\n                            modelout = here(\"joint_occurrence\",\"basic.inp\"),\n                            check = TRUE, run = TRUE, hashfilename = FALSE)\n# Using MplusAutomation\nMplusAutomation::get_sampstat(basic_mplus_fit)\n\n# Using base R\nsummary(data)"},{"path":"joint-occurrence.html","id":"enumeration-math-only","chapter":"9 Joint Occurrence","heading":"9.4 Enumeration (Math Only)","text":"code uses mplusObject function MplusAutomation package saves model runs mplus_enum folder.IMPORTANT: moving forward, make sure examine output document ensure models estimated normally. example, last model (6-class models) produce reliable output excluded.","code":"\n\nlca_enum_6  <- lapply(1:6, function(k) {\n  lca_enum  <- mplusObject(\n    \n    TITLE = glue(\"Math Attitudes: {k}-Class\"), \n    \n    VARIABLE = glue(\n      \"categorical = math_enjoy, math_useful, math_logical, math_job, math_adult; \n     usevar = math_enjoy, math_useful, math_logical, math_job, math_adult;\n     classes = c({k});\"),\n    \n    ANALYSIS = \n      \"estimator = mlr; \n    type = mixture;\n    processors = 12;\n    starts = 500 100;\",\n    \n    OUTPUT = \"sampstat residual tech11 tech14;\",\n\n    usevariables = colnames(data),\n    rdata = data)\n  \n  lca_enum_fit <- mplusModeler(lca_enum, \n                               dataout=glue(here(\"joint_occurrence\",\"enum_math\", \"data.dat\")),\n                               modelout=glue(here(\"joint_occurrence\",\"enum_math\", \"c{k}_math.inp\")) ,\n                               check=TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"joint-occurrence.html","id":"enumeration-science-only","chapter":"9 Joint Occurrence","heading":"9.5 Enumeration (Science Only)","text":"code uses mplusObject function MplusAutomation package saves model runs mplus_enum folder.IMPORTANT: moving forward, make sure examine output document ensure models estimated normally. example, last model (6-class models) produce reliable output excluded.","code":"\n\nlca_enum_6  <- lapply(1:6, function(k) {\n  lca_enum  <- mplusObject(\n    \n    TITLE = glue(\"Science Attitudes: {k}-Class\"), \n    \n    VARIABLE = glue(\n      \"categorical = sci_enjoy, sci_useful, sci_logical, sci_job, sci_adult; \n     usevar = sci_enjoy, sci_useful, sci_logical, sci_job, sci_adult;\n     classes = c({k});\"),\n    \n    ANALYSIS = \n      \"estimator = mlr; \n    type = mixture;\n    processors = 12;\n    starts = 500 100;\",\n    \n    OUTPUT = \"sampstat residual tech11 tech14;\",\n\n    usevariables = colnames(data),\n    rdata = data)\n  \n  lca_enum_fit <- mplusModeler(lca_enum, \n                               dataout=glue(here(\"joint_occurrence\",\"enum_sci\", \"data.dat\")),\n                               modelout=glue(here(\"joint_occurrence\",\"enum_sci\", \"c{k}_sci.inp\")) ,\n                               check=TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"joint-occurrence.html","id":"fit-table","chapter":"9 Joint Occurrence","heading":"9.5.0.1 Fit Table","text":"Save table:","code":"\nsource(here(\"functions\", \"enum_table_jo.R\"))\n\n# Read model outputs\noutput_enum_c1 <- readModels(here(\"joint_occurrence\", \"enum_math\"), quiet = TRUE)\noutput_enum_c2 <- readModels(here(\"joint_occurrence\", \"enum_sci\"), quiet = TRUE)\n\n# Define rows for row groups (assuming 6 models per time)\nrows_m1 <- 1:6\nrows_m2 <- 7:12\n\nfit_table_jo <- fit_table_jo(output_enum_c1, output_enum_c2, rows_m1, rows_m2)\nfit_table_jo\ngtsave(fit_table_jo, here(\"figures\", \"fit_table_jo.png\"))"},{"path":"joint-occurrence.html","id":"information-criteria-plot-4","chapter":"9 Joint Occurrence","heading":"9.5.1 Information Criteria Plot","text":"","code":"\nsource(here(\"functions\", \"ic_plot_lca.R\"))\nic_plot(output_enum_c1)\n#ggsave(here(\"figures\", \"info_criteria_jo1.png\"),  dpi = \"retina\", bg = \"white\", height=5, width=7, units=\"in\")\nic_plot(output_enum_c2)\n#ggsave(here(\"figures\", \"info_criteria_jo2.png\"),  dpi = \"retina\", bg = \"white\", height=5, width=7, units=\"in\")"},{"path":"joint-occurrence.html","id":"class-probability-plot-3","chapter":"9 Joint Occurrence","heading":"9.5.2 4-Class Probability Plot","text":"Use plot_lca function provided folder plot item probability plot. function requires one argument:\n- model_name: name Mplus readModels object (e.g., output_enum_c1$c4_math.)","code":"\nsource(here(\"functions\",\"plot_lca.R\"))\n\nplot_lca(model_name = output_enum_c1$c4_math.out)\n#ggsave(here(\"figures\", \"probability_plot_jo1.png\"), dpi = \"retina\", bg = \"white\", height=5, width=7, units=\"in\")\nplot_lca(model_name = output_enum_c2$c4_sci.out)\n#ggsave(here(\"figures\", \"probability_plot_jo2.png\"), dpi = \"retina\", bg = \"white\", height=5, width=7, units=\"in\")"},{"path":"joint-occurrence.html","id":"estimate-joint-occurrence-lca","chapter":"9 Joint Occurrence","heading":"9.6 Estimate Joint Occurrence LCA","text":"","code":""},{"path":"joint-occurrence.html","id":"step-1---estimate-unconditional-model","chapter":"9 Joint Occurrence","heading":"9.6.1 Step 1 - Estimate Unconditional Model","text":"Math AttitudesHere, included ID variable (casenum) can later join two datasets get step 2.Note: Since emerging classes similar math science, rearranged classes match using svaues option OUTPUT command. example, Class 1 Science LCA Class 4 Math LCA “High” class. changed Math class Class 4 Class 1.\nEvaluate output compare class counts proportions latent classes. Using OPTSEED function ensures replication best loglikelihood value run.Science AttitudesConfirm plots look expected (.e., identical enumeration model)","code":"\n\nstep1  <- mplusObject(\n  TITLE = \"Step 1 - Unconditional Model\", \n  VARIABLE = \"categorical = math_enjoy, math_useful, math_logical, math_job, math_adult;\n  usevar =  math_enjoy, math_useful, math_logical, math_job, math_adult;\n  idvariable = casenum; \n  classes = c(4);\",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 0;\n    OPTSEED = 830570;\",\n  \n  SAVEDATA = \n   \"File=savedata_math.dat;\n    Save=cprob;\",\n  \n  OUTPUT = \"sampstat residual tech11 tech14 svalues(4 1 2 3)\",  # I used `svalues` to rearrange the class labels\n  \n  usevariables = colnames(data),\n  rdata = data)\n\nstep1_fit <- mplusModeler(step1,\n                            dataout=here(\"joint_occurrence\", \"jo_model\", \"data.dat\"),\n                            modelout=here(\"joint_occurrence\", \"jo_model\",  \"one_math.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\n\nstep1  <- mplusObject(\n  TITLE = \"Step 1 - Unconditional Model\", \n  VARIABLE = \"categorical = sci_enjoy, sci_useful, sci_logical, sci_job, sci_adult;\n  usevar =  sci_enjoy, sci_useful, sci_logical, sci_job, sci_adult;\n  idvariable = casenum;\n  classes = c(4);\",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 0;\n    OPTSEED = 761633;\",\n  \n  SAVEDATA = \n   \"File=savedata_sci.dat;\n    Save=cprob;\",\n  \n  OUTPUT = \"sampstat residual tech11 tech1;\",\n  \n  usevariables = colnames(data),\n  rdata = data)\n\nstep1_fit <- mplusModeler(step1,\n                            dataout=here(\"joint_occurrence\", \"jo_model\", \"data.dat\"),\n                            modelout=here(\"joint_occurrence\", \"jo_model\",  \"one_sci.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\nsource(here(\"functions\",\"plot_lca.R\"))\n\noutput_math <- readModels(here(\"joint_occurrence\",\"jo_model\",\"one_math.out\"))\noutput_sci <- readModels(here(\"joint_occurrence\",\"jo_model\",\"one_sci.out\"))\n\nplot_lca(model_name = output_math)\nplot_lca(model_name = output_sci)"},{"path":"joint-occurrence.html","id":"step-2---determine-measurement-error-3","chapter":"9 Joint Occurrence","heading":"9.6.2 Step 2 - Determine Measurement Error","text":"Extract logits classification probabilities likely latent class:Extract saved dataset:Rename column savedata named “C” change “N”","code":"\n\nlogit_cprobs_math <- as.data.frame(output_math[[\"class_counts\"]]\n                                       [[\"logitProbs.mostLikely\"]])\n\nlogit_cprobs_sci <- as.data.frame(output_sci[[\"class_counts\"]]\n                                       [[\"logitProbs.mostLikely\"]])\n\nsavedata_math <- as.data.frame(output_math[[\"savedata\"]])\nsavedata_sci <- as.data.frame(output_sci[[\"savedata\"]])\n\ncolnames(savedata_math)[colnames(savedata_math)==\"C\"] <- \"N_math\"\ncolnames(savedata_sci)[colnames(savedata_sci)==\"C\"] <- \"N_sci\"\n\nsavedata <- savedata_math %>% \n  full_join(savedata_sci, by = \"CASENUM\")"},{"path":"joint-occurrence.html","id":"step-3---add-auxiliary-variables-2","chapter":"9 Joint Occurrence","heading":"9.6.3 Step 3 - Add Auxiliary Variables","text":"Build joint occurrence model:","code":"\nstep3_jo  <- mplusObject(\n  TITLE = \"Joint Occurrence LCA\", \n  \n  VARIABLE = \n \"nominal=N_math N_sci;\n  usevar = N_math N_sci;\n  classes = math(4) sci(4);\" ,\n  \n  ANALYSIS = \n \"estimator = mlr; \n  type = mixture; \n  starts = 0;\",\n \n  MODEL =\n  glue(\n \" %OVERALL%\n      \n   sci on math; \n\n  MODEL math:\n  \n  %math#1%\n  [N_math#1@{logit_cprobs_math[1,1]}];\n  [N_math#2@{logit_cprobs_math[1,2]}];\n  [N_math#3@{logit_cprobs_math[1,3]}];\n\n\n  %math#2%\n  [N_math#1@{logit_cprobs_math[2,1]}];\n  [N_math#2@{logit_cprobs_math[2,2]}];\n  [N_math#3@{logit_cprobs_math[2,3]}];\n  \n  %math#3%\n  [N_math#1@{logit_cprobs_math[3,1]}]; \n  [N_math#2@{logit_cprobs_math[3,2]}];\n  [N_math#3@{logit_cprobs_math[3,3]}];\n  \n  %math#4%\n  [N_math#1@{logit_cprobs_math[4,1]}]; \n  [N_math#2@{logit_cprobs_math[4,2]}];\n  [N_math#3@{logit_cprobs_math[4,3]}];  \n\n \n  MODEL sci:\n  \n  %sci#1%\n  [N_sci#1@{logit_cprobs_sci[1,1]}]; \n  [N_sci#2@{logit_cprobs_sci[1,2]}];\n  [N_sci#3@{logit_cprobs_sci[1,3]}];  \n\n  %sci#2%\n  [N_sci#1@{logit_cprobs_sci[2,1]}];\n  [N_sci#2@{logit_cprobs_sci[2,2]}];\n  [N_sci#3@{logit_cprobs_sci[2,3]}];\n  \n  %sci#3%\n  [N_sci#1@{logit_cprobs_sci[3,1]}];\n  [N_sci#2@{logit_cprobs_sci[3,2]}];\n  [N_sci#3@{logit_cprobs_sci[3,3]}];\n \n  %sci#4%\n  [N_sci#1@{logit_cprobs_sci[4,1]}];\n  [N_sci#2@{logit_cprobs_sci[4,2]}];\n  [N_sci#3@{logit_cprobs_sci[4,3]}];\"),\n \n  usevariables = colnames(savedata), \n  rdata = savedata)\n\nstep3_jo_fit <- mplusModeler(step3_jo,\n               dataout=here(\"joint_occurrence\",\"jo_model\",\"three.dat\"), \n               modelout=here(\"joint_occurrence\",\"jo_model\",\"three.inp\"), \n               check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"joint-occurrence.html","id":"joint-distribution","chapter":"9 Joint Occurrence","heading":"9.6.3.1 Joint Distribution","text":"Plot:Alternative plot:Table:Always check output make sure table correct!","code":"\njo_output <- readModels(here(\"joint_occurrence\",\"jo_model\",\"three.out\"))\n\nplot_lca(model_name = output_math)\nplot_lca(model_name = output_sci)\n\nsource(here(\"functions\", \"plot_patterns.R\"))\n\ntitle <- \"Joint Occurrence Model Patterns\"\nsubtitle <- \"Math & Science Attitudes\"\n\nplot_patterns(\n  model_name = jo_output,\n  facet_labels =c(              # These are the Math labels\n    `1` = \"Pro-Math with Elevated Utility Value\",\n    `2` = \"Math Ambivalent with Minimal Utility Value\",\n    `3` = \"Math Ambivalent with Elevated Utility Value\",\n    `4` = \"Anti-Math with Minimal Utility Value\"),\n  lca_labels = c('1' = \"Math Attitudes\", '2' = \"Science Attitudes\"), \n  class_labels = c(             # These are the Science labels\n    \"Pro-Science with Elevated Utility Value\",\n    \"Science Ambivalent with Minimal Utility Value\",\n    \"Science Ambivalent with Elevated Utility Value\",\n    \"Anti-Science with Minimal Utility Value\"\n    ), \n  title,\n  subtitle\n  )\n\n\n#ggsave(here(\"figures\",\"interdependencies_plot.png\"), dpi=500,bg = \"white\", height=7, width=12, units=\"in\")\n\njo_output <- readModels(here(\"joint_occurrence\",\"jo_model\",\"three.out\"))\njo_prob <- as.data.frame(jo_output$class_counts$transitionProbs$probability)\n\nc1_labels <- c(\"Pro-Math \\nwith Elevated Utility Value \\n(46%)\", \n               \"Math Ambivalent \\nwith Minimal Utility Value\\n(18%)\", \n               \"Math Ambivalent \\nwith Elevated Utility Value\\n(19%)\",\n               \"Anti-Math \\nwith Minimal Utility Value\\n(17%)\")\nc2_labels <- c(\"Pro-Science \\nwith Elevated Utility Value\\n(30%)\",\n               \"Science Ambivalent \\nwith Minimal Utility Value\\n(26%)\",\n               \"Science Ambivalent \\nwith Elevated Utility Value\\n(8%)\",\n               \"Anti-Science \\nwith Minimal Utility Value\\n(36%)\")\n\n\n# T1 → T2\nc1_c2 <- expand.grid(C1 = c1_labels, C2 = c2_labels) %>%\n  mutate(P12 = jo_prob[1:nrow(jo_prob), 1]) %>% \n  mutate(P12 = round(P12, 2))\n\n\n# Plot for T1 -> T2\n ggplot(c1_c2, aes(axis1 = C1, axis2 = C2, y = P12)) +\n  geom_alluvium(aes(fill = C1), width = 0.2, alpha = 0.7) +\n  # Make the stratum rectangles white instead of gray:\n  geom_stratum(width = 0.2, color = \"black\") +\n  geom_text(\n    stat = \"stratum\", \n    aes(label = after_stat(stratum)), \n    size = 3.5\n  ) +\n  # Label the flows themselves with the probability\n # geom_text(aes(label = P12),\n#            stat = \"flow\", nudge_x = .2, size = 5) +\n  scale_x_discrete(limits = c(\"Math Attitudes\", \"Science Attitudes\"), expand = c(.1, .1)) +\n  labs(subtitle = \"Math and Science Attitudes\", title = \"Joint Occurrence Model\", x = \"\") +\n  theme_minimal() +\n  theme(\n    text = element_text(family = \"serif\", size = 20),\n    legend.position = \"none\",\n    axis.text.x = element_text(color = \"black\"),\n    axis.title.y = element_blank(),  \n    axis.text.y  = element_blank(), \n    axis.ticks.y = element_blank(),\n    panel.grid.major = element_blank(), \n    panel.grid.minor = element_blank(),  \n    plot.subtitle = element_text(face = \"italic\", size = 20),\n    plot.title = element_text(size = 20)\n    )\n#ggsave(here(\"figures\", \"jo_sankey.jpg\"), width=8, height = 5.5, dpi=\"retina\", bg = \"white\",  units=\"in\")\n# Extract Probabilities\njo_prob_matrix <- as.matrix(jo_output$class_counts$transitionProbs$probability)\n\n\n# Label Classes\nc1_labels <- c(\"Pro-Math \\nwith Elevated Utility Value \\n(46%)\", \n               \"Math Ambivalent \\nwith Minimal Utility Value\\n(18%)\", \n               \"Math Ambivalent \\nwith Elevated Utility Value\\n(19%)\",\n               \"Anti-Math \\nwith Minimal Utility Value\\n(17%)\")\nc2_labels <- c(\"Pro-Science \\nwith Elevated Utility Value\\n(30%)\",\n               \"Science Ambivalent \\nwith Minimal Utility Value\\n(26%)\",\n               \"Science Ambivalent \\nwith Elevated Utility Value\\n(8%)\",\n               \"Anti-Science \\nwith Minimal Utility Value\\n(36%)\")\n\n# Number of Classes for each LCA\nC1 <- length(c1_labels)  \nC2 <- length(c2_labels)  \n\n# Format Probability Table\njo_df <- matrix(jo_prob_matrix, nrow = C1, ncol = C2, byrow = FALSE)\nrownames(jo_df) <- c1_labels\ncolnames(jo_df) <- c2_labels\nt_matrix <- as.data.frame(jo_df) %>%\n  rownames_to_column(var = \"Math Attitudes\")\n\n# Create Probability Table\nt_matrix %>% \n  gt(rowname_col = \"Math Attitudes\") %>%\n  tab_stubhead(label = \"Math Attitudes\") %>% \n  tab_header(\n    title = md(\"**Joint Distribution Matrix**\"),\n    subtitle = md(\"**Distribution Math Attitude Classes (Rows) conditioned on Science Attitude Classes (Columns)**\")) %>% \n  fmt_number(2:4,decimals = 3) %>% \n  tab_spanner(label = \"Science Attitudes\",columns = 2:(C2+1))#%>% \n  #gtsave(\"matrix.docx\")"},{"path":"growth-mixture-models.html","id":"growth-mixture-models","chapter":"10 Growth mixture Models","heading":"10 Growth mixture Models","text":"Example: Longitudinal Study American YouthData source: : example looks science IRT scores time (Grades 7-12). See documentation . Covariates include gender interest science issues 7th grade.","code":""},{"path":"growth-mixture-models.html","id":"load-packages-4","chapter":"10 Growth mixture Models","heading":"10.1 Load Packages","text":"","code":"\nlibrary(tidyverse)\nlibrary(MplusAutomation)\nlibrary(here)\nlibrary(DiagrammeR)\nlibrary(glue)\nlibrary(cowplot)\nlibrary(gt)\nlibrary(Hmisc)"},{"path":"growth-mixture-models.html","id":"path-diagram-1","chapter":"10 Growth mixture Models","heading":"10.2 Path Diagram","text":"Read LSAY dataset","code":"\nlsay_sci <- read_csv(here(\"data\",\"lsay_sci_gmm.csv\")) %>% \n  rename(\n    id = CASENUM,\n    female = GENDER,\n    interest7 = AB34D,\n    sci7 = ASCIIRT,\n    sci8 = CSCIIRT,\n    sci9 = ESCIIRT, \n    sci10 = GSCIIRT,\n    sci11 = ISCIIRT,\n    sci12 = KSCIIRT\n  ) %>% \n  mutate(female = ifelse(female == 1, 1, 0))"},{"path":"growth-mixture-models.html","id":"descriptive-statistics-6","chapter":"10 Growth mixture Models","heading":"10.2.1 Descriptive Statistics","text":"","code":"\nlsay_sci %>% \n  select(-id) %>% \n  psych::describe()\n#>           vars    n  mean    sd median trimmed   mad   min\n#> female       1 5945  0.49  0.50   0.00    0.49  0.00  0.00\n#> interest7    2 2992  1.80  0.76   2.00    1.76  1.48  1.00\n#> sci7         3 3071 50.41 10.20  50.04   50.18 11.33 26.14\n#> sci8         4 2527 54.05 11.16  54.64   54.25 12.11 22.82\n#> sci9         5 2326 58.69 11.24  60.40   59.15 11.05 27.36\n#> sci10        6 4690 60.32 11.02  60.84   60.50 11.72 26.97\n#> sci11        7 3592 64.10 11.21  64.75   64.51 11.10 24.44\n#> sci12        8 2826 65.85 11.65  66.25   66.28 11.14 26.38\n#>             max range  skew kurtosis   se\n#> female     1.00  1.00  0.04    -2.00 0.01\n#> interest7  3.00  2.00  0.34    -1.20 0.01\n#> sci7      88.03 61.89  0.20    -0.46 0.18\n#> sci8      83.94 61.12 -0.17    -0.65 0.22\n#> sci9      91.21 63.85 -0.35    -0.40 0.23\n#> sci10     91.33 64.36 -0.14    -0.43 0.16\n#> sci11     93.13 68.69 -0.34    -0.17 0.19\n#> sci12     95.56 69.18 -0.35     0.02 0.22"},{"path":"growth-mixture-models.html","id":"correlation-table-1","chapter":"10 Growth mixture Models","heading":"10.2.1.1 Correlation Table","text":"","code":"\ncor_data <- lsay_sci %>% \n  select(-id)\n\nrcorr(as.matrix(cor_data)) \n#>           female interest7  sci7  sci8  sci9 sci10 sci11\n#> female      1.00     -0.15 -0.07 -0.01 -0.05 -0.09 -0.11\n#> interest7  -0.15      1.00  0.24  0.24  0.25  0.26  0.22\n#> sci7       -0.07      0.24  1.00  0.81  0.76  0.75  0.74\n#> sci8       -0.01      0.24  0.81  1.00  0.85  0.81  0.81\n#> sci9       -0.05      0.25  0.76  0.85  1.00  0.88  0.86\n#> sci10      -0.09      0.26  0.75  0.81  0.88  1.00  0.88\n#> sci11      -0.11      0.22  0.74  0.81  0.86  0.88  1.00\n#> sci12      -0.14      0.26  0.74  0.76  0.82  0.83  0.90\n#>           sci12\n#> female    -0.14\n#> interest7  0.26\n#> sci7       0.74\n#> sci8       0.76\n#> sci9       0.82\n#> sci10      0.83\n#> sci11      0.90\n#> sci12      1.00\n#> \n#> n\n#>           female interest7 sci7 sci8 sci9 sci10 sci11 sci12\n#> female      5945      2992 3071 2527 2326  4690  3592  2826\n#> interest7   2992      2992 2951 2431 2236  1907  1531  1103\n#> sci7        3071      2951 3071 2494 2297  1958  1573  1134\n#> sci8        2527      2431 2494 2527 2067  1745  1436  1051\n#> sci9        2326      2236 2297 2067 2326  1792  1432  1043\n#> sci10       4690      1907 1958 1745 1792  4690  3323  2621\n#> sci11       3592      1531 1573 1436 1432  3323  3592  2433\n#> sci12       2826      1103 1134 1051 1043  2621  2433  2826\n#> \n#> P\n#>           female interest7 sci7   sci8   sci9   sci10 \n#> female           0.0000    0.0003 0.6275 0.0264 0.0000\n#> interest7 0.0000           0.0000 0.0000 0.0000 0.0000\n#> sci7      0.0003 0.0000           0.0000 0.0000 0.0000\n#> sci8      0.6275 0.0000    0.0000        0.0000 0.0000\n#> sci9      0.0264 0.0000    0.0000 0.0000        0.0000\n#> sci10     0.0000 0.0000    0.0000 0.0000 0.0000       \n#> sci11     0.0000 0.0000    0.0000 0.0000 0.0000 0.0000\n#> sci12     0.0000 0.0000    0.0000 0.0000 0.0000 0.0000\n#>           sci11  sci12 \n#> female    0.0000 0.0000\n#> interest7 0.0000 0.0000\n#> sci7      0.0000 0.0000\n#> sci8      0.0000 0.0000\n#> sci9      0.0000 0.0000\n#> sci10     0.0000 0.0000\n#> sci11            0.0000\n#> sci12     0.0000"},{"path":"growth-mixture-models.html","id":"spaghetti-plot","chapter":"10 Growth mixture Models","heading":"10.2.1.2 Spaghetti Plot","text":"","code":"\nplot_data <- lsay_sci[1:500,] %>%\n  drop_na() %>% \n  pivot_longer(cols = starts_with(\"sci\"),  \n               names_to = \"grade\",          \n               values_to = \"value\") %>% \n  mutate(grade = factor(grade, \n                        levels = c(\"sci7\", \"sci8\", \"sci9\", \"sci10\", \"sci11\", \"sci12\"), \n                        labels = c(7,8,9,10,11,12)))\n\nmean_sci <- plot_data %>%\n  drop_na() %>% \n  group_by(grade) %>%\n  summarise(mean_response = mean(value),\n            se_response = sd(value) / sqrt(n()))\n\nggplot() +                                                                   \n  geom_point(data = plot_data, aes(x = grade, y = value, group = id), alpha = .3) +     \n  geom_line(data = plot_data, aes(x = grade, y = value, group = id), alpha = .3) +      \n  geom_point(data=mean_sci, aes(x=grade, y = mean_response), color = \"Blue\", size = 1.2) +               \n  geom_line(data=mean_sci, aes(x=grade, y = mean_response, group = 1), color = \"blue\", size = 1.2) + \n  geom_errorbar(data = mean_sci, aes(x = grade, ymin = mean_response - se_response, \n                                      ymax = mean_response + se_response),\n                width = 0.2, size = 1.2, color = \"blue\") +  \n  labs(title = \"Spaghetti Plot with Mean Line and Error Bars\",\n       x=\"Grade\", \n       y=\"Science Score\") +                                                           \n  theme_cowplot()                                                                              "},{"path":"growth-mixture-models.html","id":"unconditional-growth-mixture-model","chapter":"10 Growth mixture Models","heading":"10.2.2 Unconditional Growth Mixture Model","text":"MplusAutomation code loops class-specific statements include freeing variances covariances.","code":"\ngmm_6 <- lapply(1:6, function(k){ \n  \n  # This MODEL section changes the model specification\n  MODEL <- paste(sapply(1:k, function(i) {\n    glue(\"\n    %c#{i}%\n    s WITH I;      ! covariances are freely estimated\n    sci7-sci12;    ! variances are freely estimated\n    \")\n  }), collapse = \"\\n\")\n  \n  gmm_enum  <- mplusObject(\n    TITLE = glue(\"GMM {k}-Class\"), \n    \n    VARIABLE = glue(\n    \"usevar = sci7-sci12; \n     classes = c({k}); \"),\n    \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 200 100; \n    processors = 12;\",\n    \n    MODEL = glue(\"%OVERALL%\n       i s | sci7@0 sci8@1 sci9@2 sci10@3 sci11@4 sci12@5;\n       \n       {MODEL}\"), # The `MODEL` object is placed here.\n    \n    OUTPUT = \"tech1 tech11 tech14 sampstat standardized svalues;\",\n    \n  SAVEDATA = \n    glue(\"FILE IS savedata_c{k}.dat;\n     SAVE = cprobabilities;\"),\n  \n  PLOT = \"type=plot3;\n          series = sci7-sci12(*)\",\n  \n  usevariables = colnames(lsay_sci),\n  rdata = lsay_sci)\n\ngmm_enum_fit <- mplusModeler(gmm_enum, \n                            dataout=glue(here(\"gmm\", \"gmm_enum\", \"gmm_lsay.dat\")),\n                            modelout=glue(here(\"gmm\", \"gmm_enum\", \"c{k}_gmm_lsay.inp\")) ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"growth-mixture-models.html","id":"table-of-fit-4","chapter":"10 Growth mixture Models","heading":"10.2.2.1 Table of Fit","text":"First, extract data:, create table:","code":"\n\noutput_gmm <- readModels(here(\"gmm\",\"gmm_enum\"), filefilter = \"gmm\", quiet = TRUE)\n\n# Extract fit indices\nenum_extract <- LatexSummaryTable(\n  output_gmm,\n  keepCols = c(\n    \"Title\",\n    \"Parameters\",\n    \"LL\",\n    \"BIC\",\n    \"aBIC\",\n    \"BLRT_PValue\",\n    \"T11_VLMR_PValue\",\n    \"Observations\"\n  ),\n  sortBy = \"Title\"\n)\n\n# Extract lowest class size\nmin_sizes <- map_df(names(output_gmm), ~ {\n  model <- output_gmm[[.x]]\n  min_size <- min(model$class_counts$modelEstimated$proportion) * 100\n  tibble(Model = .x, min_cs = round(min_size, 2))\n})\n\n# Combine dataframe\ncombined <- cbind(enum_extract, min_sizes)\n\n# Calculate additional fit indices\nallFit <- combined %>%\n  mutate(CAIC = -2 * LL + Parameters * (log(Observations) + 1)) %>%\n  mutate(AWE = -2 * LL + 2 * Parameters * (log(Observations) + 1.5)) %>%\n  mutate(SIC = -.5 * BIC) %>%\n  mutate(expSIC = exp(SIC - max(SIC))) %>%\n  mutate(BF = exp(SIC - lead(SIC))) %>%\n  mutate(cmPk = expSIC / sum(expSIC)) %>%\n  dplyr::select(Title, Parameters, min_cs, LL, BIC, aBIC, CAIC, AWE, BLRT_PValue, T11_VLMR_PValue, BF, cmPk) %>%\n  arrange(Parameters)\nfit_table1 <- allFit %>%\n  gt() %>%\n  tab_header(title = md(\"**Model Fit Summary Table**\")) %>%\n  cols_label(\n    Title = \"Classes\",\n    Parameters = md(\"Par\"),\n    min_cs = md(\"Min. Class Size\"),\n    LL = md(\"*LL*\"),\n    T11_VLMR_PValue = \"VLMR\",\n    BLRT_PValue = \"BLRT\",\n    BF = md(\"BF\"),\n    cmPk = md(\"*cmPk*\")\n  ) %>%\n  tab_footnote(\n    footnote = md(\n      \"*Note.* Par = Parameters; *LL* = model log likelihood;\nBIC = Bayesian information criterion;\naBIC = sample size adjusted BIC; CAIC = consistent Akaike information criterion;\nAWE = approximate weight of evidence criterion;\nBLRT = bootstrapped likelihood ratio test p-value;\nVLMR = Vuong-Lo-Mendell-Rubin adjusted likelihood ratio test p-value;\n*cmPk* = approximate correct model probability.\"\n    ),\nlocations = cells_title()\n  ) %>%\n  tab_options(column_labels.font.weight = \"bold\") %>%\n  fmt_number(c(3:8),\n             decimals = 2) %>%\n  fmt_missing(1:12,\n              missing_text = \"--\") %>%\n  fmt(\n    c(9:10, 12),\n    fns = function(x)\n      ifelse(x < 0.001, \"<.001\",\n             scales::number(x, accuracy = .01))\n  ) %>%\n  fmt(\n    11,\n    fns = function (x)\n      ifelse(x > 100, \">100\",\n             scales::number(x, accuracy = .01))\n  ) %>%  \n  tab_style(\n    style = list(\n      cell_text(weight = \"bold\")\n      ),\n    locations = list(cells_body(\n     columns = BIC,\n     row = BIC == min(BIC[c(1:6)]) # Change this to the number of classes you estimated\n    ),\n    cells_body(\n     columns = aBIC,\n     row = aBIC == min(aBIC[1:6])\n    ),\n    cells_body(\n     columns = CAIC,\n     row = CAIC == min(CAIC[1:6])\n    ),\n    cells_body(\n     columns = AWE,\n     row = AWE == min(AWE[1:6])\n    ),\n    cells_body(\n     columns = cmPk,\n     row =  cmPk == max(cmPk[1:6])\n     ),    \n    cells_body(\n     columns = BF,\n     row =  BF > 10),\n    cells_body( \n     columns =  T11_VLMR_PValue,\n     row =  ifelse(T11_VLMR_PValue < .001 & lead(T11_VLMR_PValue) > .05, T11_VLMR_PValue < .001, NA)),\n    cells_body(\n     columns =  BLRT_PValue,\n     row =  ifelse(BLRT_PValue < .001 & lead(BLRT_PValue) > .05, BLRT_PValue < .001, NA))\n  )\n)\n\nfit_table1"},{"path":"growth-mixture-models.html","id":"information-criteria-plot-5","chapter":"10 Growth mixture Models","heading":"10.2.2.2 Information Criteria Plot","text":"","code":"\nallFit %>%\n  dplyr::select(LL:AWE) %>%\n  rowid_to_column() %>%\n  pivot_longer(`BIC`:`AWE`,\n               names_to = \"Index\",\n               values_to = \"ic_value\") %>%\n  mutate(Index = factor(Index,\n                        levels = c (\"AWE\", \"CAIC\", \"BIC\", \"aBIC\"))) %>%\n  ggplot(aes(\n    x = rowid,\n    y = ic_value,\n    color = Index,\n    shape = Index,\n    group = Index,\n    lty = Index\n  )) +\n  geom_point(size = 2.0) + geom_line(size = .8) +\n  scale_x_continuous(breaks = 1:nrow(allFit)) +\n  scale_colour_grey(end = .5) +\n  theme_cowplot() +\n  labs(x = \"Number of Classes\", y = \"Information Criteria Value\", title = \"Information Criteria\") +\n  theme(\n    text = element_text(family = \"Times\", size = 12),\n    legend.text = element_text(family=\"Times\", size=12),\n    legend.key.width = unit(3, \"line\"),\n    legend.title = element_blank(),\n    legend.position = \"top\"  \n  )"},{"path":"growth-mixture-models.html","id":"plot-gmm","chapter":"10 Growth mixture Models","heading":"10.2.2.3 Plot GMM","text":"","code":"\nplotGrowthMixtures(output_gmm, estimated = TRUE, rawdata = TRUE, \n                   time_scale = c(1, 2, 3, 4, 5, 6), alpha_range = c(0, 0.01))"},{"path":"growth-mixture-models.html","id":"covariates-growth-mixture-model","chapter":"10 Growth mixture Models","heading":"10.2.3 Covariates Growth Mixture Model","text":"Two covariates used GMM analysis related latent class variable: gender interest science issues 7th grade.","code":"\n\nstep1  <- mplusObject(\n  TITLE = \"GMM with Covariates\", \n  VARIABLE = \n  \"usevar = sci7-sci12\n  female interest7;\n    \n   classes = c(4);\",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 200 100; \n    processors = 12;\",\n  \n  MODEL = \n    \"%OVERALL%\n    \n    i s on female interest7;\n    \n    i s | sci7@0 sci8@1 sci9@2 sci10@3 sci11@4 sci12@5;\n    \n    %c#1%\n    s WITH I;      ! covariances are freely estimated\n    sci7-sci12;    ! variances are freely estimated\n    i s on female interest7;\n    \n    %c#2%\n    s WITH I;     \n    sci7-sci12;   \n    i s on female interest7;\n    \n    %c#3%\n    s WITH I;     \n    sci7-sci12;    \n    i s on female interest7;\n    \n    %c#4%\n    s WITH I;     \n    sci7-sci12;   \n    i s on female interest7;\",\n  \n  OUTPUT = \"tech1 tech11 tech14 sampstat standardized svalues;\",\n    \n  SAVEDATA = \n    glue(\"FILE IS savedata_c4.dat;\n     SAVE = cprobabilities;\"),\n  \n  PLOT = \"type=plot3;\n          series = sci7-sci12(*)\",\n  \n  usevariables = colnames(lsay_sci),\n  rdata = lsay_sci)\n\nstep1_fit <- mplusModeler(step1,\n                            dataout=here(\"gmm\", \"gmm_cov\", \"gmm_cov.dat\"),\n                            modelout=here(\"gmm\", \"gmm_cov\", \"gmm_cov.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"growth-mixture-models.html","id":"plot-gmm-1","chapter":"10 Growth mixture Models","heading":"10.2.3.1 Plot GMM","text":"","code":"\ngmm_cov <- readModels(here(\"gmm\", \"gmm_cov\", \"gmm_cov.out\"))\n\nplotGrowthMixtures(gmm_cov, estimated = TRUE, rawdata = TRUE, \n                   time_scale = c(1, 2, 3, 4, 5, 6), alpha_range = c(0, 0.01), bw = TRUE)"},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"distal-outcomes-nylund-gibson-grimm-masyn-2019","chapter":"11 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"11 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","text":"chapter replicates workflow Nylund-Gibson et al. (2019).Citation: Nylund-Gibson, K., Grimm, R. P., & Masyn, K. E. (2019). Prediction latent classes: demonstration different approaches include distal outcomes mixture models. Structural equation modeling: multidisciplinary Journal, 26(6), 967-985.","code":""},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"data-overview","chapter":"11 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"11.1 Data overview","text":"Math attitude indicator:Five math attitudinal variables used response indicators LCA model.enjoym - enjoy math.enjoym - enjoy math.goodm - good math.goodm - good math.udnerstandm - usually understand math.udnerstandm - usually understand math.nervousm - math often makes nervous upset.nervousm - math often makes nervous upset.scaredm - often get scared open math book see page problems.scaredm - often get scared open math book see page problems.Covariate:female - gender (female = 1; male = 0)Distal outcomes:mathjob - Math job (1 = student felt math useful later job; 0 = student feel math useful later job)mathjob - Math job (1 = student felt math useful later job; 0 = student feel math useful later job)mathirt - Math IRT score grade 9 (continuous, higher math scores represent higher math achievement)mathirt - Math IRT score grade 9 (continuous, higher math scores represent higher math achievement)","code":""},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"load-packages-5","chapter":"11 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"11.2 Load Packages","text":"","code":"\nlibrary(MplusAutomation)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(glue)\nlibrary(gt)\nlibrary(cowplot)\nlibrary(kableExtra)\nlibrary(psych)\nlibrary(float)\nlibrary(janitor)\nlibrary(naniar)"},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"data-preparation","chapter":"11 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"11.3 Data Preparation","text":"","code":"\nlsay_df <- read_csv(here(\"distals\", \"data\", \"lsay_df.csv\"))"},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"descriptive-statistics-7","chapter":"11 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"11.4 Descriptive Statistics","text":"","code":"\npsych::describe(lsay_df)\n#>          vars    n  mean    sd median trimmed   mad   min\n#> enjoym      1 2668  0.67  0.47    1.0    0.71  0.00  0.00\n#> goodm       2 2670  0.69  0.46    1.0    0.74  0.00  0.00\n#> undrstdm    3 2648  0.76  0.43    1.0    0.83  0.00  0.00\n#> nervousm    4 2622  0.59  0.49    1.0    0.61  0.00  0.00\n#> scaredm     5 2651  0.69  0.46    1.0    0.73  0.00  0.00\n#> mathjob     6 2321  0.69  0.46    1.0    0.74  0.00  0.00\n#> mathirt     7 2241 58.81 12.60   59.3   58.93 13.49 26.57\n#> female      8 3116  0.48  0.50    0.0    0.47  0.00  0.00\n#>            max range  skew kurtosis   se\n#> enjoym    1.00  1.00 -0.72    -1.49 0.01\n#> goodm     1.00  1.00 -0.84    -1.30 0.01\n#> undrstdm  1.00  1.00 -1.24    -0.47 0.01\n#> nervousm  1.00  1.00 -0.36    -1.87 0.01\n#> scaredm   1.00  1.00 -0.81    -1.35 0.01\n#> mathjob   1.00  1.00 -0.81    -1.34 0.01\n#> mathirt  94.19 67.62 -0.06    -0.55 0.27\n#> female    1.00  1.00  0.09    -1.99 0.01\nvis_miss(lsay_df)"},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"enumeration---maysn-2017","chapter":"11 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"11.5 Enumeration - Maysn (2017)","text":"","code":"\nlca_math  <- lapply(1:6, function(k) {\n  lca_math_enum  <- mplusObject(\n      \n    TITLE = glue(\"{k}-Class\"), \n  \n    VARIABLE = glue(\n    \"categorical = enjoym-scaredm; \n     usevar = enjoym-scaredm;\n     classes = c({k}); \"),\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 200 100; \n    processors = 10;\",\n  \n  OUTPUT = \"sampstat residual tech11 tech14 svalues;\",\n  \n  PLOT = \n    \"type = plot3; \n    series = enjoym-scaredm(*);\",\n  \n  usevariables = colnames(lsay_df),\n  rdata = lsay_df)\n\nlca_enum_fit <- mplusModeler(lca_math_enum, \n                            dataout=here(\"distals\",\"enum\",\"enum.dat\"),\n                            modelout=glue(here(\"distals\",\"enum\",\"c{k}_enum.inp\")),\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"distal-as-indicator-approach","chapter":"11 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"11.6 Distal-as-indicator approach","text":"distal--indicator approach, also called 1-step approach latent class variable distal outcome means measured one modeling step (Vermunt, 2010), distal outcome part measurement model latent variable; , distal outcome treated indicator latent class variable (Muthén & Shedden, 1999).model, five math attitude variables, gender covariate, two distal outcomes included class indicators.","code":"\ndasi <- lapply(1:6, function(k) {\n  lca_enum  <- mplusObject(\n      \n    TITLE = glue(\"{k}-Class\"), \n  \n    VARIABLE = glue(\n    \"categorical = enjoym-mathjob female; \n     usevar = enjoym-female;\n     classes = c({k});\"),\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 200 100; \n    processors = 10;\",\n  \n  OUTPUT = \"sampstat residual tech1 tech8 tech10 tech11 tech14 svalues;\",\n  \n  usevariables = colnames(lsay_df),\n  rdata = lsay_df)\n\ndasi_enum_fit <- mplusModeler(lca_enum, \n                            dataout=here(\"distals\",\"enum_dasi\",\"dasi.dat\"),\n                            modelout=glue(here(\"distals\",\"enum_dasi\",\"c{k}_dasi.inp\")),\n                            check=TRUE, run = TRUE, hashfilename = FALSE)\n})"},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"table-of-fit-5","chapter":"11 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"11.6.1 Table of fit","text":"Create table:results distal--indicator approach suggested 4-class model well. Though differences emergent latent classes compared classes unconditional LCA model without distals, substantive interpretation largely remain respect math attitudinal variables.distal mean binary outcome variable (case, math job) can located “RESULTS PROBABILITY SCALE” section. excerpt presents distal mean math job Latent Class 1.distal mean continuous outcome variable (case, math IRT score) can located “MODEL RESULTS” section. excerpt shows distal mean math IRT score Latent Class 1.","code":"\nsource(here(\"functions\", \"extract_mplus_info.R\"))\n\n# Define the directory where all of the .out files are located.\noutput_dir <- here(\"distals\", \"enum_dasi\")\n\n# Get all .out files\noutput_files <- list.files(output_dir, pattern = \"\\\\.out$\", full.names = TRUE)\n\n# Process all .out files into one dataframe\nfinal_data <- map_dfr(output_files, extract_mplus_info_extended)\n\n# Extract Sample_Size from final_data\nsample_size <- unique(final_data$Sample_Size)\n\n# Read in .out files into `MplusAutomation`\noutput_dasi <- readModels(here(\"distals\", \"enum_dasi\"),\n                          filefilter = \"dasi\",\n                          quiet = TRUE)\n\n# Extract fit indices\nenum_extract <- LatexSummaryTable(\n  output_dasi,\n  keepCols = c(\n    \"Title\",\n    \"Parameters\",\n    \"LL\",\n    \"BIC\",\n    \"aBIC\",\n    \"BLRT_PValue\",\n    \"T11_VLMR_PValue\",\n    \"Observations\"\n  ),\n  sortBy = \"Title\"\n)\n\n# Calculate additional fit indices\nallFit <- enum_extract %>%\n  mutate(CAIC = -2 * LL + Parameters * (log(Observations) + 1)) %>%\n  mutate(AWE = -2 * LL + 2 * Parameters * (log(Observations) + 1.5)) %>%\n  mutate(SIC = -.5 * BIC) %>%\n  mutate(expSIC = exp(SIC - max(SIC))) %>%\n  mutate(BF = exp(SIC - lead(SIC))) %>%\n  mutate(cmPk = expSIC / sum(expSIC)) %>%\n  dplyr::select(1:5, 9:10, 6:7, 13, 14) %>%\n  arrange(Parameters)\n\n# Merge columns with LL replications and class size from `final_data`\nmerged_table <- allFit %>%\n  mutate(Title = str_trim(Title)) %>%\n  left_join(\n    final_data %>%\n      dplyr::select(\n        Class_Model,\n        Perc_Convergence,\n        Replicated_LL_Perc,\n        Smallest_Class,\n        Smallest_Class_Perc\n      ),\n    by = c(\"Title\" = \"Class_Model\")\n  ) %>%\n  mutate(Smallest_Class = coalesce(Smallest_Class, final_data$Smallest_Class[match(Title, final_data$Class_Model)])) %>%\n  relocate(Perc_Convergence, Replicated_LL_Perc, .after = LL) %>%\n  mutate(Smallest_Class_Combined = paste0(Smallest_Class, \"\\u00A0(\", Smallest_Class_Perc, \"%)\")) %>%\n  dplyr::select(-Smallest_Class, -Smallest_Class_Perc) %>%\n  dplyr::select(\n    Title,\n    Parameters,\n    LL,\n    Perc_Convergence,\n    Replicated_LL_Perc,\n    BIC,\n    aBIC,\n    CAIC,\n    AWE,\n    T11_VLMR_PValue,\n    BLRT_PValue,\n    Smallest_Class_Combined,\n    BF,\n    cmPk\n  )\nfit_table1 <- merged_table %>%\n  dplyr::select(\n    Title,\n    Parameters,\n    LL,\n    Perc_Convergence,\n    Replicated_LL_Perc,\n    BIC,\n    aBIC,\n    CAIC,\n    AWE,\n    T11_VLMR_PValue,\n    BLRT_PValue,\n    Smallest_Class_Combined\n  ) %>%\n  gt() %>%\n  tab_header(title = md(\"**Model Fit Summary Table**\")) %>%\n  tab_spanner(label = \"Model Fit Indices\", columns = c(BIC, aBIC, CAIC, AWE)) %>%\n  tab_spanner(label = \"LRTs\",\n              columns = c(T11_VLMR_PValue, BLRT_PValue)) %>%\n  tab_spanner(\n    label = md(\"Smallest\\u00A0Class\"),\n    columns = c(Smallest_Class_Combined)\n  ) %>%  # ✅ Non-Breaking Space\n  \n  cols_label(\n    Title = \"Classes\",\n    Parameters = md(\"npar\"),\n    LL = md(\"*LL*\"),\n    Perc_Convergence = \"% Converged\",\n    Replicated_LL_Perc = \"% Replicated\",\n    BIC = \"BIC\",\n    aBIC = \"aBIC\",\n    CAIC = \"CAIC\",\n    AWE = \"AWE\",\n    T11_VLMR_PValue = \"VLMR\",\n    BLRT_PValue = \"BLRT\",\n    Smallest_Class_Combined = \"n (%)\"\n  ) %>%\n  tab_footnote(\n    footnote = md(\n      \"*Note.* Par = Parameters; *LL* = model log likelihood;\n      BIC = Bayesian information criterion;\n      aBIC = sample size adjusted BIC; CAIC = consistent Akaike information criterion;\n      AWE = approximate weight of evidence criterion;\n      BLRT = bootstrapped likelihood ratio test p-value;\n      VLMR = Vuong-Lo-Mendell-Rubin adjusted likelihood ratio test p-value;\n      *cmPk* = approximate correct model probability;\n      Smallest K = Number of cases in the smallest class (n (%));\n      LL Replicated = Whether the best log-likelihood was replicated.\"\n    ),\n    locations = cells_title()\n  ) %>%\n  tab_options(column_labels.font.weight = \"bold\") %>%\n  fmt_number(columns = c(3, 6:9), decimals = 2) %>%\n  fmt(\n    columns = c(T11_VLMR_PValue, BLRT_PValue),\n    fns = function(x)\n      ifelse(is.na(x), \"—\", ifelse(\n        x < 0.001, \"<.001\", scales::number(x, accuracy = .01)\n      ))\n  ) %>%\n  fmt_percent(\n    columns = c(Perc_Convergence, Replicated_LL_Perc),\n    decimals = 0,\n    scale_values = FALSE\n  ) %>%\n  \n  cols_align(align = \"center\", columns = everything()) %>%\n  tab_style(\n    style = list(cell_text(weight = \"bold\")),\n    locations = list(\n      cells_body(columns = BIC, row = BIC == min(BIC)),\n      cells_body(columns = aBIC, row = aBIC == min(aBIC)),\n      cells_body(columns = CAIC, row = CAIC == min(CAIC)),\n      cells_body(columns = AWE, row = AWE == min(AWE)),\n      cells_body(\n        columns = T11_VLMR_PValue,\n        row = ifelse(\n          T11_VLMR_PValue < .05 &\n            lead(T11_VLMR_PValue) > .05,\n          T11_VLMR_PValue < .05,\n          NA\n        )\n      ),\n      cells_body(\n        columns = BLRT_PValue,\n        row = ifelse(BLRT_PValue < .05 &\n                       lead(BLRT_PValue) > .05, BLRT_PValue < .05, NA)\n      )\n    )\n  )\n\nfit_table1"},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"conditional-item-probability-plot","chapter":"11 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"11.6.2 Conditional Item Probability Plot","text":"","code":"\nsource(here(\"functions\", \"plot_lca.R\"))\n\nplot_lca(model_name = output_dasi$c4_dasi.out)"},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"adding-constraints-to-test-the-distal-means","chapter":"11 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"11.7 Adding constraints to test the distal means","text":"significance newly specified parameters (e.g., DJ1V2, DM1V2) indicates whether statistically significant differences latent classes. example, significant p-value DM1V2 suggests meaningful difference distal outcome math IRT score Latent Class 1 Latent Class 2.","code":"\ndasi_cons  <- mplusObject(\n  TITLE = \"D as I with constraints\", \n  VARIABLE = \n  \"categorical = enjoym-mathjob female;\n   usevar = enjoym-female;\n   classes = c(4);\",\n  \n  ANALYSIS = \n  \"estimator = mlr; \n    type = mixture;\n    starts = 200 100; \n    processors = 10;\",\n  \n  MODEL =\n    \" %overall%\n\n      %c#1%\n      [mathjob$1] (dj1);\n      [mathirt]   (dm1);\n      [female$1]  (df1);\n\n      %c#2%\n      [mathjob$1] (dj2);\n      [mathirt]   (dm2);\n      [female$1]  (df2);\n\n      %c#3%\n      [mathjob$1] (dj3);\n      [mathirt]   (dm3);\n      [female$1]  (df3);\n\n      %c#4%\n      [mathjob$1] (dj4);\n      [mathirt]   (dm4);\n      [female$1]  (df4);\",\n  \n  SAVEDATA = \n   \"File=3step_savedata.dat;\n    Save=cprob;\",\n  \n  OUTPUT = \"residual tech11 tech14\",\n  \n  MODELCONSTRAINT = \n  \"New (dj1v2 dj1v3 dj1v4 dj2v3 dj2v4 dj3v4\n       dm1v2 dm1v3 dm1v4 dm2v3 dm2v4 dm3v4\n       df1v2 df1v3 df1v4 df2v3 df2v4 df3v4);\n\n    dj1v2 = dj1-dj2;\n    dj1v3 = dj1-dj3;\n    dj1v4 = dj1-dj4;\n    dj2v3 = dj2-dj3;\n    dj2v4 = dj2-dj4;\n    dj3v4 = dj3-dj4;\n\n    dm1v2 = dm1-dm2;\n    dm1v3 = dm1-dm3;\n    dm1v4 = dm1-dm4;\n    dm2v3 = dm2-dm3;\n    dm2v4 = dm2-dm4;\n    dm3v4 = dm3-dm4;\n\n    df1v2 = df1-df2;\n    df1v3 = df1-df3;\n    df1v4 = df1-df4;\n    df2v3 = df2-df3;\n    df2v4 = df2-df4;\n    df3v4 = df3-df4;\",\n  \n  usevariables = colnames(lsay_df),\n  rdata = lsay_df)\n\ndasi_cons_fit <- mplusModeler(dasi_cons,\n                            dataout=here(\"distals\",\"constraints\",\"dasi_constraints.dat\"),\n                            modelout=here(\"distals\",\"constraints\",\"dasi_constraints.inp\"),\n                            check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"manual-3-step","chapter":"11 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"11.8 Manual 3-step","text":"","code":""},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"step-1-class-enumeration-w-auxiliary-specification","chapter":"11 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"11.8.1 Step 1 Class Enumeration w/ Auxiliary Specification","text":"first step involves identifying best-fitting unconditional model saving posterior probabilities modal class assignment model (.e., savedata: save=cprob;) specifying distal outcome variables auxiliary variables included new data file saved savedata command.step done class enumeration. example, four class model best. Therefore, re-estimating four-class model using optseed efficiency. optseed can found “RANDOM STARTS RESULTS RANKED BEST WORST LOGLIKELIHOOD VALUES” section. random start value yields best log-likelihood can used optseed, produce identical parameter estimates.SAVEDATA command, can save posterior probabilities modal class assignment steps two three.","code":"\nml_step1  <- mplusObject(\n  TITLE = \"Step 1 \", \n  VARIABLE = \n  \"categorical = enjoym-scaredm; \n   usevar = enjoym-scaredm;\n    \n   classes = c(4); \n    \n   auxiliary =   ! list all potential covariates and distals here\n   female        ! covariate\n   mathjob mathirt;      ! distal math test score in 9th grade \",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 0;\n    optseed = 484501;\",\n  \n  SAVEDATA = \n   \"File = 3step_savedata.dat;\n    Save = cprob;\",\n  \n  OUTPUT = \"residual tech11 tech14\",\n  \n  usevariables = colnames(lsay_df),\n  rdata = lsay_df)\n\nml_step1_fit <- mplusModeler(ml_step1,\n                            dataout=here(\"distals\",\"three_step\",\"ML_step1.dat\"),\n                            modelout=here(\"distals\",\"three_step\",\"ML_step1.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"step-2---determine-measurement-error-4","chapter":"11 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"11.8.2 Step 2 - Determine Measurement Error","text":"","code":"\noutput_lsay <- readModels(here(\"distals\",\"three_step\",\"ML_step1.out\"))\n\nlogit_cprobs <- as.data.frame(output_lsay[[\"class_counts\"]]\n                                       [[\"logitProbs.mostLikely\"]])\n\nsavedata_lsay <- as.data.frame(output_lsay[[\"savedata\"]])\n\ncolnames(savedata_lsay)[colnames(savedata_lsay)==\"C\"] <- \"N\""},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"step-3---add-auxiliary-variables-3","chapter":"11 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"11.8.3 Step 3 - Add Auxiliary Variables","text":"distal means outcome variables located ‘MODEL RESULTS’ section. excerpt presents distal means math job math IRT score Latent Class 1.","code":"\nML_step3  <- mplusObject(\n  TITLE = \"Step3 \", \n  \n  VARIABLE = \n \"nominal = N;\n  usevar = n;\n  \n  classes = c(4);\n  \n  usevar = female mathjob mathirt;\" ,\n  \n  ANALYSIS = \n \"estimator = mlr; \n  type = mixture; \n  starts = 100 50;\n  processors = 4;\",\n \n  DEFINE = \n   \"center female (grandmean);\",\n  \n  MODEL =\n  glue(\n \" %OVERALL%\n \n  mathirt on female; ! covariate as a predictor of the distal outcome\n  mathjob on female;\n  C on female;        ! covariate as predictor of C\n\n     %C#1%\n  [n#1@{logit_cprobs[1,1]}]; ! MUST EDIT if you do not have a 4-class model. \n  [n#2@{logit_cprobs[1,2]}];\n  [n#3@{logit_cprobs[1,3]}];\n  \n  [mathirt](m1);    ! conditional distal mean \n  mathirt;          ! conditional distal variance (freely estimated)\n  [mathjob](j1);\n  mathjob;\n\n  %C#2%\n  [n#1@{logit_cprobs[2,1]}];\n  [n#2@{logit_cprobs[2,2]}];\n  [n#3@{logit_cprobs[2,3]}];\n  \n  [mathirt](m2);\n  mathirt;\n  [mathjob](j2);\n  mathjob;\n  \n  %C#3%\n  [n#1@{logit_cprobs[3,1]}];\n  [n#2@{logit_cprobs[3,2]}];\n  [n#3@{logit_cprobs[3,3]}];\n  \n  [mathirt](m3);\n  mathirt;\n  [mathjob](j3);\n  mathjob;\n\n  %C#4%\n  [n#1@{logit_cprobs[4,1]}];\n  [n#2@{logit_cprobs[4,2]}];\n  [n#3@{logit_cprobs[4,3]}];\n  \n  [mathirt](m4);\n  mathirt; \n  [mathjob](j4);\n  mathjob;  \"),\n  \n  MODELCONSTRAINT = \n   \"New (dm1v2 dm1v3 dm2v3 dm1v4 dm2v4 dm3v4 \n         dj1v2 dj1v3 dj2v3 dj1v4 dj2v4 dj3v4 \n    );\n  \n    dm1v2 = m1-m2;  ! test pairwise distal mean differences of IRT score\n    dm1v3 = m1-m3;\n    dm2v3 = m2-m3;\n    dm1v4 = m1-m4;\n    dm2v4 = m2-m4;\n    dm3v4 = m3-m4;\n    dj1v2 = j1-j2;  ! test pairwise distal mean differences of math job\n    dj1v3 = j1-j3;\n    dj2v3 = j2-j3;\n    dj1v4 = j1-j4;\n    dj2v4 = j2-j4;\n    dj3v4 = j3-j4;\",\n  \n  MODELTEST = \"     ! omnibus test of distal means \n    m1=m2;\n    m2=m3;\n    m3=m4;\n    j1=j2;\n    j2=j3;\n    j3=j4;\",\n \n  usevariables = colnames(savedata_lsay), \n  rdata = savedata_lsay)\n\nstep3_fit <- mplusModeler(ML_step3,\n               dataout=here(\"distals\",\"three_step\",\"ML_step3.dat\"), \n               modelout=here(\"distals\",\"three_step\",\"ML_step3.inp\"), \n               check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"bch-approach","chapter":"11 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"11.9 BCH approach","text":"BCH method similar three-step approach except instead calculating average classification error second step, classification errors individual computed, inverse logits individual-level error rates used weights third step rather using modal class assignment imperfect latent class indicator. Advantages approach appears resistant shifts latent classes third step (problem manual 3-step Mplus) can often used irrespective variances equal unequal across latent classes (can problem LTB approach).","code":""},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"step-1---class-enumeration-w-auxiliary-specification-and-bch-weights","chapter":"11 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"11.9.1 Step 1 - Class Enumeration w/ Auxiliary Specification and BCH Weights","text":"","code":"\nstep1_bch  <- mplusObject(\n  TITLE = \"Step 1 - BCH Method\", \n  VARIABLE = \n  \"categorical = enjoym-scaredm; \n   usevar = enjoym-scaredm;\n    \n   classes = c(4); \n    \n   auxiliary =   ! list all potential covariates and distals here\n   female        ! covariate\n   mathjob mathirt;      ! distal math test score in 9th grade \",\n  \n  ANALYSIS = \n   \"estimator = mlr; \n    type = mixture;\n    starts = 500 100;\",\n  \n  SAVEDATA = \n   \"File=3step_savedata.dat;\n    Save=bchweights; ! Here we save the BCH weights\n    format = free;\",\n\n  OUTPUT = \"sampstat residual tech11 tech14\",\n  \n  PLOT = \n    \"type = plot3; \n    series = enjoym-scaredm(*);\",\n  \n  usevariables = colnames(lsay_df),\n  rdata = lsay_df)\n\nstep1_fit_bch <- mplusModeler(step1_bch,\n                            dataout=here(\"distals\",\"three_step\",\"BCH_Step1.dat\"),\n                            modelout=here(\"distals\",\"three_step\",\"BCH_Step1.inp\") ,\n                            check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"step-2---extract-bch-weights","chapter":"11 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"11.9.1.1 Step 2 - Extract BCH Weights","text":"Extract saved dataset part mplusObject “step1_fit_bch”Rename column savedata named “C” change “N”","code":"\noutput_bch <- readModels(here(\"distals\",\"three_step\",\"BCH_step1.out\"))\n\nsavedata_bch <- as.data.frame(output_bch[[\"savedata\"]])\ncolnames(savedata_bch)[colnames(savedata_bch)==\"C\"] <- \"N\""},{"path":"distal-outcomes-nylund-gibson-grimm-masyn-2019.html","id":"step-3---add-auxiliary-variables-and-bch-weights","chapter":"11 Distal Outcomes (Nylund-Gibson, Grimm, & Masyn, 2019)","heading":"11.9.1.2 Step 3 - Add Auxiliary Variables and BCH Weights","text":"distal means outcome variables located ‘MODEL RESULTS’ section. excerpt presents distal means math job math IRT score Latent Class 1.Note class numbering may consistent across different methods. Check class proportions make sure comparing class across methods. example, Latent Class 1 BCH results matches Latent Class 4 ML 3-step results..ML-3 step resultsBCH approach results","code":"\nstep3_bch  <- mplusObject(\n  TITLE = \"Step3 - BCH Method\", \n  \n  VARIABLE = \n \"classes = c(4);\n  \n  missing are all(9999);\n  \n  usevar = BCHW1-BCHW4 mathjob mathirt female;\n  \n  training = BCHW1-BCHW4(bch);\" ,\n  \n  ANALYSIS = \n \"estimator = mlr; \n  type = mixture; \n  starts = 500 200;\",\n  \n  MODEL =\n  glue(\n \" %OVERALL%\n  \n  mathirt on female; ! covariate as a predictor of the distal outcome\n  mathjob on female;\n  C on female;        ! covariate as predictor of C\n\n  %C#1%\n     \n  [mathirt](m1);    ! conditional distal mean \n  mathirt;          ! conditional distal variance (freely estimated)\n  [mathjob](j1);\n  mathjob;\n  \n  mathirt on female (rm1);\n  mathjob on female (rj1);\n\n  %C#2%\n  \n  [mathirt](m2);\n  mathirt;\n  [mathjob](j2);\n  mathjob;\n  \n  mathirt on female (rm2);\n  mathjob on female (rj2);\n  \n  %C#3%\n  \n  [mathirt](m3);\n  mathirt;\n  [mathjob](j3);\n  mathjob;\n  \n  mathirt on female (rm3);\n  mathjob on female (rj3);\n\n  %C#4%\n  \n  [mathirt](m4);\n  mathirt; \n  [mathjob](j4);\n  mathjob;\n \n   mathirt on female (rm4);\n   mathjob on female (rj4);\"),\n  \n  MODELCONSTRAINT = \n \n   \"New (dm1v2 dm1v3 dm2v3 dm1v4 dm2v4 dm3v4\n   dj1v2 dj1v3 dj2v3 dj1v4 dj2v4 dj3v4\n    );\n  \n    dm1v2 = m1-m2;  ! test pairwise distal mean differences of IRT score\n    dm1v3 = m1-m3;\n    dm2v3 = m2-m3;\n    dm1v4 = m1-m4;\n    dm2v4 = m2-m4;\n    dm3v4 = m3-m4;\n    dj1v2 = j1-j2;  ! test pairwise distal mean differences of math job\n    dj1v3 = j1-j3;\n    dj2v3 = j2-j3;\n    dj1v4 = j1-j4;\n    dj2v4 = j2-j4;\n    dj3v4 = j3-j4;\",\n  \n  MODELTEST = \"     ! omnibus test of distal means\n    m1=m2;\n    m2=m3;\n    m3=m4;\n    j1=j2;\n    j2=j3;\n    j3=j4;\n \",\n  \n  OUTPUT = \"Tech1 svalues sampstat\",\n \n  usevariables = colnames(savedata_bch), \n  rdata = savedata_bch)\n\nstep3_fit_bch <- mplusModeler(step3_bch,\n               dataout=here(\"distals\",\"three_step\",\"BCH_Step3.dat\"), \n               modelout=here(\"distals\",\"three_step\",\"BCH_Step3.inp\"), \n               check=TRUE, run = TRUE, hashfilename = FALSE)"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
